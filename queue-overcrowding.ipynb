{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8268787,"sourceType":"datasetVersion","datasetId":4908965}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics -q","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:57:26.572231Z","iopub.execute_input":"2024-05-05T08:57:26.572836Z","iopub.status.idle":"2024-05-05T08:57:40.754961Z","shell.execute_reply.started":"2024-05-05T08:57:26.572799Z","shell.execute_reply":"2024-05-05T08:57:40.753734Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\nelse:\n    print(\"GPU is not available, using CPU.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:30:24.699328Z","iopub.execute_input":"2024-05-04T21:30:24.699688Z","iopub.status.idle":"2024-05-04T21:30:28.632151Z","shell.execute_reply.started":"2024-05-04T21:30:24.699655Z","shell.execute_reply":"2024-05-04T21:30:28.630911Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}]},{"cell_type":"code","source":"# from ultralytics import YOLO\n# from ultralytics.solutions import object_counter\n# import cv2\n# import torch\n# import matplotlib.pyplot as plt\n\n# model = YOLO(\"yolov8l.pt\")\n# if torch.cuda.is_available():\n#     model.to('cuda')\n    \n# cap = cv2.VideoCapture(\"/kaggle/input/pune-metro-hackathon/dataset/ticketing-crowd/Crowding Near Ticketing line_1.avi\")\n# assert cap.isOpened(), \"Error reading video file\"\n# w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\n# # Define region points as a polygon with 5 points\n# region_points = [[\n#                     270.68710384915494,\n#                     832.9224757715328\n#                 ],\n#                 [\n#                     1412.6219817768006,\n#                     376.6059232869861\n#                 ],\n#                 [\n#                     1144.8227883536629,\n#                     275.9507796486947\n#                 ],\n#                 [\n#                     194.71646080318862,\n#                     563.374785355727\n#                 ]]\n\n# # Video writer\n# video_writer = cv2.VideoWriter(\"/kaggle/working/Crowding1.avi\",\n#                        cv2.VideoWriter_fourcc(*'mp4v'),\n#                        fps,\n#                        (w, h))\n\n# # Init Object Counter\n# counter = object_counter.ObjectCounter()\n# counter.set_args(view_img=True,\n#                  reg_pts=region_points,\n#                  classes_names=model.names,\n#                  draw_tracks=True,\n#                  line_thickness=2)\n\n# while cap.isOpened():\n#     success, im0 = cap.read()\n#     if not success:\n#         print(\"Video frame is empty or video processing has been successfully completed.\")\n#         break\n#     tracks = model.track(im0, persist=True, show=False)\n\n#     im0 = counter.start_counting(im0, tracks)\n#     video_writer.write(im0)\n\n# cap.release()\n# video_writer.release()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T15:41:12.590787Z","iopub.execute_input":"2024-05-04T15:41:12.591165Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n\nPolygon Counter Initiated.\n\n0: 384x640 5 persons, 12.2ms\nSpeed: 2.1ms preprocess, 12.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.6ms\nSpeed: 2.6ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 9.1ms\nSpeed: 3.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 10.1ms\nSpeed: 3.3ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.7ms\nSpeed: 3.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.1ms\nSpeed: 2.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.0ms\nSpeed: 2.8ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 9.1ms\nSpeed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.7ms\nSpeed: 3.1ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.4ms\nSpeed: 3.2ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 9.4ms\nSpeed: 3.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.9ms\nSpeed: 3.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.9ms\nSpeed: 3.2ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.5ms\nSpeed: 4.3ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.1ms\nSpeed: 3.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.9ms\nSpeed: 3.1ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.2ms\nSpeed: 2.7ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.4ms\nSpeed: 3.3ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.8ms\nSpeed: 3.1ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.9ms\nSpeed: 3.2ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.7ms\nSpeed: 3.2ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.1ms\nSpeed: 2.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 2.5ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.9ms\nSpeed: 2.5ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.6ms\nSpeed: 3.2ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.4ms\nSpeed: 3.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.1ms\nSpeed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 3.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.8ms\nSpeed: 3.3ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.3ms\nSpeed: 3.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 11.0ms\nSpeed: 3.7ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.3ms\nSpeed: 3.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 2.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 10.0ms\nSpeed: 3.2ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 2.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.2ms\nSpeed: 2.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 9.4ms\nSpeed: 3.0ms preprocess, 9.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.1ms\nSpeed: 2.5ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.1ms\nSpeed: 1.9ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 9.4ms\nSpeed: 3.0ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 3.1ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.0ms\nSpeed: 3.4ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 2.9ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.8ms\nSpeed: 4.3ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.1ms\nSpeed: 3.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.6ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.2ms\nSpeed: 2.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.0ms\nSpeed: 3.5ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.7ms\nSpeed: 3.1ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.8ms\nSpeed: 3.3ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 10.2ms\nSpeed: 3.0ms preprocess, 10.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.7ms\nSpeed: 2.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 9.8ms\nSpeed: 3.7ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.9ms\nSpeed: 2.4ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 9.4ms\nSpeed: 3.3ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 9.7ms\nSpeed: 3.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.8ms\nSpeed: 2.6ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.9ms\nSpeed: 3.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.7ms\nSpeed: 3.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 9.4ms\nSpeed: 2.4ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 9.9ms\nSpeed: 2.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 10.7ms\nSpeed: 3.8ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.9ms\nSpeed: 3.3ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 9.7ms\nSpeed: 3.4ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.8ms\nSpeed: 3.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.5ms\nSpeed: 3.1ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.4ms\nSpeed: 2.3ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.0ms\nSpeed: 3.4ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.6ms\nSpeed: 2.9ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.6ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.8ms\nSpeed: 3.4ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 suitcases, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 suitcases, 7.5ms\nSpeed: 3.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 suitcases, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 9.8ms\nSpeed: 3.2ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 9.1ms\nSpeed: 3.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.7ms\nSpeed: 2.7ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 4.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 12.0ms\nSpeed: 3.0ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.5ms\nSpeed: 3.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.4ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.2ms\nSpeed: 3.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.7ms\nSpeed: 3.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.8ms\nSpeed: 3.3ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.2ms\nSpeed: 3.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.7ms\nSpeed: 3.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.9ms\nSpeed: 3.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.4ms\nSpeed: 2.9ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.7ms\nSpeed: 3.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.9ms\nSpeed: 3.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 9.5ms\nSpeed: 3.4ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.8ms\nSpeed: 3.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 3.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 9.2ms\nSpeed: 2.8ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 9.6ms\nSpeed: 3.5ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.6ms\nSpeed: 3.2ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 9.4ms\nSpeed: 2.9ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.2ms\nSpeed: 2.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.5ms\nSpeed: 3.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 dog, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.8ms\nSpeed: 3.2ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.8ms\nSpeed: 3.2ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.7ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.2ms\nSpeed: 3.0ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.0ms\nSpeed: 2.8ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.8ms\nSpeed: 3.4ms preprocess, 8.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.8ms\nSpeed: 3.8ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.6ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.9ms\nSpeed: 3.2ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 3.1ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.8ms\nSpeed: 3.2ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.7ms\nSpeed: 3.5ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.3ms\nSpeed: 3.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 11.1ms\nSpeed: 3.8ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 3.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.1ms\nSpeed: 4.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.8ms\nSpeed: 3.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 12.4ms\nSpeed: 3.2ms preprocess, 12.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 9.3ms\nSpeed: 3.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 9.0ms\nSpeed: 2.4ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.2ms\nSpeed: 3.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 9.7ms\nSpeed: 3.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.9ms\nSpeed: 3.1ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 11.3ms\nSpeed: 2.7ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 9.8ms\nSpeed: 3.3ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 8.1ms\nSpeed: 3.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 10.1ms\nSpeed: 3.0ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 10.4ms\nSpeed: 3.7ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.9ms\nSpeed: 3.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 8.5ms\nSpeed: 2.4ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 handbag, 8.7ms\nSpeed: 3.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.0ms\nSpeed: 3.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 10.7ms\nSpeed: 2.6ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.1ms\nSpeed: 3.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.6ms\nSpeed: 3.0ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.4ms\nSpeed: 3.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 9.0ms\nSpeed: 3.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.9ms\nSpeed: 3.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 10.6ms\nSpeed: 3.3ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.8ms\nSpeed: 3.3ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.2ms\nSpeed: 3.4ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 11.7ms\nSpeed: 3.3ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.2ms\nSpeed: 3.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.1ms\nSpeed: 3.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.1ms\nSpeed: 3.2ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.2ms\nSpeed: 2.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.2ms\nSpeed: 3.8ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 1.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 9.4ms\nSpeed: 3.3ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.3ms\nSpeed: 3.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 2.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 4.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.7ms\nSpeed: 3.4ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.2ms\nSpeed: 2.4ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 11.4ms\nSpeed: 3.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 9.8ms\nSpeed: 3.2ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 2.7ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.8ms\nSpeed: 3.1ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.2ms\nSpeed: 2.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 9.8ms\nSpeed: 3.5ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 11.9ms\nSpeed: 2.6ms preprocess, 11.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.8ms\nSpeed: 3.8ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 3.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 10.0ms\nSpeed: 3.1ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 9.9ms\nSpeed: 3.2ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 9.6ms\nSpeed: 3.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 8.9ms\nSpeed: 2.6ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 8.2ms\nSpeed: 2.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 3.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 10.1ms\nSpeed: 3.9ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.2ms\nSpeed: 3.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 9.0ms\nSpeed: 3.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 2.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.2ms\nSpeed: 2.5ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 11.4ms\nSpeed: 3.6ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.7ms\nSpeed: 3.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 3.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.3ms\nSpeed: 2.4ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 9.0ms\nSpeed: 3.5ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.3ms\nSpeed: 3.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 2.7ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 3.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.0ms\nSpeed: 2.3ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 9.7ms\nSpeed: 3.0ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 10.0ms\nSpeed: 2.9ms preprocess, 10.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.4ms\nSpeed: 2.8ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.2ms\nSpeed: 3.4ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 9.5ms\nSpeed: 3.7ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 3.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.5ms\nSpeed: 3.6ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 10.8ms\nSpeed: 3.4ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 9.1ms\nSpeed: 3.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.3ms\nSpeed: 3.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.2ms\nSpeed: 3.7ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.0ms\nSpeed: 3.7ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.3ms\nSpeed: 3.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 10.1ms\nSpeed: 3.8ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 12.4ms\nSpeed: 3.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.0ms\nSpeed: 3.5ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.1ms\nSpeed: 3.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 4.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.6ms\nSpeed: 3.4ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 10.0ms\nSpeed: 2.9ms preprocess, 10.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.9ms\nSpeed: 3.4ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 9.4ms\nSpeed: 3.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 2.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 10.3ms\nSpeed: 2.9ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 2.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 12.3ms\nSpeed: 2.3ms preprocess, 12.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 9.9ms\nSpeed: 3.2ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 11.5ms\nSpeed: 2.7ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.4ms\nSpeed: 3.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 9.5ms\nSpeed: 3.6ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 10.2ms\nSpeed: 3.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 10.4ms\nSpeed: 3.3ms preprocess, 10.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 9.4ms\nSpeed: 3.5ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.7ms\nSpeed: 3.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 8.9ms\nSpeed: 3.2ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 8.3ms\nSpeed: 2.9ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 8.0ms\nSpeed: 2.7ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 9.6ms\nSpeed: 3.4ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 10.4ms\nSpeed: 3.3ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 handbags, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 handbags, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 handbags, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 handbags, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 handbags, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 handbags, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 handbags, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 handbags, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 handbags, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 handbags, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 8.6ms\nSpeed: 2.9ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 8.7ms\nSpeed: 3.2ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 9.1ms\nSpeed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 9.5ms\nSpeed: 3.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 2 suitcases, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 10.0ms\nSpeed: 3.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 8.9ms\nSpeed: 3.3ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 9.2ms\nSpeed: 2.8ms preprocess, 9.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.8ms\nSpeed: 3.4ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 9.8ms\nSpeed: 3.1ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.9ms\nSpeed: 3.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 8.4ms\nSpeed: 3.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.8ms\nSpeed: 3.1ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 9.1ms\nSpeed: 3.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.8ms\nSpeed: 3.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 12.1ms\nSpeed: 3.3ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 9.4ms\nSpeed: 3.4ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.2ms\nSpeed: 4.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.5ms\nSpeed: 2.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.9ms\nSpeed: 3.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.8ms\nSpeed: 2.5ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 8.8ms\nSpeed: 3.3ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 9.7ms\nSpeed: 3.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.9ms\nSpeed: 3.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 10.9ms\nSpeed: 3.1ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.1ms\nSpeed: 2.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.5ms\nSpeed: 3.7ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.8ms\nSpeed: 3.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 8.3ms\nSpeed: 2.8ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 10.9ms\nSpeed: 3.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.7ms\nSpeed: 3.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.9ms\nSpeed: 3.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 skateboard, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 9.1ms\nSpeed: 3.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.3ms\nSpeed: 2.4ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 10.1ms\nSpeed: 3.4ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 10.1ms\nSpeed: 3.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.0ms\nSpeed: 3.5ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.5ms\nSpeed: 3.7ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.7ms\nSpeed: 3.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 8.4ms\nSpeed: 2.9ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 skateboard, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 skateboard, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 2.6ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 9.0ms\nSpeed: 3.2ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 8.0ms\nSpeed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 12.4ms\nSpeed: 3.1ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 11.4ms\nSpeed: 2.4ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 9.4ms\nSpeed: 3.3ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 9.0ms\nSpeed: 4.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 8.8ms\nSpeed: 3.3ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 8.1ms\nSpeed: 3.7ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 9.1ms\nSpeed: 3.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 4.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 12.7ms\nSpeed: 2.2ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 8.3ms\nSpeed: 3.6ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 9.9ms\nSpeed: 3.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 handbags, 1 suitcase, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 handbags, 1 suitcase, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 3.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 2 suitcases, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.2ms\nSpeed: 2.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 2 suitcases, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 10.1ms\nSpeed: 3.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 handbags, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 handbags, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 handbags, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 handbags, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 handbags, 1 suitcase, 10.8ms\nSpeed: 2.8ms preprocess, 10.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 handbags, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 9.5ms\nSpeed: 3.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 9.2ms\nSpeed: 3.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 2 handbags, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 2 handbags, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 2 handbags, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 3 backpacks, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 1 skateboard, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 1 skateboard, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 10.2ms\nSpeed: 3.4ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 9.4ms\nSpeed: 3.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 9.4ms\nSpeed: 3.4ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 9.6ms\nSpeed: 2.9ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 12.5ms\nSpeed: 3.3ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 8.9ms\nSpeed: 3.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 suitcases, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 10.1ms\nSpeed: 3.5ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 10.1ms\nSpeed: 3.7ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 7.5ms\nSpeed: 3.5ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 skateboard, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 8.2ms\nSpeed: 3.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 9.7ms\nSpeed: 3.4ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 8.9ms\nSpeed: 3.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 9.1ms\nSpeed: 3.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.6ms\nSpeed: 2.7ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 7.6ms\nSpeed: 3.6ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 3.4ms preprocess, 7.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 8.8ms\nSpeed: 2.1ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.9ms\nSpeed: 2.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.3ms\nSpeed: 2.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.6ms\nSpeed: 3.8ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.7ms\nSpeed: 3.7ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.9ms\nSpeed: 3.8ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 8.0ms\nSpeed: 2.5ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.2ms\nSpeed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 11.5ms\nSpeed: 2.7ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.5ms\nSpeed: 3.1ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.9ms\nSpeed: 3.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 11.3ms\nSpeed: 3.3ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.9ms\nSpeed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 9.6ms\nSpeed: 3.2ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 9.5ms\nSpeed: 2.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.7ms\nSpeed: 3.6ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 9.2ms\nSpeed: 3.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 9.5ms\nSpeed: 3.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 suitcases, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.2ms\nSpeed: 3.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 suitcases, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 suitcases, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 13.1ms\nSpeed: 3.9ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 11.9ms\nSpeed: 3.1ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 9.6ms\nSpeed: 3.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 8.9ms\nSpeed: 3.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 10.0ms\nSpeed: 3.2ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.6ms\nSpeed: 2.4ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 9.2ms\nSpeed: 3.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 9.3ms\nSpeed: 3.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.9ms\nSpeed: 3.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 9.7ms\nSpeed: 3.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 12.1ms\nSpeed: 3.4ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 9.0ms\nSpeed: 3.4ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 9.8ms\nSpeed: 3.6ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 9.5ms\nSpeed: 3.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 9.2ms\nSpeed: 2.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 9.0ms\nSpeed: 3.7ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 2.8ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 10.0ms\nSpeed: 3.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 11.3ms\nSpeed: 3.1ms preprocess, 11.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 10.7ms\nSpeed: 3.4ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 9.2ms\nSpeed: 4.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 9.4ms\nSpeed: 3.2ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 9.4ms\nSpeed: 3.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 8.1ms\nSpeed: 3.6ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 10.4ms\nSpeed: 3.6ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 13.0ms\nSpeed: 3.2ms preprocess, 13.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 8.7ms\nSpeed: 3.4ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 1 suitcase, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 2 backpacks, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 2 suitcases, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 9.5ms\nSpeed: 3.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 3 suitcases, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 2 suitcases, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 handbag, 2 suitcases, 7.9ms\nSpeed: 3.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 1 suitcase, 8.6ms\nSpeed: 3.8ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 handbag, 1 suitcase, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 8.0ms\nSpeed: 3.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 9.2ms\nSpeed: 3.9ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 suitcases, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 3 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 3 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 3 suitcases, 7.8ms\nSpeed: 3.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 3 suitcases, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 3 suitcases, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 9.5ms\nSpeed: 3.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 8.9ms\nSpeed: 2.6ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.2ms\nSpeed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 9.9ms\nSpeed: 3.2ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 8.4ms\nSpeed: 3.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 8.0ms\nSpeed: 2.6ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 8.8ms\nSpeed: 3.6ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 9.8ms\nSpeed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.2ms\nSpeed: 2.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 9.4ms\nSpeed: 3.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 9.1ms\nSpeed: 3.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 3 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 3 suitcases, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 suitcases, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 suitcases, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 9.9ms\nSpeed: 3.2ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 2 suitcases, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 9.2ms\nSpeed: 3.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 1 suitcase, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 2 suitcases, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 9.0ms\nSpeed: 2.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 10.0ms\nSpeed: 3.3ms preprocess, 10.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 9.1ms\nSpeed: 3.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 10.7ms\nSpeed: 3.4ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 10.0ms\nSpeed: 3.7ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.6ms\nSpeed: 3.6ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 10.1ms\nSpeed: 3.3ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 8.6ms\nSpeed: 2.3ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 handbag, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 motorcycle, 1 backpack, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 motorcycle, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 backpacks, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 9.9ms\nSpeed: 3.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.3ms\nSpeed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 backpacks, 1 suitcase, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 backpacks, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 2 suitcases, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.2ms\nSpeed: 2.5ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.0ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 8.9ms\nSpeed: 2.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 1 suitcase, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 1 suitcase, 11.2ms\nSpeed: 3.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.5ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.0ms\nSpeed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.6ms\nSpeed: 3.8ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 9.8ms\nSpeed: 3.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.7ms\nSpeed: 3.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.2ms\nSpeed: 3.1ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.2ms\nSpeed: 2.9ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 9.7ms\nSpeed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 3.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.5ms\nSpeed: 3.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.5ms\nSpeed: 3.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.6ms\nSpeed: 3.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 8.0ms\nSpeed: 3.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.3ms\nSpeed: 3.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 1 backpack, 7.4ms\nSpeed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 1 backpack, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 1 backpack, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 1 backpack, 8.1ms\nSpeed: 3.8ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 backpack, 7.5ms\nSpeed: 3.1ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.9ms\nSpeed: 3.6ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 backpack, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.2ms\nSpeed: 2.1ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 3.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.8ms\nSpeed: 3.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 11.0ms\nSpeed: 2.9ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 suitcase, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.3ms\nSpeed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.4ms\nSpeed: 3.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 backpack, 7.6ms\nSpeed: 3.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.8ms\nSpeed: 3.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.6ms\nSpeed: 3.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 9.2ms\nSpeed: 3.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 7.4ms\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO\nfrom ultralytics.solutions import object_counter\nimport cv2\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm  \n\nmodel = YOLO(\"yolov8l.pt\")\n    \ncap = cv2.VideoCapture(\"/kaggle/input/pune-metro-hackathon/dataset/ticketing-crowd/1 (4).avi\")\nassert cap.isOpened(), \"Error reading video file\"\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get the total number of frames\n\n# Video writer\nvideo_writer = cv2.VideoWriter(\"/kaggle/working/Crowding.avi\",\n                               cv2.VideoWriter_fourcc(*'mp4v'),\n                               fps,\n                               (w, h))\nregion_points = [[\n                    270.68710384915494,\n                    832.9224757715328\n                ],\n                [\n                    1412.6219817768006,\n                    376.6059232869861\n                ],\n                [\n                    1144.8227883536629,\n                    275.9507796486947\n                ],\n                [\n                    194.71646080318862,\n                    563.374785355727\n                ],\n[\n                    1755.8332824707031,\n                    658.333330154419\n                ],\n                [\n                    1198.3688207484472,\n                    410.4562040438583\n                ],\n                [\n                    1412.7385654027764,\n                    309.42440099782146\n                ],\n                [\n                    1816.7820171599649,\n                    462.04134659038795\n                ],\n                [\n                    1753.4708226784344,\n                    657.1481308683869\n                ],\n    [\n                    518.0476563912373,\n                    183.17800685797798\n                ],\n                [\n                    643.5791581511816,\n                    253.5973026767949\n                ],\n                [\n                    793.4314328410121,\n                    219.8410211065073\n                ],\n                [\n                    658.1724149412855,\n                    159.89388610739655\n                ]\n]\n\n# Initialize Object Counter\ncounter = object_counter.ObjectCounter()\ncounter.set_args(view_img=True,\n                 reg_pts=region_points,  \n                 classes_names=model.names,\n                 draw_tracks=True,\n                 line_thickness=2)\n\n# Loop through each frame of the video with tqdm progress bar\nfor _ in tqdm(range(frame_count), desc=\"Processing Video\"):\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n    tracks = model.track(im0, persist=True, show=False)\n\n    im0 = counter.start_counting(im0, tracks)\n    video_writer.write(im0)\n\ncap.release()\nvideo_writer.release()\nprint(\"Video processing complete and the file has been saved.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:44:52.264526Z","iopub.execute_input":"2024-05-04T21:44:52.265059Z","iopub.status.idle":"2024-05-04T21:58:00.773263Z","shell.execute_reply.started":"2024-05-04T21:44:52.265028Z","shell.execute_reply":"2024-05-04T21:58:00.772146Z"},"scrolled":true,"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n\nPolygon Counter Initiated.\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 0/30753 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 24.8ms\nSpeed: 3.0ms preprocess, 24.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 1/30753 [00:01<9:24:02,  1.10s/it]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 24.8ms\nSpeed: 4.0ms preprocess, 24.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 2/30753 [00:01<4:24:03,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 25.0ms\nSpeed: 4.3ms preprocess, 25.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 24.6ms\nSpeed: 3.9ms preprocess, 24.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 4/30753 [00:01<2:09:57,  3.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 24.9ms\nSpeed: 3.3ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 25.6ms\nSpeed: 3.4ms preprocess, 25.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 6/30753 [00:01<1:31:17,  5.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 25.1ms\nSpeed: 3.1ms preprocess, 25.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 25.0ms\nSpeed: 3.1ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 8/30753 [00:01<1:14:01,  6.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 24.7ms\nSpeed: 3.3ms preprocess, 24.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 25.4ms\nSpeed: 2.9ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 10/30753 [00:01<1:04:46,  7.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 24.5ms\nSpeed: 4.8ms preprocess, 24.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 25.6ms\nSpeed: 3.1ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 12/30753 [00:02<59:26,  8.62it/s]  ","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 25.9ms\nSpeed: 3.5ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 25.4ms\nSpeed: 3.9ms preprocess, 25.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 14/30753 [00:02<56:58,  8.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 25.7ms\nSpeed: 3.9ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 24.7ms\nSpeed: 3.0ms preprocess, 24.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 16/30753 [00:02<54:28,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 24.6ms\nSpeed: 3.1ms preprocess, 24.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 25.5ms\nSpeed: 3.4ms preprocess, 25.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 18/30753 [00:02<52:31,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 25.1ms\nSpeed: 2.9ms preprocess, 25.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 20/30753 [00:02<51:34,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 26.1ms\nSpeed: 3.8ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 26.1ms\nSpeed: 4.2ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 22/30753 [00:03<51:19,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 24.9ms\nSpeed: 3.4ms preprocess, 24.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 24.8ms\nSpeed: 3.2ms preprocess, 24.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 24/30753 [00:03<50:37, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 25.2ms\nSpeed: 3.5ms preprocess, 25.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 25.8ms\nSpeed: 4.0ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 26/30753 [00:03<51:12, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 25.6ms\nSpeed: 3.8ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 26.0ms\nSpeed: 4.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 28/30753 [00:03<51:46,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 26.0ms\nSpeed: 4.2ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 29/30753 [00:03<51:56,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 30/30753 [00:03<51:50,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 31/30753 [00:04<51:42,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 32/30753 [00:04<51:38,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 34/30753 [00:04<51:11, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 35/30753 [00:04<51:17,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.5ms\nSpeed: 3.9ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 37/30753 [00:04<51:57,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 38/30753 [00:04<52:05,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 39/30753 [00:04<51:53,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 41/30753 [00:05<51:02, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 43/30753 [00:05<50:46, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 29.1ms\nSpeed: 4.4ms preprocess, 29.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.9ms\nSpeed: 4.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 45/30753 [00:05<50:40, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 47/30753 [00:05<50:33, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 49/30753 [00:05<51:22,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 50/30753 [00:05<51:29,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.0ms\nSpeed: 4.2ms preprocess, 26.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 51/30753 [00:06<52:53,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 52/30753 [00:06<53:15,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 53/30753 [00:06<53:48,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 55/30753 [00:06<53:32,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.5ms\nSpeed: 4.2ms preprocess, 26.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 56/30753 [00:06<54:35,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 57/30753 [00:06<53:45,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.4ms\nSpeed: 3.3ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 59/30753 [00:06<52:04,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 60/30753 [00:06<52:01,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 61/30753 [00:07<53:42,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 63/30753 [00:07<51:56,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 65/30753 [00:07<50:51, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 67/30753 [00:07<50:36, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 69/30753 [00:07<50:26, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 71/30753 [00:08<50:03, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 73/30753 [00:08<50:28, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 75/30753 [00:08<50:06, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 25.9ms\nSpeed: 3.9ms preprocess, 25.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 77/30753 [00:08<51:48,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 78/30753 [00:08<52:18,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 79/30753 [00:08<52:22,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 80/30753 [00:09<52:49,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 81/30753 [00:09<53:17,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 82/30753 [00:09<52:46,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 84/30753 [00:09<51:38,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 85/30753 [00:09<52:51,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 86/30753 [00:09<52:31,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 87/30753 [00:09<52:11,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 88/30753 [00:09<52:01,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 90/30753 [00:10<51:13,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 92/30753 [00:10<50:43, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 94/30753 [00:10<50:13, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 96/30753 [00:10<49:31, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 98/30753 [00:10<50:07, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 100/30753 [00:10<49:56, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 102/30753 [00:11<51:33,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 104/30753 [00:11<50:53, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 106/30753 [00:11<50:56, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 108/30753 [00:11<50:26, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.9ms\nSpeed: 4.0ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 110/30753 [00:12<51:26,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 112/30753 [00:12<51:01, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 114/30753 [00:12<50:29, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 116/30753 [00:12<50:15, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 118/30753 [00:12<50:17, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.1ms\nSpeed: 3.9ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 120/30753 [00:12<50:03, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.4ms\nSpeed: 4.5ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 122/30753 [00:13<51:10,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 124/30753 [00:13<50:52, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.8ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 126/30753 [00:13<50:35, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.6ms\nSpeed: 4.2ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 128/30753 [00:13<50:55, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 130/30753 [00:13<50:05, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.4ms\nSpeed: 3.7ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 132/30753 [00:14<49:22, 10.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 134/30753 [00:14<49:58, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 136/30753 [00:14<49:26, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.7ms\nSpeed: 3.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 138/30753 [00:14<49:08, 10.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 140/30753 [00:14<48:52, 10.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.4ms\nSpeed: 3.6ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 142/30753 [00:15<48:37, 10.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 144/30753 [00:15<48:32, 10.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.6ms\nSpeed: 3.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 146/30753 [00:15<49:20, 10.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 5.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 148/30753 [00:15<49:03, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 150/30753 [00:15<49:03, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.3ms\nSpeed: 4.2ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   0%|          | 152/30753 [00:16<50:50, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 154/30753 [00:16<50:22, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 156/30753 [00:16<51:17,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 25.4ms\nSpeed: 4.6ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 157/30753 [00:16<51:38,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 159/30753 [00:16<50:14, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.3ms\nSpeed: 2.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 161/30753 [00:16<49:37, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 25.8ms\nSpeed: 3.2ms preprocess, 25.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 163/30753 [00:17<48:53, 10.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 165/30753 [00:17<48:29, 10.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 167/30753 [00:17<48:14, 10.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 169/30753 [00:17<49:08, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 171/30753 [00:17<49:21, 10.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 173/30753 [00:18<49:03, 10.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 175/30753 [00:18<48:40, 10.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 3.0ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 177/30753 [00:18<49:00, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 179/30753 [00:18<48:46, 10.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 181/30753 [00:18<49:16, 10.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 183/30753 [00:19<49:01, 10.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.2ms\nSpeed: 3.1ms preprocess, 26.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 185/30753 [00:19<48:26, 10.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 187/30753 [00:19<48:15, 10.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 30.6ms\nSpeed: 3.3ms preprocess, 30.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 189/30753 [00:19<48:18, 10.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 191/30753 [00:19<48:11, 10.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 193/30753 [00:20<48:59, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 195/30753 [00:20<49:05, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.5ms\nSpeed: 3.3ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 197/30753 [00:20<48:54, 10.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 199/30753 [00:20<48:42, 10.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.5ms\nSpeed: 3.6ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 201/30753 [00:20<49:35, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 203/30753 [00:21<49:58, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 205/30753 [00:21<50:32, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 207/30753 [00:21<49:49, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 209/30753 [00:21<49:13, 10.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 211/30753 [00:21<48:54, 10.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.6ms\nSpeed: 3.7ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 213/30753 [00:21<48:18, 10.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 215/30753 [00:22<48:14, 10.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 217/30753 [00:22<48:43, 10.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.0ms\nSpeed: 3.4ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 219/30753 [00:22<48:35, 10.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 221/30753 [00:22<48:14, 10.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 223/30753 [00:22<47:58, 10.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 225/30753 [00:23<47:45, 10.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.0ms\nSpeed: 3.8ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 227/30753 [00:23<48:09, 10.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 229/30753 [00:23<48:27, 10.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 231/30753 [00:23<48:05, 10.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 233/30753 [00:23<47:45, 10.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 235/30753 [00:24<47:28, 10.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 237/30753 [00:24<47:24, 10.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 239/30753 [00:24<47:18, 10.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.4ms\nSpeed: 3.9ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 241/30753 [00:24<48:01, 10.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 30.1ms\nSpeed: 3.1ms preprocess, 30.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 243/30753 [00:24<48:00, 10.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.5ms\nSpeed: 3.7ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.4ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 245/30753 [00:24<48:02, 10.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.3ms\nSpeed: 3.1ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.6ms\nSpeed: 3.2ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 247/30753 [00:25<47:51, 10.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 249/30753 [00:25<47:47, 10.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.3ms\nSpeed: 3.3ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 251/30753 [00:25<48:54, 10.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 29.1ms\nSpeed: 4.0ms preprocess, 29.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 253/30753 [00:25<50:12, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 255/30753 [00:25<50:14, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 257/30753 [00:26<49:44, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 259/30753 [00:26<49:25, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 24.1ms\nSpeed: 4.2ms preprocess, 24.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 261/30753 [00:26<50:04, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.5ms\nSpeed: 4.1ms preprocess, 26.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 263/30753 [00:26<49:25, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 265/30753 [00:26<50:24, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 4.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 267/30753 [00:27<50:00, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.3ms\nSpeed: 4.5ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 269/30753 [00:27<49:42, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 2.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 271/30753 [00:27<49:05, 10.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 273/30753 [00:27<48:57, 10.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 275/30753 [00:27<49:06, 10.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 277/30753 [00:28<49:27, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.5ms\nSpeed: 4.2ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 279/30753 [00:28<48:53, 10.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 281/30753 [00:28<48:49, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 283/30753 [00:28<48:32, 10.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 285/30753 [00:28<48:23, 10.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 287/30753 [00:29<48:12, 10.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 289/30753 [00:29<48:59, 10.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 291/30753 [00:29<48:35, 10.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 293/30753 [00:29<48:42, 10.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 295/30753 [00:29<48:25, 10.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 297/30753 [00:30<48:10, 10.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 25.7ms\nSpeed: 4.3ms preprocess, 25.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 299/30753 [00:30<48:01, 10.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 301/30753 [00:30<48:48, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 303/30753 [00:30<49:07, 10.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.7ms\nSpeed: 3.4ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 305/30753 [00:30<49:28, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.2ms\nSpeed: 3.2ms preprocess, 26.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 307/30753 [00:30<48:54, 10.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 309/30753 [00:31<48:43, 10.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 311/30753 [00:31<49:17, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 313/30753 [00:31<49:51, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.6ms\nSpeed: 3.4ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 315/30753 [00:31<49:37, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 317/30753 [00:31<49:29, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 319/30753 [00:32<49:23, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 321/30753 [00:32<49:23, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.6ms\nSpeed: 3.2ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 323/30753 [00:32<48:42, 10.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 325/30753 [00:32<49:18, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 2.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 327/30753 [00:32<49:20, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 329/30753 [00:33<49:02, 10.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 331/30753 [00:33<48:40, 10.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 333/30753 [00:33<48:10, 10.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 335/30753 [00:33<47:42, 10.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 337/30753 [00:33<48:11, 10.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 339/30753 [00:34<48:01, 10.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 341/30753 [00:34<47:35, 10.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 343/30753 [00:34<47:36, 10.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 345/30753 [00:34<47:51, 10.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 347/30753 [00:34<47:51, 10.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 2.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 349/30753 [00:35<48:44, 10.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 351/30753 [00:35<49:42, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 353/30753 [00:35<49:52, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 355/30753 [00:35<49:42, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.9ms\nSpeed: 4.6ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 357/30753 [00:35<49:15, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 359/30753 [00:36<49:07, 10.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 361/30753 [00:36<49:56, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 363/30753 [00:36<49:41, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.6ms\nSpeed: 3.4ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 365/30753 [00:36<51:20,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 367/30753 [00:36<50:28, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.5ms\nSpeed: 3.4ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 369/30753 [00:37<49:34, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 371/30753 [00:37<49:07, 10.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.5ms\nSpeed: 3.9ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 373/30753 [00:37<49:36, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.5ms\nSpeed: 3.1ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 375/30753 [00:37<49:22, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 377/30753 [00:37<49:20, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 379/30753 [00:37<48:44, 10.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.3ms\nSpeed: 3.9ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 381/30753 [00:38<48:29, 10.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|          | 383/30753 [00:38<48:27, 10.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 385/30753 [00:38<49:07, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 387/30753 [00:38<49:19, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 389/30753 [00:38<49:12, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 391/30753 [00:39<49:16, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 393/30753 [00:39<49:10, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 29.6ms\nSpeed: 4.3ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 395/30753 [00:39<49:09, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 4.6ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 397/30753 [00:39<49:40, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.6ms\nSpeed: 4.7ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 399/30753 [00:39<50:30, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 401/30753 [00:40<51:38,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 4.4ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 402/30753 [00:40<53:25,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 403/30753 [00:40<53:43,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 25.5ms\nSpeed: 3.4ms preprocess, 25.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 405/30753 [00:40<52:58,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 406/30753 [00:40<52:34,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.0ms\nSpeed: 4.3ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 408/30753 [00:40<51:29,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 409/30753 [00:40<52:11,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 410/30753 [00:41<52:36,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.0ms\nSpeed: 3.2ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 412/30753 [00:41<51:20,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 414/30753 [00:41<50:07, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 416/30753 [00:41<49:21, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.7ms\nSpeed: 4.2ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 418/30753 [00:41<49:23, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 420/30753 [00:42<49:15, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 422/30753 [00:42<49:49, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.8ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 424/30753 [00:42<49:26, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 426/30753 [00:42<49:01, 10.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 428/30753 [00:42<49:31, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 430/30753 [00:43<49:05, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 432/30753 [00:43<48:46, 10.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 434/30753 [00:43<49:16, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 436/30753 [00:43<48:40, 10.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 438/30753 [00:43<48:14, 10.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 440/30753 [00:43<47:47, 10.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.5ms\nSpeed: 3.7ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 442/30753 [00:44<47:37, 10.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 444/30753 [00:44<47:59, 10.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 446/30753 [00:44<48:45, 10.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.5ms\nSpeed: 3.3ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 448/30753 [00:44<48:19, 10.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 450/30753 [00:44<48:09, 10.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 452/30753 [00:45<49:52, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 454/30753 [00:45<50:06, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 456/30753 [00:45<49:25, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 458/30753 [00:45<49:51, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   1%|▏         | 460/30753 [00:45<49:07, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 462/30753 [00:46<48:41, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 26.3ms\nSpeed: 3.2ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 464/30753 [00:46<48:33, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 466/30753 [00:46<49:52, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 25.6ms\nSpeed: 4.1ms preprocess, 25.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 468/30753 [00:46<49:25, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 4.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 470/30753 [00:46<50:16, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 472/30753 [00:47<49:28, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 7 persons, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 clock, 25.8ms\nSpeed: 3.4ms preprocess, 25.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 474/30753 [00:47<48:57, 10.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 7 persons, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 476/30753 [00:47<48:30, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 7 persons, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 1 clock, 25.8ms\nSpeed: 4.2ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 478/30753 [00:47<48:37, 10.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 7 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 480/30753 [00:47<48:06, 10.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 482/30753 [00:48<48:38, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 8 persons, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 484/30753 [00:48<48:44, 10.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 486/30753 [00:48<48:54, 10.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 488/30753 [00:48<49:02, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 490/30753 [00:48<48:52, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 492/30753 [00:49<48:50, 10.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 494/30753 [00:49<49:37, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 496/30753 [00:49<49:16, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 498/30753 [00:49<48:54, 10.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.0ms\nSpeed: 4.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 500/30753 [00:49<49:04, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 502/30753 [00:50<50:33,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 504/30753 [00:50<49:54, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 506/30753 [00:50<50:40,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 508/30753 [00:50<49:56, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 510/30753 [00:50<49:37, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.7ms\nSpeed: 3.6ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 512/30753 [00:51<49:20, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 28.7ms\nSpeed: 4.0ms preprocess, 28.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 514/30753 [00:51<49:20, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 516/30753 [00:51<49:03, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 518/30753 [00:51<49:49, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 26.7ms\nSpeed: 3.5ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 520/30753 [00:51<49:32, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 522/30753 [00:51<48:55, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.6ms\nSpeed: 3.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 524/30753 [00:52<48:24, 10.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 526/30753 [00:52<48:21, 10.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 528/30753 [00:52<48:55, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 530/30753 [00:52<49:39, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 532/30753 [00:52<49:03, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.3ms\nSpeed: 3.3ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 534/30753 [00:53<48:25, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 536/30753 [00:53<48:16, 10.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 538/30753 [00:53<47:58, 10.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 540/30753 [00:53<48:01, 10.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 542/30753 [00:53<49:13, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 544/30753 [00:54<49:21, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 546/30753 [00:54<49:38, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 548/30753 [00:54<49:25, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 550/30753 [00:54<49:23, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 552/30753 [00:54<50:42,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 553/30753 [00:55<51:13,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 555/30753 [00:55<50:50,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 557/30753 [00:55<50:11, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 559/30753 [00:55<49:59, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 561/30753 [00:55<50:15, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 563/30753 [00:56<49:45, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 565/30753 [00:56<50:23,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 566/30753 [00:56<51:07,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.4ms preprocess, 26.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 568/30753 [00:56<51:55,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 569/30753 [00:56<51:39,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 571/30753 [00:56<50:49,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 573/30753 [00:57<50:31,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 575/30753 [00:57<50:14, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 577/30753 [00:57<50:41,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 578/30753 [00:57<50:42,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 580/30753 [00:57<50:13, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 581/30753 [00:57<50:17, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 582/30753 [00:57<50:18, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 583/30753 [00:58<50:20,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 584/30753 [00:58<50:33,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 586/30753 [00:58<49:53, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 588/30753 [00:58<49:39, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 590/30753 [00:58<50:52,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 592/30753 [00:58<49:53, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 594/30753 [00:59<49:12, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 596/30753 [00:59<48:59, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 598/30753 [00:59<48:38, 10.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 26.4ms\nSpeed: 3.0ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 600/30753 [00:59<48:27, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 602/30753 [00:59<49:29, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 604/30753 [01:00<48:55, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 606/30753 [01:00<49:06, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 28.6ms\nSpeed: 3.2ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 608/30753 [01:00<48:55, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 610/30753 [01:00<48:52, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 612/30753 [01:00<49:03, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 614/30753 [01:01<50:06, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 616/30753 [01:01<50:06, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 618/30753 [01:01<49:46, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 620/30753 [01:01<49:22, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 622/30753 [01:01<49:18, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 624/30753 [01:02<48:57, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 626/30753 [01:02<49:43, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 628/30753 [01:02<50:18,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 1 clock, 29.3ms\nSpeed: 4.0ms preprocess, 29.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 629/30753 [01:02<50:17,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 631/30753 [01:02<50:04, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 1 backpack, 1 suitcase, 1 clock, 26.3ms\nSpeed: 3.0ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 633/30753 [01:02<49:26, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 28.8ms\nSpeed: 3.0ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 635/30753 [01:03<49:33, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 637/30753 [01:03<50:23,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 638/30753 [01:03<50:22,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 639/30753 [01:03<50:24,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 640/30753 [01:03<50:29,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 641/30753 [01:03<50:30,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 642/30753 [01:03<50:33,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 643/30753 [01:03<50:40,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.5ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 644/30753 [01:04<50:50,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 645/30753 [01:04<50:43,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 26.3ms\nSpeed: 4.1ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 647/30753 [01:04<50:15,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 26.1ms\nSpeed: 4.2ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 648/30753 [01:04<50:15,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 649/30753 [01:04<51:21,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.8ms\nSpeed: 3.4ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 650/30753 [01:04<51:17,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 651/30753 [01:04<52:50,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 652/30753 [01:04<53:11,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 653/30753 [01:05<52:41,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 655/30753 [01:05<52:03,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 656/30753 [01:05<52:01,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 658/30753 [01:05<51:01,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 659/30753 [01:05<50:58,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 660/30753 [01:05<51:23,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 661/30753 [01:05<52:22,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.9ms\nSpeed: 3.3ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 662/30753 [01:05<51:56,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 663/30753 [01:06<51:27,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 665/30753 [01:06<50:42,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.0ms\nSpeed: 3.1ms preprocess, 26.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 666/30753 [01:06<50:38,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 667/30753 [01:06<50:53,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 25.2ms\nSpeed: 3.4ms preprocess, 25.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 668/30753 [01:06<52:45,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 669/30753 [01:06<52:27,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 670/30753 [01:06<51:57,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 671/30753 [01:06<52:02,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 3.1ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 673/30753 [01:07<51:51,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 674/30753 [01:07<51:27,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 676/30753 [01:07<50:54,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 677/30753 [01:07<51:28,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 679/30753 [01:07<50:42,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 680/30753 [01:07<50:36,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 682/30753 [01:07<49:47, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 684/30753 [01:08<49:46, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 686/30753 [01:08<50:37,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 687/30753 [01:08<50:36,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 688/30753 [01:08<50:38,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 689/30753 [01:08<50:49,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.7ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 690/30753 [01:08<50:58,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 5.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 691/30753 [01:08<50:57,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 693/30753 [01:09<50:16,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 695/30753 [01:09<49:48, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 697/30753 [01:09<50:29,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 699/30753 [01:09<50:13,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 700/30753 [01:09<50:18,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 701/30753 [01:09<51:49,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 702/30753 [01:09<51:56,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 703/30753 [01:10<51:33,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 705/30753 [01:10<51:14,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 3.9ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 707/30753 [01:10<50:19,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 709/30753 [01:10<50:48,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.8ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 710/30753 [01:10<51:00,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 711/30753 [01:10<50:59,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 712/30753 [01:11<50:52,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 713/30753 [01:11<50:53,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 715/30753 [01:11<52:09,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 716/30753 [01:11<55:19,  9.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 717/30753 [01:11<56:33,  8.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 718/30753 [01:11<55:38,  9.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 25.9ms\nSpeed: 4.4ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 719/30753 [01:11<56:33,  8.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 720/30753 [01:11<56:13,  8.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 25.6ms\nSpeed: 4.3ms preprocess, 25.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 721/30753 [01:12<56:24,  8.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 4.8ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 722/30753 [01:12<55:25,  9.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 723/30753 [01:12<55:15,  9.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 25.9ms\nSpeed: 4.7ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 724/30753 [01:12<54:04,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 725/30753 [01:12<53:35,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 726/30753 [01:12<53:12,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 727/30753 [01:12<53:38,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 728/30753 [01:12<53:06,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 729/30753 [01:12<52:25,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 730/30753 [01:12<51:54,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 731/30753 [01:13<51:43,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 732/30753 [01:13<51:21,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 733/30753 [01:13<52:21,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 734/30753 [01:13<52:21,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.6ms\nSpeed: 4.2ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 735/30753 [01:13<52:08,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 736/30753 [01:13<51:44,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 737/30753 [01:13<51:31,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 738/30753 [01:13<51:21,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 739/30753 [01:13<51:13,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 740/30753 [01:13<51:12,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 741/30753 [01:14<51:15,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 742/30753 [01:14<51:22,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 743/30753 [01:14<51:27,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 25.8ms\nSpeed: 3.9ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 744/30753 [01:14<51:09,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 745/30753 [01:14<52:18,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 746/30753 [01:14<51:58,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 747/30753 [01:14<51:40,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 2.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 749/30753 [01:14<50:57,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 29.3ms\nSpeed: 3.4ms preprocess, 29.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 750/30753 [01:15<51:09,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 751/30753 [01:15<52:35,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 752/30753 [01:15<53:10,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.6ms\nSpeed: 3.6ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 753/30753 [01:15<52:47,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 754/30753 [01:15<52:04,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.7ms\nSpeed: 3.8ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 755/30753 [01:15<52:57,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 756/30753 [01:15<52:40,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 757/30753 [01:15<53:43,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 758/30753 [01:15<53:20,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 4.0ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 759/30753 [01:15<52:44,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 760/30753 [01:16<52:59,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 761/30753 [01:16<52:32,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.1ms\nSpeed: 5.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 762/30753 [01:16<52:55,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 763/30753 [01:16<53:03,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 764/30753 [01:16<56:13,  8.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 765/30753 [01:16<54:57,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 766/30753 [01:16<53:36,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 31.0ms\nSpeed: 3.8ms preprocess, 31.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 767/30753 [01:16<53:37,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   2%|▏         | 768/30753 [01:16<53:15,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 769/30753 [01:17<54:28,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 770/30753 [01:17<53:52,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.3ms\nSpeed: 3.4ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 771/30753 [01:17<53:16,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 772/30753 [01:17<53:06,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 773/30753 [01:17<52:52,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 774/30753 [01:17<52:23,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 775/30753 [01:17<52:23,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 776/30753 [01:17<52:18,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 777/30753 [01:17<53:11,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 778/30753 [01:18<52:44,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 779/30753 [01:18<52:08,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 2 suitcases, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 780/30753 [01:18<52:11,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 2 suitcases, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 781/30753 [01:18<53:42,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 2 suitcases, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 782/30753 [01:18<53:15,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 783/30753 [01:18<52:44,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 784/30753 [01:18<52:17,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 785/30753 [01:18<52:30,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 786/30753 [01:18<52:02,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 787/30753 [01:18<51:55,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 788/30753 [01:19<52:10,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 789/30753 [01:19<52:13,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 790/30753 [01:19<52:10,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 791/30753 [01:19<52:33,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 792/30753 [01:19<52:17,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.8ms\nSpeed: 4.2ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 793/30753 [01:19<54:07,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 794/30753 [01:19<53:39,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 795/30753 [01:19<53:01,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 796/30753 [01:19<53:00,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 797/30753 [01:20<53:04,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 798/30753 [01:20<52:52,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 799/30753 [01:20<52:32,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 800/30753 [01:20<52:18,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 801/30753 [01:20<53:47,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 802/30753 [01:20<54:39,  9.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 803/30753 [01:20<54:01,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 804/30753 [01:20<53:19,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 805/30753 [01:20<54:28,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.9ms\nSpeed: 4.2ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 806/30753 [01:20<54:09,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 807/30753 [01:21<53:14,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 808/30753 [01:21<52:49,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 809/30753 [01:21<52:27,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 810/30753 [01:21<53:07,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 811/30753 [01:21<52:33,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 2 suitcases, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 2.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 813/30753 [01:21<50:50,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 814/30753 [01:21<50:52,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.8ms\nSpeed: 3.5ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 815/30753 [01:21<51:10,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 816/30753 [01:22<51:06,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 817/30753 [01:22<52:21,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 818/30753 [01:22<52:38,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 819/30753 [01:22<52:22,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.3ms\nSpeed: 3.8ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 820/30753 [01:22<52:07,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 822/30753 [01:22<51:03,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 823/30753 [01:22<51:04,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 824/30753 [01:22<50:59,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 825/30753 [01:22<50:42,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 826/30753 [01:23<50:32,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 827/30753 [01:23<51:30,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 828/30753 [01:23<51:04,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 829/30753 [01:23<52:20,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 830/30753 [01:23<51:45,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 832/30753 [01:23<50:54,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 833/30753 [01:23<51:05,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 834/30753 [01:23<51:11,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.5ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 835/30753 [01:23<51:14,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 836/30753 [01:24<51:14,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 837/30753 [01:24<51:00,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 838/30753 [01:24<51:04,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 839/30753 [01:24<51:06,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 840/30753 [01:24<51:12,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 841/30753 [01:24<52:12,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 842/30753 [01:24<51:47,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 844/30753 [01:24<50:34,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 846/30753 [01:25<50:10,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 2 suitcases, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 847/30753 [01:25<50:21,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 848/30753 [01:25<50:37,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 849/30753 [01:25<50:58,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 850/30753 [01:25<51:09,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.9ms\nSpeed: 3.9ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 851/30753 [01:25<53:27,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 852/30753 [01:25<54:05,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 853/30753 [01:25<54:41,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 854/30753 [01:25<53:35,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 855/30753 [01:26<53:50,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 856/30753 [01:26<53:14,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.0ms\nSpeed: 4.2ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 857/30753 [01:26<53:12,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 858/30753 [01:26<52:48,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 859/30753 [01:26<54:18,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 25.8ms\nSpeed: 4.0ms preprocess, 25.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 860/30753 [01:26<54:29,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 862/30753 [01:26<52:19,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 863/30753 [01:26<51:43,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 864/30753 [01:27<51:27,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 865/30753 [01:27<52:48,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 866/30753 [01:27<52:42,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 867/30753 [01:27<52:41,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 868/30753 [01:27<52:34,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 869/30753 [01:27<52:08,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 870/30753 [01:27<52:21,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 871/30753 [01:27<51:50,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.7ms\nSpeed: 4.8ms preprocess, 26.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 872/30753 [01:27<52:17,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 873/30753 [01:27<51:51,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 874/30753 [01:28<51:37,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 875/30753 [01:28<51:23,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 877/30753 [01:28<51:51,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 878/30753 [01:28<52:12,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 879/30753 [01:28<51:48,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 880/30753 [01:28<51:42,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 881/30753 [01:28<51:29,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 882/30753 [01:28<51:16,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.6ms\nSpeed: 4.2ms preprocess, 28.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 883/30753 [01:28<51:42,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 884/30753 [01:29<51:41,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 885/30753 [01:29<51:41,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 2.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 886/30753 [01:29<51:14,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 887/30753 [01:29<51:32,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 888/30753 [01:29<51:25,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 889/30753 [01:29<52:54,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 890/30753 [01:29<52:36,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 891/30753 [01:29<52:01,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 892/30753 [01:29<51:31,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.8ms\nSpeed: 4.6ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 893/30753 [01:30<51:55,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 894/30753 [01:30<51:21,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 895/30753 [01:30<50:55,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 897/30753 [01:30<50:29,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.3ms\nSpeed: 3.0ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 899/30753 [01:30<50:07,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 900/30753 [01:30<50:07,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 901/30753 [01:30<51:45,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 902/30753 [01:30<52:11,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 903/30753 [01:31<51:49,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 904/30753 [01:31<51:31,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 905/30753 [01:31<52:31,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 907/30753 [01:31<51:06,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 908/30753 [01:31<51:19,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 909/30753 [01:31<51:33,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 910/30753 [01:31<52:28,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 2 suitcases, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 911/30753 [01:31<52:10,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 912/30753 [01:32<51:46,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.9ms\nSpeed: 3.6ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 913/30753 [01:32<53:05,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.8ms\nSpeed: 3.3ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 914/30753 [01:32<53:01,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 915/30753 [01:32<52:25,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 916/30753 [01:32<52:11,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.5ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 917/30753 [01:32<51:55,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 918/30753 [01:32<51:53,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 4.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 919/30753 [01:32<51:44,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 920/30753 [01:32<51:32,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.8ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 921/30753 [01:32<51:46,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 922/30753 [01:33<51:35,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 923/30753 [01:33<51:52,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 924/30753 [01:33<51:40,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 925/30753 [01:33<52:48,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 926/30753 [01:33<52:48,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 927/30753 [01:33<53:41,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 928/30753 [01:33<52:31,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 929/30753 [01:33<52:04,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 930/30753 [01:33<51:42,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 931/30753 [01:33<51:26,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.5ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 932/30753 [01:34<51:33,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 933/30753 [01:34<51:35,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 934/30753 [01:34<51:51,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 935/30753 [01:34<52:11,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 936/30753 [01:34<52:10,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 937/30753 [01:34<53:43,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 938/30753 [01:34<53:07,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 939/30753 [01:34<52:40,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 940/30753 [01:34<52:24,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 941/30753 [01:35<52:28,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 942/30753 [01:35<52:12,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 943/30753 [01:35<52:19,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 944/30753 [01:35<52:16,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 945/30753 [01:35<52:21,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 946/30753 [01:35<52:10,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 947/30753 [01:35<52:09,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 948/30753 [01:35<52:09,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 949/30753 [01:35<53:21,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 950/30753 [01:36<52:47,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 suitcase, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 951/30753 [01:36<54:16,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 952/30753 [01:36<54:26,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 26.4ms\nSpeed: 4.0ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 953/30753 [01:36<53:10,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 954/30753 [01:36<52:35,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 suitcase, 1 clock, 25.8ms\nSpeed: 6.6ms preprocess, 25.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 955/30753 [01:36<54:08,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 956/30753 [01:36<54:22,  9.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 957/30753 [01:36<53:44,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 958/30753 [01:36<53:06,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 959/30753 [01:36<52:36,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 960/30753 [01:37<52:09,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 suitcases, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 961/30753 [01:37<53:11,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 suitcases, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 962/30753 [01:37<52:11,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 suitcases, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 963/30753 [01:37<51:47,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 964/30753 [01:37<51:20,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 suitcases, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 966/30753 [01:37<50:07,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 967/30753 [01:37<50:09,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 968/30753 [01:37<50:12,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 969/30753 [01:37<50:08,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 970/30753 [01:38<50:19,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 971/30753 [01:38<50:34,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 972/30753 [01:38<50:32,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 973/30753 [01:38<52:09,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 974/30753 [01:38<52:15,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 975/30753 [01:38<51:34,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 976/30753 [01:38<51:16,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 977/30753 [01:38<52:20,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 29.1ms\nSpeed: 3.5ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 978/30753 [01:38<52:10,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 979/30753 [01:39<51:36,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 980/30753 [01:39<51:06,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 982/30753 [01:39<49:58,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 983/30753 [01:39<50:07,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 985/30753 [01:39<50:28,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.9ms\nSpeed: 3.6ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 986/30753 [01:39<50:41,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 987/30753 [01:39<50:35,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 30.2ms\nSpeed: 3.7ms preprocess, 30.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 988/30753 [01:39<50:54,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 989/30753 [01:40<50:34,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 990/30753 [01:40<50:30,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 992/30753 [01:40<50:00,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 suitcase, 1 clock, 31.0ms\nSpeed: 3.8ms preprocess, 31.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 993/30753 [01:40<50:05,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 suitcase, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 995/30753 [01:40<49:19, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 996/30753 [01:40<49:25, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 4.2ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 997/30753 [01:40<50:50,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 998/30753 [01:40<51:06,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 999/30753 [01:41<51:02,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1000/30753 [01:41<50:40,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1001/30753 [01:41<52:40,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.9ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1002/30753 [01:41<53:11,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 4.0ms preprocess, 28.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1003/30753 [01:41<52:46,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1004/30753 [01:41<51:55,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1005/30753 [01:41<52:27,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 29.2ms\nSpeed: 3.6ms preprocess, 29.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1006/30753 [01:41<51:50,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1008/30753 [01:42<50:26,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1009/30753 [01:42<51:10,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1010/30753 [01:42<51:11,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1012/30753 [01:42<50:25,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 4.4ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1013/30753 [01:42<50:30,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1014/30753 [01:42<52:35,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 3.2ms preprocess, 25.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1015/30753 [01:42<52:26,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 5.0ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1016/30753 [01:42<52:17,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.1ms\nSpeed: 3.2ms preprocess, 26.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1017/30753 [01:42<53:12,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1018/30753 [01:43<53:14,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1019/30753 [01:43<53:22,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1020/30753 [01:43<52:45,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1021/30753 [01:43<53:44,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1022/30753 [01:43<53:17,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1023/30753 [01:43<52:15,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 25.4ms\nSpeed: 4.2ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1024/30753 [01:43<51:43,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1025/30753 [01:43<51:50,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1026/30753 [01:43<51:33,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1027/30753 [01:44<52:23,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1028/30753 [01:44<51:56,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1029/30753 [01:44<51:13,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1031/30753 [01:44<50:15,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1033/30753 [01:44<50:42,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1034/30753 [01:44<50:29,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1036/30753 [01:44<49:58,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1037/30753 [01:45<50:02,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1038/30753 [01:45<50:02,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1039/30753 [01:45<50:11,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1040/30753 [01:45<50:14,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.7ms\nSpeed: 4.4ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1042/30753 [01:45<49:45,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1043/30753 [01:45<49:49,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1044/30753 [01:45<50:01,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1045/30753 [01:45<51:50,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1046/30753 [01:45<51:35,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1047/30753 [01:46<51:13,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1048/30753 [01:46<50:45,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.5ms\nSpeed: 4.3ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1049/30753 [01:46<51:22,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1050/30753 [01:46<51:01,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.5ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1051/30753 [01:46<52:51,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 24.3ms\nSpeed: 4.9ms preprocess, 24.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1052/30753 [01:46<53:20,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 29.0ms\nSpeed: 3.1ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1054/30753 [01:46<51:08,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1055/30753 [01:46<50:48,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1057/30753 [01:47<50:54,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1058/30753 [01:47<50:38,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1059/30753 [01:47<50:24,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1060/30753 [01:47<50:17,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.9ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1061/30753 [01:47<50:31,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.5ms preprocess, 28.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1062/30753 [01:47<50:28,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1063/30753 [01:47<50:29,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1064/30753 [01:47<50:24,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1065/30753 [01:47<50:14,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1067/30753 [01:48<49:41,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1069/30753 [01:48<50:26,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1070/30753 [01:48<50:25,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1071/30753 [01:48<50:41,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1073/30753 [01:48<49:46,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   3%|▎         | 1075/30753 [01:48<49:23, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1077/30753 [01:49<49:49,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1078/30753 [01:49<49:58,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1079/30753 [01:49<50:08,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1080/30753 [01:49<50:15,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1081/30753 [01:49<52:13,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1082/30753 [01:49<51:45,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1083/30753 [01:49<51:25,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1084/30753 [01:49<51:17,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1085/30753 [01:49<51:32,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.8ms\nSpeed: 3.3ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1086/30753 [01:50<51:14,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1087/30753 [01:50<51:11,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1088/30753 [01:50<50:55,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1089/30753 [01:50<51:03,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1090/30753 [01:50<50:52,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1091/30753 [01:50<51:10,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1092/30753 [01:50<51:10,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1093/30753 [01:50<52:23,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1094/30753 [01:50<51:37,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1096/30753 [01:51<50:30,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1097/30753 [01:51<50:20,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1099/30753 [01:51<49:21, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1101/30753 [01:51<50:11,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1102/30753 [01:51<50:44,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1104/30753 [01:51<49:26,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1105/30753 [01:51<50:09,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1106/30753 [01:52<50:02,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1108/30753 [01:52<49:05, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1110/30753 [01:52<49:26,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1112/30753 [01:52<48:53, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1114/30753 [01:52<49:02, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1116/30753 [01:53<48:57, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.8ms\nSpeed: 4.1ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1118/30753 [01:53<49:44,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1120/30753 [01:53<49:05, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1122/30753 [01:53<48:50, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1124/30753 [01:53<48:44, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1126/30753 [01:54<48:42, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1128/30753 [01:54<49:20, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1130/30753 [01:54<50:30,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1132/30753 [01:54<49:28,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1134/30753 [01:54<49:07, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1136/30753 [01:55<49:02, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1138/30753 [01:55<49:19, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1140/30753 [01:55<50:10,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1141/30753 [01:55<51:07,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1142/30753 [01:55<51:02,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1143/30753 [01:55<51:05,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1144/30753 [01:55<50:56,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1145/30753 [01:56<50:50,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1146/30753 [01:56<50:47,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1148/30753 [01:56<50:44,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1150/30753 [01:56<51:57,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1151/30753 [01:56<52:59,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1152/30753 [01:56<53:14,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▎         | 1153/30753 [01:56<53:32,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1154/30753 [01:56<52:52,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1155/30753 [01:57<52:50,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1156/30753 [01:57<51:55,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1157/30753 [01:57<51:16,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.9ms\nSpeed: 3.3ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1159/30753 [01:57<50:04,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1160/30753 [01:57<50:38,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 28.8ms\nSpeed: 4.0ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1162/30753 [01:57<50:00,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1163/30753 [01:57<50:02,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1164/30753 [01:57<50:19,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1165/30753 [01:58<51:33,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1166/30753 [01:58<51:19,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1167/30753 [01:58<50:59,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1168/30753 [01:58<50:55,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1169/30753 [01:58<51:05,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1170/30753 [01:58<50:35,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1171/30753 [01:58<50:18,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.0ms preprocess, 28.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1173/30753 [01:58<49:17, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1175/30753 [01:59<49:17, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1176/30753 [01:59<49:20,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1177/30753 [01:59<50:58,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1178/30753 [01:59<50:58,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1179/30753 [01:59<51:00,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1180/30753 [01:59<51:07,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1181/30753 [01:59<51:27,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.8ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1182/30753 [01:59<51:01,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1183/30753 [01:59<50:54,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1184/30753 [02:00<50:48,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1185/30753 [02:00<50:45,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 5.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1186/30753 [02:00<51:03,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1187/30753 [02:00<50:55,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 29.5ms\nSpeed: 4.4ms preprocess, 29.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1188/30753 [02:00<51:23,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1189/30753 [02:00<53:01,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1190/30753 [02:00<52:01,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1191/30753 [02:00<51:34,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1192/30753 [02:00<51:05,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1193/30753 [02:00<51:08,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1194/30753 [02:01<51:03,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1195/30753 [02:01<50:51,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1197/30753 [02:01<50:05,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.1ms\nSpeed: 4.2ms preprocess, 26.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1198/30753 [02:01<49:59,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1199/30753 [02:01<50:19,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1200/30753 [02:01<50:19,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1201/30753 [02:01<51:48,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1202/30753 [02:01<52:41,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1203/30753 [02:02<51:59,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1204/30753 [02:02<51:14,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1205/30753 [02:02<52:16,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1206/30753 [02:02<51:26,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1208/30753 [02:02<50:12,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1209/30753 [02:02<50:09,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1210/30753 [02:02<50:58,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1211/30753 [02:02<50:43,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1213/30753 [02:03<50:52,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1215/30753 [02:03<50:05,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.5ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1217/30753 [02:03<49:33,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1219/30753 [02:03<49:20,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1220/30753 [02:03<49:21,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.2ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1222/30753 [02:03<48:46, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 2.9ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1224/30753 [02:04<48:49, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1226/30753 [02:04<49:42,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1227/30753 [02:04<50:13,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 3.3ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1229/30753 [02:04<49:32,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1231/30753 [02:04<49:15,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1232/30753 [02:04<49:20,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1234/30753 [02:05<49:01, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1236/30753 [02:05<48:54, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1238/30753 [02:05<49:53,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1239/30753 [02:05<49:57,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1240/30753 [02:05<50:10,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1241/30753 [02:05<49:56,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1243/30753 [02:06<49:42,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1244/30753 [02:06<49:45,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1245/30753 [02:06<50:19,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1246/30753 [02:06<50:12,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1247/30753 [02:06<51:46,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 24.4ms\nSpeed: 4.6ms preprocess, 24.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1248/30753 [02:06<52:08,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1249/30753 [02:06<53:18,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1250/30753 [02:06<52:31,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1251/30753 [02:06<53:19,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1252/30753 [02:07<53:27,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1253/30753 [02:07<52:12,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1255/30753 [02:07<51:26,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1256/30753 [02:07<51:25,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1257/30753 [02:07<51:26,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 31.8ms\nSpeed: 4.3ms preprocess, 31.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1258/30753 [02:07<51:45,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1259/30753 [02:07<51:49,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1260/30753 [02:07<52:10,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1261/30753 [02:07<52:53,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1262/30753 [02:08<51:53,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1263/30753 [02:08<51:12,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1264/30753 [02:08<51:20,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1265/30753 [02:08<51:17,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1266/30753 [02:08<51:11,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1267/30753 [02:08<51:15,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1268/30753 [02:08<51:17,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1269/30753 [02:08<51:05,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1270/30753 [02:08<50:35,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1271/30753 [02:09<51:05,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1272/30753 [02:09<50:40,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1273/30753 [02:09<52:13,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 25.8ms\nSpeed: 4.3ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1274/30753 [02:09<51:54,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1275/30753 [02:09<51:35,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1276/30753 [02:09<51:26,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1277/30753 [02:09<52:46,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1278/30753 [02:09<52:08,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1279/30753 [02:09<51:21,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1280/30753 [02:09<50:59,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1281/30753 [02:10<50:40,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1282/30753 [02:10<50:28,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1283/30753 [02:10<50:47,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1284/30753 [02:10<50:47,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1285/30753 [02:10<52:39,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1286/30753 [02:10<52:28,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1287/30753 [02:10<52:30,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1288/30753 [02:10<51:53,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1289/30753 [02:10<51:33,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1290/30753 [02:11<51:30,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1291/30753 [02:11<51:20,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1292/30753 [02:11<51:03,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1293/30753 [02:11<50:49,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1294/30753 [02:11<50:49,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1295/30753 [02:11<50:36,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 4.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1296/30753 [02:11<50:46,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1297/30753 [02:11<52:17,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1298/30753 [02:11<51:48,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1299/30753 [02:11<51:17,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1300/30753 [02:12<50:57,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 29.1ms\nSpeed: 3.4ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1301/30753 [02:12<52:49,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.7ms\nSpeed: 3.9ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1302/30753 [02:12<53:20,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1303/30753 [02:12<52:33,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1304/30753 [02:12<51:48,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1305/30753 [02:12<51:49,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1306/30753 [02:12<52:39,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.6ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1307/30753 [02:12<52:24,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1308/30753 [02:12<51:46,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1309/30753 [02:13<52:44,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1310/30753 [02:13<51:54,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1311/30753 [02:13<51:58,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1312/30753 [02:13<51:13,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1313/30753 [02:13<50:56,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 28.7ms\nSpeed: 3.4ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1314/30753 [02:13<50:56,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1315/30753 [02:13<50:51,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1316/30753 [02:13<51:01,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 26.6ms\nSpeed: 3.3ms preprocess, 26.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1317/30753 [02:13<50:34,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 28.9ms\nSpeed: 3.1ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1318/30753 [02:13<51:04,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 26.5ms\nSpeed: 4.5ms preprocess, 26.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1319/30753 [02:14<53:14,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.7ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1320/30753 [02:14<52:58,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1321/30753 [02:14<57:58,  8.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 25.9ms\nSpeed: 4.3ms preprocess, 25.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1322/30753 [02:14<57:45,  8.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 25.3ms\nSpeed: 4.3ms preprocess, 25.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1323/30753 [02:14<59:09,  8.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 25.3ms\nSpeed: 4.6ms preprocess, 25.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1324/30753 [02:14<57:11,  8.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1325/30753 [02:14<56:02,  8.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1326/30753 [02:14<54:46,  8.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1327/30753 [02:15<55:03,  8.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1328/30753 [02:15<54:13,  9.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1329/30753 [02:15<53:24,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1330/30753 [02:15<52:39,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1331/30753 [02:15<51:57,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1332/30753 [02:15<51:44,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1333/30753 [02:15<53:11,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1334/30753 [02:15<52:28,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1335/30753 [02:15<51:59,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1336/30753 [02:15<51:32,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1337/30753 [02:16<51:05,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1338/30753 [02:16<50:28,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.1ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1339/30753 [02:16<51:04,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1340/30753 [02:16<50:39,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1341/30753 [02:16<52:48,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 4.6ms preprocess, 25.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1342/30753 [02:16<53:45,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1343/30753 [02:16<53:35,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1344/30753 [02:16<53:32,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1345/30753 [02:16<54:23,  9.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1346/30753 [02:17<53:38,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1347/30753 [02:17<52:27,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 4.5ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1348/30753 [02:17<51:49,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1349/30753 [02:17<51:33,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1350/30753 [02:17<51:06,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1351/30753 [02:17<52:37,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1352/30753 [02:17<53:39,  9.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1353/30753 [02:17<52:43,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.0ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1354/30753 [02:17<52:16,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1355/30753 [02:17<51:47,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.1ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1356/30753 [02:18<51:28,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1357/30753 [02:18<52:21,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1358/30753 [02:18<51:56,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 29.7ms\nSpeed: 3.8ms preprocess, 29.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1359/30753 [02:18<52:38,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1360/30753 [02:18<52:33,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1361/30753 [02:18<52:24,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1362/30753 [02:18<52:02,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1363/30753 [02:18<51:39,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1364/30753 [02:18<51:42,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1365/30753 [02:19<52:11,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1366/30753 [02:19<52:04,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1367/30753 [02:19<51:51,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1368/30753 [02:19<51:42,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1369/30753 [02:19<52:51,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1370/30753 [02:19<52:48,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1371/30753 [02:19<53:51,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1372/30753 [02:19<53:12,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1373/30753 [02:19<52:54,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1374/30753 [02:20<52:23,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1375/30753 [02:20<51:58,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1376/30753 [02:20<51:48,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1377/30753 [02:20<52:28,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1378/30753 [02:20<52:22,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1379/30753 [02:20<51:30,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1380/30753 [02:20<51:08,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 4.7ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1381/30753 [02:20<52:38,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1382/30753 [02:20<51:52,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   4%|▍         | 1383/30753 [02:20<51:13,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1384/30753 [02:21<51:17,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1385/30753 [02:21<51:00,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1386/30753 [02:21<50:45,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1387/30753 [02:21<50:43,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1388/30753 [02:21<50:39,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 2.8ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1389/30753 [02:21<50:33,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1390/30753 [02:21<50:41,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1391/30753 [02:21<51:03,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1392/30753 [02:21<50:50,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1393/30753 [02:22<51:58,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1394/30753 [02:22<51:37,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1395/30753 [02:22<51:19,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1396/30753 [02:22<51:21,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1397/30753 [02:22<51:28,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1398/30753 [02:22<51:26,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1399/30753 [02:22<51:26,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1400/30753 [02:22<51:35,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1401/30753 [02:22<53:05,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1402/30753 [02:22<53:23,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1403/30753 [02:23<52:32,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1404/30753 [02:23<51:43,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 30.4ms\nSpeed: 3.6ms preprocess, 30.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1405/30753 [02:23<53:03,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1406/30753 [02:23<52:37,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1407/30753 [02:23<51:34,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1408/30753 [02:23<51:26,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1409/30753 [02:23<52:09,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1410/30753 [02:23<51:43,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1412/30753 [02:24<50:50,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1413/30753 [02:24<50:41,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1414/30753 [02:24<50:55,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1415/30753 [02:24<51:06,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 3.1ms preprocess, 26.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1416/30753 [02:24<51:30,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1417/30753 [02:24<52:53,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1418/30753 [02:24<52:40,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1419/30753 [02:24<52:18,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1420/30753 [02:24<52:00,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1421/30753 [02:24<52:16,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1422/30753 [02:25<51:42,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1423/30753 [02:25<51:28,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 3.2ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1424/30753 [02:25<51:32,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1425/30753 [02:25<51:58,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1426/30753 [02:25<52:09,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1427/30753 [02:25<53:19,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1428/30753 [02:25<53:08,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1429/30753 [02:25<54:10,  9.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.2ms preprocess, 28.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1430/30753 [02:25<54:02,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1431/30753 [02:26<53:17,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1432/30753 [02:26<52:40,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 29.1ms\nSpeed: 3.6ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1433/30753 [02:26<53:01,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1434/30753 [02:26<52:40,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.7ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1435/30753 [02:26<55:32,  8.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1436/30753 [02:26<54:51,  8.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1437/30753 [02:26<53:35,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1438/30753 [02:26<52:46,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1439/30753 [02:26<52:37,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1440/30753 [02:27<52:13,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1441/30753 [02:27<53:04,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1442/30753 [02:27<52:00,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1443/30753 [02:27<51:26,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1444/30753 [02:27<51:11,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1445/30753 [02:27<51:23,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.7ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1446/30753 [02:27<51:48,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 4.1ms preprocess, 26.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1447/30753 [02:27<51:50,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 24.8ms\nSpeed: 4.1ms preprocess, 24.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1448/30753 [02:27<52:06,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1449/30753 [02:28<51:41,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1450/30753 [02:28<51:36,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1451/30753 [02:28<53:11,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1452/30753 [02:28<53:31,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.2ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1453/30753 [02:28<54:21,  8.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1454/30753 [02:28<53:43,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1455/30753 [02:28<53:09,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.5ms preprocess, 26.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1456/30753 [02:28<52:47,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 30.7ms\nSpeed: 3.9ms preprocess, 30.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1457/30753 [02:28<52:36,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.9ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1458/30753 [02:28<52:20,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1459/30753 [02:29<52:45,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1460/30753 [02:29<52:46,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1461/30753 [02:29<52:10,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1462/30753 [02:29<52:14,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1463/30753 [02:29<51:55,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1464/30753 [02:29<51:59,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1465/30753 [02:29<52:57,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1466/30753 [02:29<53:04,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1467/30753 [02:29<52:46,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1468/30753 [02:30<53:01,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1469/30753 [02:30<52:44,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1470/30753 [02:30<52:19,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1471/30753 [02:30<52:18,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1472/30753 [02:30<52:21,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1473/30753 [02:30<52:07,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1474/30753 [02:30<51:52,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1475/30753 [02:30<51:48,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1476/30753 [02:30<51:42,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.5ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1477/30753 [02:31<53:05,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1478/30753 [02:31<52:41,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1479/30753 [02:31<52:28,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1480/30753 [02:31<52:15,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1481/30753 [02:31<52:21,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1482/30753 [02:31<52:28,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.7ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1483/30753 [02:31<52:43,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1484/30753 [02:31<52:46,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1485/30753 [02:31<52:30,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1486/30753 [02:31<52:23,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1487/30753 [02:32<52:20,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1488/30753 [02:32<52:32,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1489/30753 [02:32<53:34,  9.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1490/30753 [02:32<53:08,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1491/30753 [02:32<52:47,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1492/30753 [02:32<52:30,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1493/30753 [02:32<52:21,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1494/30753 [02:32<52:24,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1495/30753 [02:32<52:21,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1496/30753 [02:33<52:25,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1497/30753 [02:33<52:16,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1498/30753 [02:33<52:01,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1499/30753 [02:33<51:53,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1500/30753 [02:33<52:23,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1501/30753 [02:33<53:54,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.2ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1502/30753 [02:33<54:55,  8.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1503/30753 [02:33<54:30,  8.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1504/30753 [02:33<53:39,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1505/30753 [02:34<52:39,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1506/30753 [02:34<51:48,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1507/30753 [02:34<51:26,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1508/30753 [02:34<52:42,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1509/30753 [02:34<52:42,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1510/30753 [02:34<52:32,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1511/30753 [02:34<52:17,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1512/30753 [02:34<53:18,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1513/30753 [02:34<54:19,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1514/30753 [02:35<54:24,  8.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1515/30753 [02:35<53:58,  9.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1516/30753 [02:35<53:53,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.8ms preprocess, 26.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1517/30753 [02:35<53:44,  9.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1518/30753 [02:35<53:30,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1519/30753 [02:35<53:07,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1520/30753 [02:35<53:14,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 4.0ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1521/30753 [02:35<52:52,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1522/30753 [02:35<52:57,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 28.6ms\nSpeed: 4.4ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1523/30753 [02:36<53:29,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 25.8ms\nSpeed: 4.6ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1524/30753 [02:36<53:16,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1525/30753 [02:36<54:25,  8.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1526/30753 [02:36<54:19,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1527/30753 [02:36<57:07,  8.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 26.3ms\nSpeed: 3.6ms preprocess, 26.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1528/30753 [02:36<56:22,  8.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1529/30753 [02:36<55:19,  8.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1530/30753 [02:36<54:02,  9.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1531/30753 [02:36<53:15,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1532/30753 [02:37<53:18,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1533/30753 [02:37<52:49,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1534/30753 [02:37<52:17,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1535/30753 [02:37<51:57,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1536/30753 [02:37<51:39,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▍         | 1537/30753 [02:37<53:20,  9.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1538/30753 [02:37<53:11,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1539/30753 [02:37<52:35,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1540/30753 [02:37<52:11,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1541/30753 [02:37<52:06,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1542/30753 [02:38<51:24,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1543/30753 [02:38<51:16,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1544/30753 [02:38<51:16,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1545/30753 [02:38<51:24,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1546/30753 [02:38<51:11,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1547/30753 [02:38<51:12,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1548/30753 [02:38<51:26,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1549/30753 [02:38<52:44,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1550/30753 [02:38<52:12,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1551/30753 [02:39<53:10,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1552/30753 [02:39<53:08,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1553/30753 [02:39<52:34,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1554/30753 [02:39<52:12,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1555/30753 [02:39<51:33,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1556/30753 [02:39<51:28,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1557/30753 [02:39<52:35,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1558/30753 [02:39<52:27,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1559/30753 [02:39<51:58,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1560/30753 [02:40<51:56,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1561/30753 [02:40<53:05,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1562/30753 [02:40<52:28,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1563/30753 [02:40<52:00,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1564/30753 [02:40<51:27,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.6ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1565/30753 [02:40<51:27,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1566/30753 [02:40<51:01,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1567/30753 [02:40<51:08,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1568/30753 [02:40<51:03,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1569/30753 [02:40<51:06,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1570/30753 [02:41<50:42,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1571/30753 [02:41<50:43,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1572/30753 [02:41<50:56,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1573/30753 [02:41<52:30,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1574/30753 [02:41<52:03,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1575/30753 [02:41<51:37,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1576/30753 [02:41<51:11,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1577/30753 [02:41<52:24,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1578/30753 [02:41<51:36,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1580/30753 [02:42<50:02,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1582/30753 [02:42<49:18,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1583/30753 [02:42<49:21,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1584/30753 [02:42<49:56,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1585/30753 [02:42<51:26,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1586/30753 [02:42<51:08,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1587/30753 [02:42<50:56,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1589/30753 [02:43<50:06,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1590/30753 [02:43<49:47,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 2.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1591/30753 [02:43<49:41,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1592/30753 [02:43<49:40,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1593/30753 [02:43<49:30,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1594/30753 [02:43<49:22,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1595/30753 [02:43<49:38,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1596/30753 [02:43<49:56,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1597/30753 [02:43<51:30,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.1ms preprocess, 29.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1598/30753 [02:43<50:59,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1599/30753 [02:44<50:37,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1601/30753 [02:44<50:38,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1602/30753 [02:44<50:46,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1603/30753 [02:44<50:30,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.4ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1604/30753 [02:44<50:10,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1605/30753 [02:44<50:06,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1606/30753 [02:44<50:46,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1607/30753 [02:44<50:46,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1608/30753 [02:45<51:03,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1609/30753 [02:45<52:24,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1610/30753 [02:45<52:07,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1611/30753 [02:45<51:52,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.6ms\nSpeed: 4.1ms preprocess, 25.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1612/30753 [02:45<52:54,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1613/30753 [02:45<52:23,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.4ms preprocess, 28.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1614/30753 [02:45<53:16,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1615/30753 [02:45<55:10,  8.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1616/30753 [02:45<54:07,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1617/30753 [02:46<54:19,  8.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 5.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1618/30753 [02:46<55:48,  8.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1619/30753 [02:46<56:06,  8.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.6ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1620/30753 [02:46<54:54,  8.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1621/30753 [02:46<57:55,  8.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 25.0ms\nSpeed: 4.4ms preprocess, 25.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1622/30753 [02:46<56:07,  8.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 4.4ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1623/30753 [02:46<54:43,  8.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1624/30753 [02:46<52:59,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1625/30753 [02:46<52:05,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1626/30753 [02:47<51:44,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1627/30753 [02:47<52:16,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1628/30753 [02:47<51:25,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1629/30753 [02:47<50:34,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1630/30753 [02:47<50:00,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 29.5ms\nSpeed: 3.1ms preprocess, 29.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1631/30753 [02:47<50:02,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1633/30753 [02:47<50:43,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1634/30753 [02:47<50:45,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1635/30753 [02:47<50:46,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1636/30753 [02:48<50:12,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1637/30753 [02:48<49:50,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 4.1ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1638/30753 [02:48<49:46,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1640/30753 [02:48<49:04,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1641/30753 [02:48<49:34,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1642/30753 [02:48<49:47,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1643/30753 [02:48<49:50,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1644/30753 [02:48<49:49,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1645/30753 [02:49<51:10,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1646/30753 [02:49<50:36,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1647/30753 [02:49<50:03,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1648/30753 [02:49<49:57,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1650/30753 [02:49<49:15,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1651/30753 [02:49<50:50,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1652/30753 [02:49<52:00,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.8ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1653/30753 [02:49<51:43,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1655/30753 [02:50<50:22,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1656/30753 [02:50<50:47,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1657/30753 [02:50<51:58,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1658/30753 [02:50<51:34,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1659/30753 [02:50<51:39,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1660/30753 [02:50<51:18,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1661/30753 [02:50<51:37,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1662/30753 [02:50<50:50,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.5ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1663/30753 [02:50<50:20,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1664/30753 [02:50<49:59,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1665/30753 [02:51<49:57,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1666/30753 [02:51<50:02,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1667/30753 [02:51<49:55,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1668/30753 [02:51<49:44,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1669/30753 [02:51<51:05,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1670/30753 [02:51<50:38,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1671/30753 [02:51<50:43,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1672/30753 [02:51<50:06,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1673/30753 [02:51<50:30,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.0ms preprocess, 26.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1674/30753 [02:52<50:44,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1675/30753 [02:52<50:39,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1676/30753 [02:52<50:15,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1677/30753 [02:52<51:11,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.5ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1678/30753 [02:52<50:28,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1679/30753 [02:52<50:06,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1680/30753 [02:52<49:52,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1681/30753 [02:52<51:28,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1682/30753 [02:52<50:42,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.2ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1683/30753 [02:52<50:17,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1684/30753 [02:53<50:03,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1685/30753 [02:53<49:57,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1686/30753 [02:53<49:57,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1687/30753 [02:53<49:43,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1688/30753 [02:53<49:52,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1689/30753 [02:53<50:00,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1690/30753 [02:53<49:43,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   5%|▌         | 1691/30753 [02:53<49:41,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1692/30753 [02:53<49:40,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.8ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1693/30753 [02:54<51:03,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1695/30753 [02:54<49:30,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1697/30753 [02:54<48:51,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.1ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1698/30753 [02:54<48:49,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1699/30753 [02:54<48:53,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1700/30753 [02:54<49:04,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 3.7ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1701/30753 [02:54<50:07,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1702/30753 [02:54<50:51,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.1ms preprocess, 26.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1703/30753 [02:55<50:25,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1704/30753 [02:55<50:19,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1705/30753 [02:55<51:00,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.9ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1706/30753 [02:55<51:24,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.9ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1707/30753 [02:55<50:49,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1709/30753 [02:55<49:38,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1710/30753 [02:55<49:44,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.2ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1711/30753 [02:55<49:47,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1712/30753 [02:55<49:33,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1713/30753 [02:56<49:40,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1714/30753 [02:56<49:27,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.6ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1715/30753 [02:56<50:29,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 3.7ms preprocess, 25.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1717/30753 [02:56<51:12,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 5.7ms preprocess, 25.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1718/30753 [02:56<51:13,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1719/30753 [02:56<50:58,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1720/30753 [02:56<50:43,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1722/30753 [02:56<49:52,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1723/30753 [02:57<49:41,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 umbrella, 1 clock, 26.0ms\nSpeed: 4.4ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1724/30753 [02:57<49:39,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 umbrella, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1725/30753 [02:57<49:38,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1727/30753 [02:57<49:47,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 29.6ms\nSpeed: 3.2ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1728/30753 [02:57<49:51,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1729/30753 [02:57<50:44,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 28.8ms\nSpeed: 3.4ms preprocess, 28.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1731/30753 [02:57<49:31,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1733/30753 [02:58<48:35,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 28.7ms\nSpeed: 2.8ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 umbrella, 1 clock, 28.6ms\nSpeed: 3.6ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1735/30753 [02:58<48:23,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 umbrella, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1736/30753 [02:58<48:38,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 umbrella, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1737/30753 [02:58<48:58,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 umbrella, 1 clock, 27.1ms\nSpeed: 5.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1738/30753 [02:58<49:13,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 umbrella, 1 clock, 28.4ms\nSpeed: 2.8ms preprocess, 28.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1739/30753 [02:58<49:52,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 umbrella, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1740/30753 [02:58<49:38,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 umbrella, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1741/30753 [02:58<50:57,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1742/30753 [02:59<50:33,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.8ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1744/30753 [02:59<48:55,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1745/30753 [02:59<49:00,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1746/30753 [02:59<49:02,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1747/30753 [02:59<48:59,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1748/30753 [02:59<49:09,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.6ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1749/30753 [02:59<49:28,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1750/30753 [02:59<49:45,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1751/30753 [02:59<51:10,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1752/30753 [03:00<51:45,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.8ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1753/30753 [03:00<52:10,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1755/30753 [03:00<49:51,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1756/30753 [03:00<49:54,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1758/30753 [03:00<49:04,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1759/30753 [03:00<49:09,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1760/30753 [03:00<49:33,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1761/30753 [03:00<49:15,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1763/30753 [03:01<48:36,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1765/30753 [03:01<49:10,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1766/30753 [03:01<49:05,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1767/30753 [03:01<48:55,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1769/30753 [03:01<48:38,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 2.8ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1771/30753 [03:01<48:10, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.7ms preprocess, 28.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1773/30753 [03:02<47:47, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1775/30753 [03:02<47:50, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1777/30753 [03:02<48:50,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1778/30753 [03:02<48:49,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.4ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1779/30753 [03:02<48:54,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1781/30753 [03:02<48:14, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1783/30753 [03:03<47:48, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1785/30753 [03:03<47:23, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.4ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1787/30753 [03:03<47:18, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1789/30753 [03:03<48:31,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1790/30753 [03:03<48:35,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1792/30753 [03:04<48:14, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1794/30753 [03:04<47:53, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1796/30753 [03:04<47:43, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1798/30753 [03:04<47:49, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1800/30753 [03:04<48:03, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.9ms\nSpeed: 3.5ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1802/30753 [03:05<49:22,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.4ms\nSpeed: 4.7ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1803/30753 [03:05<49:18,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1804/30753 [03:05<49:08,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1806/30753 [03:05<49:11,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1807/30753 [03:05<49:20,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1808/30753 [03:05<49:05,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1809/30753 [03:05<49:15,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.8ms\nSpeed: 4.0ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1810/30753 [03:05<49:25,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1811/30753 [03:06<49:11,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1812/30753 [03:06<49:01,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1813/30753 [03:06<50:22,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1814/30753 [03:06<49:54,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 25.4ms\nSpeed: 4.0ms preprocess, 25.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1816/30753 [03:06<50:10,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1817/30753 [03:06<50:00,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.7ms\nSpeed: 4.5ms preprocess, 26.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1818/30753 [03:06<50:04,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 25.8ms\nSpeed: 4.4ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1819/30753 [03:06<50:00,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 4.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1820/30753 [03:06<50:09,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1821/30753 [03:07<50:18,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.8ms\nSpeed: 4.2ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1822/30753 [03:07<50:07,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1824/30753 [03:07<49:09,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1825/30753 [03:07<50:03,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 2.8ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1827/30753 [03:07<49:55,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1828/30753 [03:07<50:03,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1830/30753 [03:07<48:30,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1832/30753 [03:08<47:53, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.8ms\nSpeed: 3.2ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1834/30753 [03:08<47:47, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1836/30753 [03:08<48:01, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1837/30753 [03:08<48:56,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1838/30753 [03:08<48:58,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1839/30753 [03:08<48:51,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.6ms\nSpeed: 2.9ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1840/30753 [03:08<48:46,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1841/30753 [03:09<48:41,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1843/30753 [03:09<48:18,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1845/30753 [03:09<47:50, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1847/30753 [03:09<47:45, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1849/30753 [03:09<49:04,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1850/30753 [03:09<48:58,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1851/30753 [03:10<50:03,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.9ms\nSpeed: 4.5ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1852/30753 [03:10<50:18,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1853/30753 [03:10<50:05,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1855/30753 [03:10<49:38,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1856/30753 [03:10<49:24,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1858/30753 [03:10<48:58,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1859/30753 [03:10<49:20,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1860/30753 [03:11<49:19,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1861/30753 [03:11<50:19,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1862/30753 [03:11<49:47,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 29.1ms\nSpeed: 3.2ms preprocess, 29.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1864/30753 [03:11<48:41,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1865/30753 [03:11<48:35,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1867/30753 [03:11<48:06, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1868/30753 [03:11<48:08, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1870/30753 [03:12<47:45, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1872/30753 [03:12<47:22, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1874/30753 [03:12<47:56, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.1ms\nSpeed: 3.9ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1876/30753 [03:12<47:43, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 30.1ms\nSpeed: 3.9ms preprocess, 30.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1878/30753 [03:12<48:27,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1879/30753 [03:12<48:24,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1881/30753 [03:13<47:47, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 4.5ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1883/30753 [03:13<47:40, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 4.6ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1885/30753 [03:13<48:16,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1886/30753 [03:13<48:21,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1887/30753 [03:13<48:30,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1888/30753 [03:13<48:52,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1889/30753 [03:13<48:43,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1891/30753 [03:14<47:59, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1893/30753 [03:14<47:45, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 26.9ms\nSpeed: 2.8ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1895/30753 [03:14<47:31, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1897/30753 [03:14<48:38,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1898/30753 [03:14<48:38,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1899/30753 [03:14<48:33,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.3ms\nSpeed: 2.9ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1900/30753 [03:15<48:30,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1901/30753 [03:15<49:44,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.3ms\nSpeed: 4.4ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1902/30753 [03:15<50:05,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 2.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1904/30753 [03:15<48:50,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1905/30753 [03:15<48:57,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1907/30753 [03:15<48:23,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1908/30753 [03:15<48:21,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1909/30753 [03:15<49:32,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1910/30753 [03:16<49:12,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.7ms\nSpeed: 4.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1912/30753 [03:16<49:04,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 2.8ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1913/30753 [03:16<49:23,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1914/30753 [03:16<50:11,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 25.1ms\nSpeed: 3.9ms preprocess, 25.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1915/30753 [03:16<50:19,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1916/30753 [03:16<49:55,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1917/30753 [03:16<52:25,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1918/30753 [03:16<52:30,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.6ms\nSpeed: 4.9ms preprocess, 27.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1919/30753 [03:17<52:18,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1920/30753 [03:17<52:31,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.8ms\nSpeed: 4.4ms preprocess, 28.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1921/30753 [03:17<53:23,  9.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▌         | 1922/30753 [03:17<52:36,  9.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1923/30753 [03:17<51:33,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.1ms\nSpeed: 4.1ms preprocess, 26.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1925/30753 [03:17<50:50,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1926/30753 [03:17<50:25,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1927/30753 [03:17<50:57,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 29.0ms\nSpeed: 4.0ms preprocess, 29.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1928/30753 [03:17<51:00,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1930/30753 [03:18<49:17,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1932/30753 [03:18<48:24,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 clock, 27.8ms\nSpeed: 2.8ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1933/30753 [03:18<49:19,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1934/30753 [03:18<49:11,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1936/30753 [03:18<48:21,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.2ms preprocess, 28.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1937/30753 [03:18<48:38,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1938/30753 [03:18<48:50,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1939/30753 [03:19<48:44,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1940/30753 [03:19<48:39,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.0ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1942/30753 [03:19<48:03,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1943/30753 [03:19<48:15,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1944/30753 [03:19<48:17,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1945/30753 [03:19<49:39,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1946/30753 [03:19<49:35,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1947/30753 [03:19<49:29,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1948/30753 [03:20<49:20,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1950/30753 [03:20<48:36,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1951/30753 [03:20<49:48,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1952/30753 [03:20<50:05,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1954/30753 [03:20<48:44,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1955/30753 [03:20<49:15,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1957/30753 [03:20<48:55,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1958/30753 [03:21<48:55,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.1ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1960/30753 [03:21<48:53,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1961/30753 [03:21<48:44,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1963/30753 [03:21<47:49, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1965/30753 [03:21<47:56, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1966/30753 [03:21<48:05,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1967/30753 [03:21<48:10,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1968/30753 [03:22<48:08,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1969/30753 [03:22<49:12,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.3ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1970/30753 [03:22<48:57,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1972/30753 [03:22<48:18,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1973/30753 [03:22<48:19,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.3ms\nSpeed: 3.5ms preprocess, 29.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1975/30753 [03:22<47:50, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1977/30753 [03:22<48:28,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1978/30753 [03:23<48:30,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1980/30753 [03:23<47:43, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1982/30753 [03:23<48:06,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1984/30753 [03:23<47:36, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1986/30753 [03:23<47:09, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.8ms preprocess, 26.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 2.7ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1988/30753 [03:24<46:41, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1990/30753 [03:24<46:31, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1992/30753 [03:24<46:32, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1994/30753 [03:24<47:37, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1996/30753 [03:24<47:39, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   6%|▋         | 1998/30753 [03:25<47:44, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2000/30753 [03:25<47:48, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2002/30753 [03:25<48:39,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2004/30753 [03:25<47:58,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.2ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2006/30753 [03:25<48:24,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.7ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2008/30753 [03:26<47:38, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2010/30753 [03:26<47:34, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2012/30753 [03:26<47:21, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2014/30753 [03:26<48:15,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2016/30753 [03:26<48:09,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2017/30753 [03:26<48:45,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.9ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2019/30753 [03:27<48:10,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.3ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2020/30753 [03:27<48:09,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2021/30753 [03:27<48:08,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2023/30753 [03:27<47:18, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.7ms preprocess, 28.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2025/30753 [03:27<47:30, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2027/30753 [03:27<48:58,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2028/30753 [03:28<48:54,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2029/30753 [03:28<49:35,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2031/30753 [03:28<48:32,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2033/30753 [03:28<48:07,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2034/30753 [03:28<48:05,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2035/30753 [03:28<48:08,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2037/30753 [03:28<47:39, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2039/30753 [03:29<47:26, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2041/30753 [03:29<48:13,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2042/30753 [03:29<48:17,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2044/30753 [03:29<47:50, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2045/30753 [03:29<48:03,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2046/30753 [03:29<48:02,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.6ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2047/30753 [03:29<48:12,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 25.6ms\nSpeed: 4.4ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2048/30753 [03:30<48:16,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 29.4ms\nSpeed: 3.8ms preprocess, 29.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2049/30753 [03:30<48:39,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2051/30753 [03:30<48:57,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2052/30753 [03:30<49:27,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2053/30753 [03:30<49:58,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2055/30753 [03:30<49:21,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2057/30753 [03:30<48:35,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2058/30753 [03:31<48:33,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2059/30753 [03:31<48:28,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2060/30753 [03:31<48:21,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2061/30753 [03:31<48:22,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.4ms\nSpeed: 4.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2062/30753 [03:31<48:22,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2064/30753 [03:31<47:27, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2066/30753 [03:31<48:07,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2068/30753 [03:32<47:47, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2069/30753 [03:32<47:52,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2071/30753 [03:32<47:37, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2073/30753 [03:32<47:26, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.0ms\nSpeed: 3.7ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2075/30753 [03:32<47:27, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2077/30753 [03:32<47:58,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2078/30753 [03:33<48:07,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2080/30753 [03:33<47:24, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 26.9ms\nSpeed: 2.9ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2082/30753 [03:33<47:05, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2084/30753 [03:33<47:11, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2086/30753 [03:33<47:19, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2088/30753 [03:34<47:34, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2090/30753 [03:34<48:08,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2092/30753 [03:34<47:49,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2094/30753 [03:34<47:27, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.2ms\nSpeed: 3.1ms preprocess, 29.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2096/30753 [03:34<47:26, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2098/30753 [03:35<47:05, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 30.0ms\nSpeed: 3.6ms preprocess, 30.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2100/30753 [03:35<47:25, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.3ms\nSpeed: 3.7ms preprocess, 29.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2102/30753 [03:35<48:48,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2104/30753 [03:35<48:21,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2105/30753 [03:35<48:51,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2107/30753 [03:35<48:09,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2108/30753 [03:36<48:07,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2109/30753 [03:36<48:21,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2110/30753 [03:36<48:58,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2111/30753 [03:36<48:51,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2112/30753 [03:36<50:33,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 4.2ms preprocess, 25.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2113/30753 [03:36<51:19,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2114/30753 [03:36<50:25,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2115/30753 [03:36<49:59,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2116/30753 [03:36<49:28,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2117/30753 [03:37<49:12,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2118/30753 [03:37<48:49,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2120/30753 [03:37<47:54,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.8ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2122/30753 [03:37<47:23, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2124/30753 [03:37<47:00, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2126/30753 [03:37<47:46,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2127/30753 [03:38<48:08,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2129/30753 [03:38<47:29, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2131/30753 [03:38<46:50, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2133/30753 [03:38<46:35, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2135/30753 [03:38<46:34, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 26.1ms\nSpeed: 3.6ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2137/30753 [03:38<47:14, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2139/30753 [03:39<47:00, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2141/30753 [03:39<46:41, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 29.6ms\nSpeed: 2.7ms preprocess, 29.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2143/30753 [03:39<46:36, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2145/30753 [03:39<46:23, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2147/30753 [03:39<46:51, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2149/30753 [03:40<47:49,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2150/30753 [03:40<47:56,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2151/30753 [03:40<48:51,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2152/30753 [03:40<49:20,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2153/30753 [03:40<48:57,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2155/30753 [03:40<48:52,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2156/30753 [03:40<48:51,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2158/30753 [03:41<47:51,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2160/30753 [03:41<48:10,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2161/30753 [03:41<48:51,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2162/30753 [03:41<48:41,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2164/30753 [03:41<47:47,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2165/30753 [03:41<47:53,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2167/30753 [03:42<47:11, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.7ms\nSpeed: 4.3ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2169/30753 [03:42<47:23, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2171/30753 [03:42<47:25, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 4.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2173/30753 [03:42<48:04,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2174/30753 [03:42<48:01,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2176/30753 [03:42<47:33, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2178/30753 [03:43<47:42,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 umbrella, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2179/30753 [03:43<47:46,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 umbrella, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2180/30753 [03:43<48:02,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 umbrella, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2181/30753 [03:43<48:11,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 umbrella, 1 clock, 26.4ms\nSpeed: 4.0ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2182/30753 [03:43<48:08,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 umbrella, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 umbrella, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2184/30753 [03:43<47:50,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 umbrella, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2185/30753 [03:43<49:28,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 umbrella, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2186/30753 [03:43<49:13,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 umbrella, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2188/30753 [03:44<48:14,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2190/30753 [03:44<47:46,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2192/30753 [03:44<47:09, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2194/30753 [03:44<47:21, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 5.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2196/30753 [03:44<47:38,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2197/30753 [03:45<48:39,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2199/30753 [03:45<47:47,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2201/30753 [03:45<47:48,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 3.8ms preprocess, 26.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2202/30753 [03:45<48:12,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2204/30753 [03:45<47:27, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2205/30753 [03:45<47:59,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2206/30753 [03:45<48:08,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2207/30753 [03:46<48:09,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.9ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2209/30753 [03:46<48:44,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2210/30753 [03:46<49:05,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 2.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2211/30753 [03:46<49:12,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 25.2ms\nSpeed: 3.9ms preprocess, 25.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2212/30753 [03:46<49:39,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2213/30753 [03:46<49:13,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2215/30753 [03:46<48:29,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2216/30753 [03:46<48:42,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2217/30753 [03:47<48:44,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2218/30753 [03:47<48:36,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2219/30753 [03:47<48:22,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2221/30753 [03:47<48:45,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 2.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2223/30753 [03:47<48:00,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2225/30753 [03:47<47:38,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2226/30753 [03:47<47:42,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2227/30753 [03:48<51:13,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.1ms\nSpeed: 4.0ms preprocess, 25.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2228/30753 [03:48<50:52,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.3ms\nSpeed: 4.8ms preprocess, 25.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2229/30753 [03:48<52:59,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.4ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2230/30753 [03:48<51:56,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 4.0ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2231/30753 [03:48<51:25,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 4.2ms preprocess, 25.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2232/30753 [03:48<51:28,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2233/30753 [03:48<53:32,  8.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2234/30753 [03:48<52:35,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2235/30753 [03:48<51:28,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2236/30753 [03:49<50:18,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2237/30753 [03:49<49:37,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2239/30753 [03:49<48:33,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2241/30753 [03:49<47:54,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2243/30753 [03:49<47:33,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2244/30753 [03:49<47:49,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2245/30753 [03:49<49:00,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2247/30753 [03:50<48:02,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2249/30753 [03:50<47:23, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2250/30753 [03:50<47:29, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.7ms\nSpeed: 3.5ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2251/30753 [03:50<48:21,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2252/30753 [03:50<48:52,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.5ms\nSpeed: 4.7ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2254/30753 [03:50<47:47,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2256/30753 [03:51<47:48,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2257/30753 [03:51<48:25,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2258/30753 [03:51<48:12,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2260/30753 [03:51<47:42,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2261/30753 [03:51<48:14,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2262/30753 [03:51<48:24,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.6ms\nSpeed: 4.1ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2264/30753 [03:51<47:39,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2265/30753 [03:52<47:53,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2266/30753 [03:52<47:49,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2267/30753 [03:52<47:44,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2268/30753 [03:52<47:51,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2269/30753 [03:52<49:06,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2270/30753 [03:52<49:05,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2271/30753 [03:52<48:43,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2272/30753 [03:52<48:21,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.9ms\nSpeed: 3.3ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 25.9ms\nSpeed: 4.5ms preprocess, 25.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2274/30753 [03:52<47:53,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2275/30753 [03:53<47:52,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2277/30753 [03:53<47:46,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2278/30753 [03:53<47:56,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2280/30753 [03:53<47:15, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2281/30753 [03:53<47:51,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2282/30753 [03:53<48:02,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 26.8ms\nSpeed: 4.5ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2284/30753 [03:53<47:34,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2285/30753 [03:54<47:50,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2287/30753 [03:54<47:23, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 2.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2289/30753 [03:54<46:45, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2291/30753 [03:54<46:37, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2293/30753 [03:54<47:07, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2295/30753 [03:55<47:19, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2297/30753 [03:55<47:10, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.4ms\nSpeed: 4.4ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2299/30753 [03:55<46:53, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2301/30753 [03:55<47:18, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2303/30753 [03:55<47:39,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2305/30753 [03:56<47:51,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   7%|▋         | 2306/30753 [03:56<48:14,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 24.1ms\nSpeed: 3.2ms preprocess, 24.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2308/30753 [03:56<47:49,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.0ms\nSpeed: 3.5ms preprocess, 26.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2310/30753 [03:56<48:36,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2312/30753 [03:56<47:49,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2313/30753 [03:56<47:49,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2315/30753 [03:57<47:46,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2316/30753 [03:57<47:45,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2317/30753 [03:57<48:36,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2318/30753 [03:57<48:47,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.5ms\nSpeed: 4.3ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2319/30753 [03:57<48:43,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2320/30753 [03:57<48:34,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2321/30753 [03:57<48:37,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2322/30753 [03:57<48:41,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2323/30753 [03:57<48:20,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2324/30753 [03:57<48:23,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.7ms\nSpeed: 3.6ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 clock, 28.5ms\nSpeed: 3.8ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2326/30753 [03:58<47:39,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2327/30753 [03:58<48:44,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2328/30753 [03:58<48:43,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2329/30753 [03:58<50:07,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2330/30753 [03:58<50:03,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2331/30753 [03:58<49:54,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 4.0ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2332/30753 [03:58<49:42,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2333/30753 [03:58<49:35,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2334/30753 [03:59<49:28,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2335/30753 [03:59<49:02,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2336/30753 [03:59<48:49,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2337/30753 [03:59<48:55,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2338/30753 [03:59<48:53,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2339/30753 [03:59<48:55,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2340/30753 [03:59<48:51,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2341/30753 [03:59<50:30,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2342/30753 [03:59<50:03,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2343/30753 [03:59<49:16,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2344/30753 [04:00<48:55,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2345/30753 [04:00<48:29,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2346/30753 [04:00<48:35,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2348/30753 [04:00<47:58,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2349/30753 [04:00<47:51,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2351/30753 [04:00<48:46,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2352/30753 [04:00<49:09,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2353/30753 [04:00<49:55,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.5ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2354/30753 [04:01<49:33,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.2ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2355/30753 [04:01<49:36,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2356/30753 [04:01<49:08,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2357/30753 [04:01<49:00,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2359/30753 [04:01<47:48,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2360/30753 [04:01<47:45,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2362/30753 [04:01<47:16, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2363/30753 [04:01<47:28,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2364/30753 [04:02<47:34,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2365/30753 [04:02<48:39,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2366/30753 [04:02<48:30,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2368/30753 [04:02<47:35,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2369/30753 [04:02<47:45,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2371/30753 [04:02<47:01, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2373/30753 [04:02<46:51, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 25.6ms\nSpeed: 3.4ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2375/30753 [04:03<46:47, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2377/30753 [04:03<47:23,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2378/30753 [04:03<47:27,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2380/30753 [04:03<46:42, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2382/30753 [04:03<46:31, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2384/30753 [04:04<46:12, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2386/30753 [04:04<46:40, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2388/30753 [04:04<47:03, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2390/30753 [04:04<48:10,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2392/30753 [04:04<47:53,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2393/30753 [04:04<47:50,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2394/30753 [04:05<47:45,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.6ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2395/30753 [04:05<47:46,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 3.1ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2396/30753 [04:05<47:58,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2397/30753 [04:05<47:53,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 4.3ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.2ms preprocess, 26.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2399/30753 [04:05<47:26,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2400/30753 [04:05<47:24,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2401/30753 [04:05<48:41,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2402/30753 [04:05<49:13,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2403/30753 [04:06<48:47,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2404/30753 [04:06<48:32,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.9ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2405/30753 [04:06<48:51,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2407/30753 [04:06<47:13, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2408/30753 [04:06<49:35,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2410/30753 [04:06<48:16,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2411/30753 [04:06<48:31,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2412/30753 [04:06<48:22,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2413/30753 [04:07<49:42,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2414/30753 [04:07<49:34,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2416/30753 [04:07<48:31,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2417/30753 [04:07<48:20,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2418/30753 [04:07<48:11,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2420/30753 [04:07<47:47,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2421/30753 [04:07<47:41,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2422/30753 [04:07<47:35,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2423/30753 [04:08<47:43,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2425/30753 [04:08<48:01,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2426/30753 [04:08<47:58,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2427/30753 [04:08<48:42,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2428/30753 [04:08<48:30,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2429/30753 [04:08<48:17,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2430/30753 [04:08<48:14,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2432/30753 [04:08<47:29,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.0ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2433/30753 [04:09<47:39,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2435/30753 [04:09<47:11, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2437/30753 [04:09<48:07,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2438/30753 [04:09<47:56,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2439/30753 [04:09<47:46,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2440/30753 [04:09<47:43,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2441/30753 [04:09<47:56,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2442/30753 [04:09<47:57,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2443/30753 [04:10<48:00,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2444/30753 [04:10<47:54,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2446/30753 [04:10<47:34,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2447/30753 [04:10<47:32,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 4.3ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2449/30753 [04:10<48:12,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2450/30753 [04:10<48:02,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2451/30753 [04:10<49:15,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2452/30753 [04:11<49:35,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2453/30753 [04:11<49:02,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2455/30753 [04:11<47:56,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2457/30753 [04:11<47:25,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2458/30753 [04:11<47:23,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2459/30753 [04:11<47:24,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2460/30753 [04:11<47:29,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2461/30753 [04:11<48:38,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2462/30753 [04:12<48:26,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2464/30753 [04:12<47:38,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2466/30753 [04:12<47:13,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2468/30753 [04:12<47:12,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 2.9ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2470/30753 [04:12<46:29, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2472/30753 [04:13<46:31, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2474/30753 [04:13<47:00, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2476/30753 [04:13<46:58, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 2.9ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2478/30753 [04:13<47:33,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2479/30753 [04:13<47:29,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2481/30753 [04:13<47:07, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2483/30753 [04:14<46:56, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2485/30753 [04:14<47:21,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2486/30753 [04:14<47:24,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2487/30753 [04:14<47:22,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2489/30753 [04:14<47:03, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2491/30753 [04:14<46:34, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2493/30753 [04:15<46:25, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2495/30753 [04:15<46:15, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2497/30753 [04:15<47:13,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.5ms preprocess, 26.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2499/30753 [04:15<46:52, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2501/30753 [04:15<47:44,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.2ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2502/30753 [04:16<48:18,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2503/30753 [04:16<48:09,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.5ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2504/30753 [04:16<47:57,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.8ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2505/30753 [04:16<48:11,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.5ms\nSpeed: 4.3ms preprocess, 25.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2507/30753 [04:16<48:27,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2508/30753 [04:16<48:15,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2509/30753 [04:16<49:04,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2510/30753 [04:16<48:34,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.9ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2512/30753 [04:17<47:38,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2514/30753 [04:17<46:53, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2516/30753 [04:17<46:13, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2518/30753 [04:17<46:32, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2520/30753 [04:17<46:35, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2522/30753 [04:18<47:30,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2524/30753 [04:18<46:54, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2526/30753 [04:18<46:38, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2528/30753 [04:18<47:21,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 2.7ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2530/30753 [04:18<46:25, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2532/30753 [04:19<46:00, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2534/30753 [04:19<46:31, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2536/30753 [04:19<47:09,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 3.8ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2537/30753 [04:19<47:43,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2538/30753 [04:19<47:58,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2539/30753 [04:19<48:25,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2540/30753 [04:19<48:46,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2541/30753 [04:19<50:45,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2542/30753 [04:20<50:20,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2543/30753 [04:20<49:35,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2544/30753 [04:20<49:00,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2545/30753 [04:20<49:56,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2546/30753 [04:20<50:24,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2547/30753 [04:20<49:57,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2548/30753 [04:20<49:22,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2550/30753 [04:20<47:38,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2551/30753 [04:21<48:45,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2552/30753 [04:21<48:52,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2554/30753 [04:21<48:06,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2556/30753 [04:21<46:59, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2557/30753 [04:21<47:45,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2559/30753 [04:21<47:01,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2561/30753 [04:22<46:46, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2562/30753 [04:22<46:56, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2564/30753 [04:22<46:21, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2566/30753 [04:22<45:53, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 2.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2568/30753 [04:22<45:21, 10.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2570/30753 [04:22<46:10, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 3.0ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.4ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2572/30753 [04:23<46:04, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.6ms\nSpeed: 2.9ms preprocess, 29.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2574/30753 [04:23<45:44, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 2.8ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2576/30753 [04:23<45:18, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 2.8ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2578/30753 [04:23<46:11, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.9ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2580/30753 [04:23<46:14, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2582/30753 [04:24<47:09,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2584/30753 [04:24<46:24, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2586/30753 [04:24<45:48, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 2.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2588/30753 [04:24<45:51, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2590/30753 [04:24<46:00, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2592/30753 [04:25<46:13, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2594/30753 [04:25<47:12,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2596/30753 [04:25<46:52, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 2.6ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2598/30753 [04:25<46:30, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2600/30753 [04:25<46:27, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2602/30753 [04:26<47:33,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.0ms\nSpeed: 3.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2604/30753 [04:26<47:12,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2605/30753 [04:26<47:59,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2606/30753 [04:26<48:48,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2608/30753 [04:26<47:44,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2609/30753 [04:26<47:45,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2611/30753 [04:26<46:58,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   8%|▊         | 2613/30753 [04:27<46:07, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2615/30753 [04:27<45:56, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2617/30753 [04:27<46:19, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2619/30753 [04:27<46:12, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.0ms\nSpeed: 3.2ms preprocess, 25.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2621/30753 [04:27<46:02, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2623/30753 [04:28<45:50, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 2.9ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2625/30753 [04:28<45:41, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2627/30753 [04:28<46:22, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2629/30753 [04:28<47:06,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2631/30753 [04:28<46:28, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2633/30753 [04:29<46:32, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2635/30753 [04:29<46:08, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2637/30753 [04:29<46:04, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2639/30753 [04:29<45:23, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2641/30753 [04:29<46:21, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2643/30753 [04:30<46:14, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2645/30753 [04:30<46:14, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.7ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2647/30753 [04:30<46:07, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2649/30753 [04:30<46:16, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2651/30753 [04:30<46:55,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2652/30753 [04:31<47:22,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2653/30753 [04:31<48:06,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2654/30753 [04:31<48:19,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2656/30753 [04:31<47:09,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.5ms\nSpeed: 3.6ms preprocess, 26.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2658/30753 [04:31<46:29, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2660/30753 [04:31<46:17, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2662/30753 [04:32<46:04, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2664/30753 [04:32<46:10, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2666/30753 [04:32<46:40, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2668/30753 [04:32<46:06, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.5ms\nSpeed: 3.2ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2670/30753 [04:32<45:46, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2672/30753 [04:33<45:32, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.6ms\nSpeed: 4.1ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 25.6ms\nSpeed: 3.9ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2674/30753 [04:33<45:26, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.7ms\nSpeed: 3.6ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2676/30753 [04:33<45:25, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 3.4ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2678/30753 [04:33<46:11, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2680/30753 [04:33<45:50, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2682/30753 [04:33<45:42, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.5ms\nSpeed: 3.4ms preprocess, 26.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2684/30753 [04:34<45:38, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2686/30753 [04:34<45:41, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2688/30753 [04:34<45:32, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▊         | 2690/30753 [04:34<46:27, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2692/30753 [04:34<45:55, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2694/30753 [04:35<46:12, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2696/30753 [04:35<46:10, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 4.1ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2698/30753 [04:35<45:53, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2700/30753 [04:35<45:49, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2702/30753 [04:35<47:05,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2704/30753 [04:36<47:01,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2706/30753 [04:36<46:47,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 24.5ms\nSpeed: 4.0ms preprocess, 24.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2708/30753 [04:36<47:07,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2709/30753 [04:36<47:08,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2710/30753 [04:36<47:27,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2712/30753 [04:36<46:44, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2713/30753 [04:37<47:22,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2715/30753 [04:37<46:52,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2717/30753 [04:37<46:10, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2719/30753 [04:37<45:57, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2721/30753 [04:37<45:36, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 2.6ms preprocess, 26.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 2.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2723/30753 [04:38<45:03, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2725/30753 [04:38<45:41, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2727/30753 [04:38<46:20, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2729/30753 [04:38<46:27, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2731/30753 [04:38<46:07, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2733/30753 [04:39<45:36, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 2.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 26.2ms\nSpeed: 3.2ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2735/30753 [04:39<45:13, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2737/30753 [04:39<45:42, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2739/30753 [04:39<45:28, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2741/30753 [04:39<45:42, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 2.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2743/30753 [04:40<45:40, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2745/30753 [04:40<45:41, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2747/30753 [04:40<45:37, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2749/30753 [04:40<46:24, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2751/30753 [04:40<47:16,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2752/30753 [04:40<47:42,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2753/30753 [04:41<47:40,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2754/30753 [04:41<47:32,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2755/30753 [04:41<47:56,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2757/30753 [04:41<46:52,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2759/30753 [04:41<46:40, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2760/30753 [04:41<46:43,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.4ms\nSpeed: 3.0ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2761/30753 [04:41<47:14,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2762/30753 [04:41<47:05,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 26.4ms\nSpeed: 4.0ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2764/30753 [04:42<46:22, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2766/30753 [04:42<45:56, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2768/30753 [04:42<45:39, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2770/30753 [04:42<45:26, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2772/30753 [04:42<44:54, 10.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.5ms\nSpeed: 3.0ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2774/30753 [04:43<46:01, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2776/30753 [04:43<46:03, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2778/30753 [04:43<46:51,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2779/30753 [04:43<46:55,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2781/30753 [04:43<46:40,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2782/30753 [04:43<46:51,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2783/30753 [04:44<46:59,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2784/30753 [04:44<46:58,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2785/30753 [04:44<48:07,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2786/30753 [04:44<47:44,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2788/30753 [04:44<47:04,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2789/30753 [04:44<47:19,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 29.1ms\nSpeed: 3.2ms preprocess, 29.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2791/30753 [04:44<46:48,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2793/30753 [04:45<46:42,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.7ms\nSpeed: 4.5ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2794/30753 [04:45<46:53,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2795/30753 [04:45<46:58,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2797/30753 [04:45<47:39,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2798/30753 [04:45<47:28,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2799/30753 [04:45<47:18,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2800/30753 [04:45<47:19,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2801/30753 [04:45<48:56,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.4ms\nSpeed: 4.6ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2802/30753 [04:45<49:14,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2804/30753 [04:46<47:49,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2805/30753 [04:46<49:09,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2806/30753 [04:46<48:38,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2807/30753 [04:46<49:21,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 25.3ms\nSpeed: 4.2ms preprocess, 25.3ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2808/30753 [04:46<49:24,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2809/30753 [04:46<50:10,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2810/30753 [04:46<49:47,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2811/30753 [04:46<48:52,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2812/30753 [04:47<48:17,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2814/30753 [04:47<47:18,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2816/30753 [04:47<46:23, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.7ms\nSpeed: 3.1ms preprocess, 29.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2817/30753 [04:47<46:30, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2819/30753 [04:47<46:12, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2821/30753 [04:47<46:51,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2822/30753 [04:48<46:50,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2824/30753 [04:48<46:37,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2826/30753 [04:48<46:14, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2828/30753 [04:48<46:57,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2830/30753 [04:48<46:19, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2832/30753 [04:48<45:41, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2834/30753 [04:49<46:35,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2835/30753 [04:49<46:38,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2836/30753 [04:49<46:42,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 2.9ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2838/30753 [04:49<46:18, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2840/30753 [04:49<46:14, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2842/30753 [04:49<46:09, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2844/30753 [04:50<46:13, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2846/30753 [04:50<47:16,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2847/30753 [04:50<47:08,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2848/30753 [04:50<47:08,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.1ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2849/30753 [04:50<47:11,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2850/30753 [04:50<49:08,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 25.8ms\nSpeed: 4.3ms preprocess, 25.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2851/30753 [04:50<51:09,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 25.2ms\nSpeed: 4.3ms preprocess, 25.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2852/30753 [04:51<51:22,  9.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2853/30753 [04:51<52:22,  8.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 25.7ms\nSpeed: 4.4ms preprocess, 25.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2854/30753 [04:51<52:05,  8.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2855/30753 [04:51<52:07,  8.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2856/30753 [04:51<50:50,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2857/30753 [04:51<52:11,  8.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2858/30753 [04:51<51:12,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2859/30753 [04:51<51:34,  9.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2860/30753 [04:51<52:05,  8.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2861/30753 [04:52<52:06,  8.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2862/30753 [04:52<50:28,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.4ms\nSpeed: 4.6ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2863/30753 [04:52<49:33,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.9ms\nSpeed: 4.0ms preprocess, 29.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2864/30753 [04:52<49:08,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2865/30753 [04:52<48:47,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2866/30753 [04:52<48:40,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2867/30753 [04:52<48:29,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2868/30753 [04:52<48:18,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2869/30753 [04:52<49:29,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2870/30753 [04:53<48:41,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2872/30753 [04:53<47:25,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2873/30753 [04:53<47:18,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 26.6ms\nSpeed: 3.3ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2875/30753 [04:53<46:32,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2877/30753 [04:53<46:51,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2879/30753 [04:53<46:19, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2881/30753 [04:54<46:56,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2882/30753 [04:54<46:51,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 4.3ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2884/30753 [04:54<46:22, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2886/30753 [04:54<46:05, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2888/30753 [04:54<45:29, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2890/30753 [04:54<45:15, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2892/30753 [04:55<45:26, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2894/30753 [04:55<46:20, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2896/30753 [04:55<46:10, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2898/30753 [04:55<46:04, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2900/30753 [04:55<46:06, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2902/30753 [04:56<47:28,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2903/30753 [04:56<48:39,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2905/30753 [04:56<50:16,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2907/30753 [04:56<48:52,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.1ms\nSpeed: 4.5ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2909/30753 [04:56<47:49,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.4ms\nSpeed: 3.0ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2911/30753 [04:57<47:11,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2912/30753 [04:57<47:13,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2913/30753 [04:57<47:13,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2914/30753 [04:57<47:19,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2915/30753 [04:57<47:14,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2916/30753 [04:57<47:01,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2917/30753 [04:57<48:11,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2918/30753 [04:57<47:54,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2919/30753 [04:57<47:45,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:   9%|▉         | 2920/30753 [04:58<47:51,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2922/30753 [04:58<47:14,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2924/30753 [04:58<46:34,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2925/30753 [04:58<46:32,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2926/30753 [04:58<46:36,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2927/30753 [04:58<47:22,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2928/30753 [04:58<47:09,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2929/30753 [04:58<47:53,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2930/30753 [04:59<47:51,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2932/30753 [04:59<46:55,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2933/30753 [04:59<46:58,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2934/30753 [04:59<46:57,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2935/30753 [04:59<47:00,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2937/30753 [04:59<46:33,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.2ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2939/30753 [04:59<46:07, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2941/30753 [05:00<46:38,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2942/30753 [05:00<46:42,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2944/30753 [05:00<46:14, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2945/30753 [05:00<46:33,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2946/30753 [05:00<46:32,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 4.6ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2947/30753 [05:00<46:36,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2949/30753 [05:00<46:02, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2951/30753 [05:01<47:15,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.9ms\nSpeed: 3.7ms preprocess, 28.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2952/30753 [05:01<47:51,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 30.3ms\nSpeed: 3.5ms preprocess, 30.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2953/30753 [05:01<48:42,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2954/30753 [05:01<48:11,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2955/30753 [05:01<48:48,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2956/30753 [05:01<48:11,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.1ms\nSpeed: 4.3ms preprocess, 26.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2958/30753 [05:01<47:18,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2959/30753 [05:02<47:11,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2960/30753 [05:02<47:32,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2961/30753 [05:02<47:21,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2962/30753 [05:02<47:42,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2964/30753 [05:02<46:26,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.4ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2965/30753 [05:02<47:21,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2966/30753 [05:02<47:47,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2967/30753 [05:02<47:39,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2969/30753 [05:03<46:52,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2970/30753 [05:03<46:50,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2971/30753 [05:03<46:58,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2972/30753 [05:03<46:55,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2973/30753 [05:03<47:04,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2974/30753 [05:03<47:08,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2975/30753 [05:03<46:58,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2976/30753 [05:03<46:59,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2977/30753 [05:03<48:33,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2978/30753 [05:03<48:00,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2980/30753 [05:04<47:08,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2981/30753 [05:04<47:03,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2982/30753 [05:04<47:10,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2983/30753 [05:04<47:03,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2984/30753 [05:04<47:12,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2985/30753 [05:04<47:24,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2986/30753 [05:04<47:30,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2987/30753 [05:04<47:33,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2988/30753 [05:04<47:21,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2989/30753 [05:05<48:33,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2990/30753 [05:05<48:47,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2991/30753 [05:05<48:25,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2992/30753 [05:05<48:09,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2993/30753 [05:05<48:00,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2994/30753 [05:05<48:13,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2995/30753 [05:05<47:45,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2996/30753 [05:05<47:35,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2997/30753 [05:05<47:27,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.0ms\nSpeed: 4.3ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2998/30753 [05:06<47:13,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 2999/30753 [05:06<47:39,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3000/30753 [05:06<47:29,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.4ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3001/30753 [05:06<48:16,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3002/30753 [05:06<48:47,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 4.7ms preprocess, 25.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3003/30753 [05:06<49:54,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3005/30753 [05:06<48:45,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3006/30753 [05:06<48:27,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3008/30753 [05:07<47:17,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3009/30753 [05:07<47:05,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3010/30753 [05:07<47:57,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3011/30753 [05:07<47:52,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.4ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3013/30753 [05:07<47:51,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3014/30753 [05:07<47:29,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3016/30753 [05:07<46:55,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3017/30753 [05:07<47:04,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3018/30753 [05:08<47:04,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.5ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3019/30753 [05:08<47:19,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3020/30753 [05:08<47:32,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3021/30753 [05:08<47:27,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3022/30753 [05:08<47:10,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3023/30753 [05:08<47:16,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 4.2ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3024/30753 [05:08<47:16,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3025/30753 [05:08<48:30,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3026/30753 [05:08<48:01,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3027/30753 [05:09<48:20,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3029/30753 [05:09<47:13,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3030/30753 [05:09<47:16,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3032/30753 [05:09<46:14,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3034/30753 [05:09<45:48, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3036/30753 [05:09<45:46, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3038/30753 [05:10<46:41,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3039/30753 [05:10<46:39,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3040/30753 [05:10<46:40,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3042/30753 [05:10<46:17,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.6ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3043/30753 [05:10<46:43,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3045/30753 [05:10<46:25,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3046/30753 [05:10<46:40,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3047/30753 [05:11<46:34,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3048/30753 [05:11<46:36,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3049/30753 [05:11<47:31,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3050/30753 [05:11<47:13,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3051/30753 [05:11<48:12,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3052/30753 [05:11<48:27,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3053/30753 [05:11<47:58,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3055/30753 [05:11<47:23,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.1ms\nSpeed: 4.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3056/30753 [05:11<47:19,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3057/30753 [05:12<47:08,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.8ms\nSpeed: 3.1ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3059/30753 [05:12<46:30,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3061/30753 [05:12<47:13,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3062/30753 [05:12<47:05,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3064/30753 [05:12<46:30,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3066/30753 [05:12<46:22,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3067/30753 [05:13<46:28,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3068/30753 [05:13<46:30,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3069/30753 [05:13<46:42,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.0ms\nSpeed: 4.6ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3070/30753 [05:13<46:51,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3071/30753 [05:13<46:57,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3072/30753 [05:13<47:01,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3073/30753 [05:13<48:12,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 26.2ms\nSpeed: 4.2ms preprocess, 26.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3074/30753 [05:13<48:06,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|▉         | 3075/30753 [05:13<47:37,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3077/30753 [05:14<47:31,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3078/30753 [05:14<47:22,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3079/30753 [05:14<47:14,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3081/30753 [05:14<46:33,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3082/30753 [05:14<46:29,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3083/30753 [05:14<46:33,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3085/30753 [05:14<46:52,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3087/30753 [05:15<46:23,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3088/30753 [05:15<46:22,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3089/30753 [05:15<46:49,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3090/30753 [05:15<46:53,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3091/30753 [05:15<47:02,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3092/30753 [05:15<46:59,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3093/30753 [05:15<46:48,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 26.7ms\nSpeed: 4.2ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3094/30753 [05:15<46:51,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3095/30753 [05:15<47:06,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3096/30753 [05:16<47:12,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 skateboard, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3097/30753 [05:16<48:29,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3098/30753 [05:16<48:02,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 24.8ms\nSpeed: 5.2ms preprocess, 24.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3099/30753 [05:16<47:40,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3100/30753 [05:16<47:19,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 4.2ms preprocess, 25.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3101/30753 [05:16<50:16,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3102/30753 [05:16<50:14,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3103/30753 [05:16<49:21,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3105/30753 [05:16<48:24,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3106/30753 [05:17<47:53,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3107/30753 [05:17<47:32,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3109/30753 [05:17<47:34,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3110/30753 [05:17<48:00,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.3ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3111/30753 [05:17<47:31,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.4ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3113/30753 [05:17<46:39,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.9ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3114/30753 [05:17<46:34,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3115/30753 [05:17<46:27,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.9ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3117/30753 [05:18<46:08,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.7ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3119/30753 [05:18<46:02, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3121/30753 [05:18<46:51,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3122/30753 [05:18<46:48,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3123/30753 [05:18<46:54,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3124/30753 [05:18<46:58,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3125/30753 [05:19<46:53,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3127/30753 [05:19<46:51,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3128/30753 [05:19<47:00,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3130/30753 [05:19<46:16,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3131/30753 [05:19<46:13,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 3.2ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3133/30753 [05:19<46:56,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3134/30753 [05:19<47:01,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3135/30753 [05:20<46:51,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3136/30753 [05:20<46:52,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3137/30753 [05:20<47:05,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.1ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3138/30753 [05:20<46:58,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3139/30753 [05:20<46:54,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3140/30753 [05:20<46:53,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3142/30753 [05:20<46:21,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3143/30753 [05:20<46:28,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3144/30753 [05:20<46:48,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3145/30753 [05:21<48:10,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3146/30753 [05:21<48:18,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.0ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3147/30753 [05:21<47:54,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.1ms\nSpeed: 4.5ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3148/30753 [05:21<47:40,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.2ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3149/30753 [05:21<47:58,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3150/30753 [05:21<47:36,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3151/30753 [05:21<49:08,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3152/30753 [05:21<49:26,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3153/30753 [05:21<48:37,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3154/30753 [05:21<47:54,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3155/30753 [05:22<48:17,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.9ms\nSpeed: 3.2ms preprocess, 28.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3156/30753 [05:22<48:40,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 25.8ms\nSpeed: 5.3ms preprocess, 25.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3157/30753 [05:22<51:01,  9.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.7ms\nSpeed: 4.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3158/30753 [05:22<50:11,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 25.9ms\nSpeed: 3.5ms preprocess, 25.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3159/30753 [05:22<51:01,  9.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3160/30753 [05:22<52:08,  8.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3161/30753 [05:22<51:35,  8.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3162/30753 [05:22<50:07,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3163/30753 [05:22<49:15,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3164/30753 [05:23<48:44,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3165/30753 [05:23<48:15,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3166/30753 [05:23<48:08,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3167/30753 [05:23<48:15,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3168/30753 [05:23<48:15,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3169/30753 [05:23<48:52,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3170/30753 [05:23<48:05,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3172/30753 [05:23<46:50,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3173/30753 [05:24<46:44,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3174/30753 [05:24<46:34,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3175/30753 [05:24<46:27,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3177/30753 [05:24<46:56,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3178/30753 [05:24<46:55,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3179/30753 [05:24<46:41,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3180/30753 [05:24<46:31,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3181/30753 [05:24<47:45,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3182/30753 [05:24<47:17,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3183/30753 [05:25<46:55,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3185/30753 [05:25<46:37,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3186/30753 [05:25<46:51,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3187/30753 [05:25<46:38,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3188/30753 [05:25<46:48,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.7ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3190/30753 [05:25<46:20,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.5ms\nSpeed: 3.8ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3192/30753 [05:25<45:50, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.1ms\nSpeed: 3.7ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3193/30753 [05:26<46:55,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3194/30753 [05:26<46:53,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3195/30753 [05:26<46:57,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3196/30753 [05:26<47:06,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3197/30753 [05:26<47:35,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 25.7ms\nSpeed: 5.9ms preprocess, 25.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3198/30753 [05:26<48:26,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3199/30753 [05:26<48:02,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3200/30753 [05:26<47:38,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3201/30753 [05:26<48:55,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3202/30753 [05:27<48:59,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3203/30753 [05:27<48:10,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3204/30753 [05:27<47:44,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3205/30753 [05:27<48:56,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3206/30753 [05:27<49:16,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.9ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3207/30753 [05:27<48:49,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3208/30753 [05:27<48:18,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3209/30753 [05:27<47:55,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 4.8ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3210/30753 [05:27<48:09,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3211/30753 [05:27<48:01,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3212/30753 [05:28<48:14,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3213/30753 [05:28<47:33,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3214/30753 [05:28<47:17,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3215/30753 [05:28<47:20,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3216/30753 [05:28<47:36,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3217/30753 [05:28<49:17,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3218/30753 [05:28<48:43,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3219/30753 [05:28<48:43,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3220/30753 [05:28<48:25,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3221/30753 [05:28<48:10,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3222/30753 [05:29<47:41,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3223/30753 [05:29<47:45,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3224/30753 [05:29<47:18,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3225/30753 [05:29<47:32,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3226/30753 [05:29<47:38,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3227/30753 [05:29<48:52,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3228/30753 [05:29<48:50,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  10%|█         | 3229/30753 [05:29<49:50,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3230/30753 [05:29<48:53,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3231/30753 [05:30<48:24,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3232/30753 [05:30<48:00,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3233/30753 [05:30<48:02,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3234/30753 [05:30<48:00,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3235/30753 [05:30<47:33,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3236/30753 [05:30<47:24,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3237/30753 [05:30<47:02,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3238/30753 [05:30<46:56,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3239/30753 [05:30<46:54,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 3.1ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3240/30753 [05:30<46:40,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3241/30753 [05:31<48:07,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3242/30753 [05:31<47:49,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3243/30753 [05:31<47:16,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3244/30753 [05:31<47:30,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3245/30753 [05:31<47:03,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3246/30753 [05:31<46:59,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3247/30753 [05:31<47:06,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3249/30753 [05:31<46:26,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3251/30753 [05:32<47:16,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.6ms\nSpeed: 3.7ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3252/30753 [05:32<47:45,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3253/30753 [05:32<48:37,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3255/30753 [05:32<47:56,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3256/30753 [05:32<47:35,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.5ms\nSpeed: 3.2ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3258/30753 [05:32<46:32,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3260/30753 [05:33<46:28,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 29.4ms\nSpeed: 3.4ms preprocess, 29.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3262/30753 [05:33<45:55,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3264/30753 [05:33<45:55,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3265/30753 [05:33<46:48,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3266/30753 [05:33<46:57,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3267/30753 [05:33<46:47,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 5.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3268/30753 [05:33<46:57,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3269/30753 [05:33<47:06,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3270/30753 [05:34<46:58,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 30.9ms\nSpeed: 4.0ms preprocess, 30.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3271/30753 [05:34<47:10,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3272/30753 [05:34<47:18,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 30.1ms\nSpeed: 4.2ms preprocess, 30.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3273/30753 [05:34<47:40,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3274/30753 [05:34<47:50,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3275/30753 [05:34<47:40,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3276/30753 [05:34<47:13,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3277/30753 [05:34<48:24,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3278/30753 [05:34<47:38,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.0ms\nSpeed: 3.1ms preprocess, 29.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3279/30753 [05:34<47:20,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3280/30753 [05:35<47:04,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3282/30753 [05:35<46:17,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3283/30753 [05:35<46:24,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3284/30753 [05:35<46:40,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 4.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3285/30753 [05:35<46:53,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3286/30753 [05:35<47:00,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3287/30753 [05:35<47:15,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3288/30753 [05:35<47:24,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3289/30753 [05:36<48:19,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3290/30753 [05:36<48:12,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3291/30753 [05:36<47:43,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.4ms\nSpeed: 4.7ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3292/30753 [05:36<47:13,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3293/30753 [05:36<47:28,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 25.2ms\nSpeed: 3.9ms preprocess, 25.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3294/30753 [05:36<49:14,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3295/30753 [05:36<48:28,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3296/30753 [05:36<48:25,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3297/30753 [05:36<48:02,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3298/30753 [05:36<47:48,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3299/30753 [05:37<47:30,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3300/30753 [05:37<47:12,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3301/30753 [05:37<48:16,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3302/30753 [05:37<48:37,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3303/30753 [05:37<47:50,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3305/30753 [05:37<46:13,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3306/30753 [05:37<46:43,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3308/30753 [05:37<45:49,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3310/30753 [05:38<45:39, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3311/30753 [05:38<46:09,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 2.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3312/30753 [05:38<46:12,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3313/30753 [05:38<47:45,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3314/30753 [05:38<47:35,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3315/30753 [05:38<47:26,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3316/30753 [05:38<47:13,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3317/30753 [05:38<46:53,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.0ms\nSpeed: 4.4ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3318/30753 [05:39<46:38,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3320/30753 [05:39<46:02,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3322/30753 [05:39<45:33, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.4ms\nSpeed: 3.1ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3323/30753 [05:39<45:56,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3324/30753 [05:39<45:56,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3325/30753 [05:39<47:04,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3326/30753 [05:39<47:00,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3327/30753 [05:39<47:28,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 3.7ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3328/30753 [05:40<47:28,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3329/30753 [05:40<47:06,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3331/30753 [05:40<45:55,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3332/30753 [05:40<45:54,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3333/30753 [05:40<45:52,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3334/30753 [05:40<45:58,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3336/30753 [05:40<45:03, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3338/30753 [05:41<45:23, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3340/30753 [05:41<44:54, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3342/30753 [05:41<44:48, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3344/30753 [05:41<44:29, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 2.8ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3346/30753 [05:41<44:05, 10.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3348/30753 [05:41<44:16, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3350/30753 [05:42<45:11, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3352/30753 [05:42<46:26,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3354/30753 [05:42<45:59,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3355/30753 [05:42<46:19,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3357/30753 [05:42<45:42,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3359/30753 [05:43<45:09, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3361/30753 [05:43<45:53,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3362/30753 [05:43<45:58,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3364/30753 [05:43<45:41,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3366/30753 [05:43<45:38, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3367/30753 [05:43<45:40,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3368/30753 [05:44<45:50,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 4.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3369/30753 [05:44<45:58,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3370/30753 [05:44<46:08,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3371/30753 [05:44<46:24,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3372/30753 [05:44<46:23,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3373/30753 [05:44<47:13,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3375/30753 [05:44<46:00,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3377/30753 [05:44<45:56,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 2.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3379/30753 [05:45<45:24, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3381/30753 [05:45<45:18, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3383/30753 [05:45<45:25, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 2.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3385/30753 [05:45<45:40,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3387/30753 [05:45<45:19, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3389/30753 [05:46<45:19, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3391/30753 [05:46<45:08, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3393/30753 [05:46<46:44,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3394/30753 [05:46<46:34,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3395/30753 [05:46<46:26,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.4ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3396/30753 [05:46<46:18,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3397/30753 [05:46<47:09,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3398/30753 [05:47<46:53,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3400/30753 [05:47<46:06,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3401/30753 [05:47<47:16,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3402/30753 [05:47<47:45,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3404/30753 [05:47<45:59,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 3.9ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3405/30753 [05:47<46:28,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.9ms\nSpeed: 3.4ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3407/30753 [05:47<45:49,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3409/30753 [05:48<46:25,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 3.7ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3410/30753 [05:48<46:27,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3411/30753 [05:48<46:15,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 4.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3412/30753 [05:48<46:22,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3413/30753 [05:48<46:21,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3415/30753 [05:48<45:39,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3417/30753 [05:48<45:32, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3419/30753 [05:49<45:14, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3421/30753 [05:49<45:35,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 3.6ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3422/30753 [05:49<45:44,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3424/30753 [05:49<45:25, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3426/30753 [05:49<45:21, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3428/30753 [05:50<45:57,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3430/30753 [05:50<45:31, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3432/30753 [05:50<45:22, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3434/30753 [05:50<45:50,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3436/30753 [05:50<45:21, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 29.0ms\nSpeed: 3.3ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3438/30753 [05:51<45:04, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.8ms\nSpeed: 2.8ms preprocess, 28.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3440/30753 [05:51<44:37, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3442/30753 [05:51<44:31, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 3.4ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3444/30753 [05:51<44:41, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 29.0ms\nSpeed: 3.0ms preprocess, 29.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3446/30753 [05:51<45:51,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3447/30753 [05:51<45:51,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.6ms\nSpeed: 4.7ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3449/30753 [05:52<45:24, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3451/30753 [05:52<45:52,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3452/30753 [05:52<46:24,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.5ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3453/30753 [05:52<46:17,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 3.9ms preprocess, 28.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3455/30753 [05:52<46:20,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3457/30753 [05:52<46:32,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█         | 3459/30753 [05:53<46:12,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3460/30753 [05:53<46:20,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3461/30753 [05:53<46:19,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 4.2ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3462/30753 [05:53<46:10,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 25.4ms\nSpeed: 4.2ms preprocess, 25.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3464/30753 [05:53<47:42,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.4ms\nSpeed: 4.1ms preprocess, 26.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3465/30753 [05:53<48:38,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3466/30753 [05:53<48:35,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 25.5ms\nSpeed: 4.4ms preprocess, 25.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 25.7ms\nSpeed: 4.0ms preprocess, 25.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3468/30753 [05:54<48:25,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.6ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3469/30753 [05:54<49:01,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3470/30753 [05:54<48:24,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3471/30753 [05:54<47:55,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3472/30753 [05:54<47:19,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.0ms\nSpeed: 4.4ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3473/30753 [05:54<47:22,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.0ms\nSpeed: 3.9ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3474/30753 [05:54<47:06,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 3.2ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3476/30753 [05:54<45:59,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 4.2ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3477/30753 [05:55<46:43,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3479/30753 [05:55<45:27, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.8ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3481/30753 [05:55<45:53,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 4.0ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3483/30753 [05:55<45:19, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3484/30753 [05:55<45:43,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.8ms\nSpeed: 3.6ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3486/30753 [05:55<45:32,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3487/30753 [05:56<45:37,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3489/30753 [05:56<46:03,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 25.8ms\nSpeed: 4.4ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3490/30753 [05:56<45:56,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3491/30753 [05:56<46:37,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 25.3ms\nSpeed: 4.7ms preprocess, 25.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3492/30753 [05:56<46:52,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3493/30753 [05:56<47:42,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3494/30753 [05:56<47:22,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3496/30753 [05:56<46:23,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3498/30753 [05:57<45:26, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3500/30753 [05:57<44:58, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3502/30753 [05:57<46:04,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3504/30753 [05:57<45:24, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3505/30753 [05:57<46:06,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 26.5ms\nSpeed: 3.0ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3507/30753 [05:58<45:11, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3509/30753 [05:58<44:38, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3511/30753 [05:58<44:49, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3513/30753 [05:58<44:45, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3515/30753 [05:58<44:24, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 2.9ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3517/30753 [05:59<45:12, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3519/30753 [05:59<44:59, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.2ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3521/30753 [05:59<44:53, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 28.6ms\nSpeed: 3.6ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3523/30753 [05:59<44:47, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3525/30753 [05:59<44:38, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3527/30753 [06:00<44:56, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3529/30753 [06:00<45:13, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.3ms\nSpeed: 2.9ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3531/30753 [06:00<44:28, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3533/30753 [06:00<44:15, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  11%|█▏        | 3535/30753 [06:00<44:05, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3537/30753 [06:01<43:47, 10.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3539/30753 [06:01<43:34, 10.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3541/30753 [06:01<44:04, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3543/30753 [06:01<44:08, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3545/30753 [06:01<44:03, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3547/30753 [06:01<44:05, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.2ms\nSpeed: 2.7ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3549/30753 [06:02<43:49, 10.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3551/30753 [06:02<44:35, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3553/30753 [06:02<45:26,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.3ms\nSpeed: 2.9ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 2.9ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3555/30753 [06:02<44:27, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3557/30753 [06:02<44:12, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3559/30753 [06:03<44:16, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 5.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3561/30753 [06:03<44:20, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 26.0ms\nSpeed: 3.3ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3563/30753 [06:03<43:54, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3565/30753 [06:03<44:24, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3567/30753 [06:03<44:04, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3569/30753 [06:04<43:41, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 29.0ms\nSpeed: 3.0ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3571/30753 [06:04<43:39, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.8ms\nSpeed: 3.6ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3573/30753 [06:04<44:06, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.9ms\nSpeed: 3.9ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3575/30753 [06:04<44:17, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.5ms\nSpeed: 2.8ms preprocess, 28.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.9ms\nSpeed: 3.3ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3577/30753 [06:04<45:12, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3579/30753 [06:05<45:06, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3581/30753 [06:05<44:34, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3583/30753 [06:05<44:07, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3585/30753 [06:05<44:06, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 4.2ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3587/30753 [06:05<44:24, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3589/30753 [06:06<45:15, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3591/30753 [06:06<45:27,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3593/30753 [06:06<46:11,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3594/30753 [06:06<46:01,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3596/30753 [06:06<45:32,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 4.5ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 4.7ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3598/30753 [06:07<45:19,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3600/30753 [06:07<45:07, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3602/30753 [06:07<46:11,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.1ms\nSpeed: 4.5ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3604/30753 [06:07<45:35,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3606/30753 [06:07<45:20,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3608/30753 [06:08<44:46, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3610/30753 [06:08<44:06, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3612/30753 [06:08<44:00, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3614/30753 [06:08<45:12, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3616/30753 [06:08<44:57, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.9ms\nSpeed: 3.1ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 29.0ms\nSpeed: 4.3ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3618/30753 [06:09<44:59, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3620/30753 [06:09<44:30, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3622/30753 [06:09<44:06, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3624/30753 [06:09<44:09, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3626/30753 [06:09<45:02, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3628/30753 [06:10<45:06, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 29.1ms\nSpeed: 4.2ms preprocess, 29.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3630/30753 [06:10<44:52, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3632/30753 [06:10<44:37, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3634/30753 [06:10<44:50, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.8ms\nSpeed: 4.0ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3636/30753 [06:10<44:53, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3638/30753 [06:11<45:20,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 3.8ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3640/30753 [06:11<44:51, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 2.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3642/30753 [06:11<44:30, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 2.9ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3644/30753 [06:11<44:20, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 4.1ms preprocess, 28.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3646/30753 [06:11<44:31, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3648/30753 [06:11<44:15, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3650/30753 [06:12<44:59, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3652/30753 [06:12<45:51,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3653/30753 [06:12<45:54,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.2ms\nSpeed: 3.2ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3655/30753 [06:12<45:03, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3657/30753 [06:12<44:53, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3659/30753 [06:13<44:18, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3661/30753 [06:13<44:32, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3663/30753 [06:13<44:14, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 3.8ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3665/30753 [06:13<44:16, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 2.7ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3667/30753 [06:13<44:02, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3669/30753 [06:14<44:21, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3671/30753 [06:14<44:25, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.8ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 4.0ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3673/30753 [06:14<45:23,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3674/30753 [06:14<45:28,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3676/30753 [06:14<45:10,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3677/30753 [06:14<45:41,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 2.8ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3679/30753 [06:15<45:04, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3681/30753 [06:15<44:29, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 2.8ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3683/30753 [06:15<43:54, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 29.2ms\nSpeed: 4.3ms preprocess, 29.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3685/30753 [06:15<44:40, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 30.0ms\nSpeed: 4.2ms preprocess, 30.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3687/30753 [06:15<44:59, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3689/30753 [06:16<44:40, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3691/30753 [06:16<44:47, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 29.1ms\nSpeed: 3.1ms preprocess, 29.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3693/30753 [06:16<45:16,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 25.3ms\nSpeed: 5.0ms preprocess, 25.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3694/30753 [06:16<45:54,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3695/30753 [06:16<45:54,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3696/30753 [06:16<45:44,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3697/30753 [06:16<46:42,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3698/30753 [06:16<46:19,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3699/30753 [06:17<46:00,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3701/30753 [06:17<46:06,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3702/30753 [06:17<46:22,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3704/30753 [06:17<45:40,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3705/30753 [06:17<45:55,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3707/30753 [06:17<45:23,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3709/30753 [06:18<45:30,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3710/30753 [06:18<45:47,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 31.0ms\nSpeed: 2.9ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3712/30753 [06:18<45:26,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3714/30753 [06:18<45:15,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3715/30753 [06:18<45:14,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3716/30753 [06:18<45:20,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.0ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3717/30753 [06:18<45:27,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3719/30753 [06:19<45:07,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3721/30753 [06:19<45:44,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.6ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3722/30753 [06:19<45:40,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 4.4ms preprocess, 25.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3724/30753 [06:19<45:08,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.5ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3725/30753 [06:19<45:12,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3726/30753 [06:19<45:20,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3727/30753 [06:19<45:51,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3729/30753 [06:20<45:25,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3731/30753 [06:20<44:38, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.2ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3733/30753 [06:20<45:11,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3734/30753 [06:20<45:16,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.6ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3735/30753 [06:20<45:16,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3736/30753 [06:20<45:25,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3737/30753 [06:20<45:19,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3738/30753 [06:21<45:24,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3740/30753 [06:21<45:02, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3742/30753 [06:21<44:54, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3744/30753 [06:21<44:44, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 29.8ms\nSpeed: 3.7ms preprocess, 29.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3746/30753 [06:21<45:39,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 4.3ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3748/30753 [06:22<45:19,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.6ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3750/30753 [06:22<45:06,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3751/30753 [06:22<46:08,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3752/30753 [06:22<46:37,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3753/30753 [06:22<46:15,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3755/30753 [06:22<46:15,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.8ms preprocess, 28.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3757/30753 [06:22<46:15,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3759/30753 [06:23<45:36,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3760/30753 [06:23<45:33,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3761/30753 [06:23<45:36,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3763/30753 [06:23<45:03,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3764/30753 [06:23<45:11,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.3ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3766/30753 [06:23<44:49, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3768/30753 [06:24<44:46, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3770/30753 [06:24<45:20,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 30.3ms\nSpeed: 3.1ms preprocess, 30.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3772/30753 [06:24<44:56, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.1ms\nSpeed: 4.0ms preprocess, 26.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3774/30753 [06:24<44:22, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.6ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3776/30753 [06:24<43:57, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 24.0ms\nSpeed: 4.0ms preprocess, 24.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3778/30753 [06:25<46:26,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3779/30753 [06:25<46:40,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3780/30753 [06:25<46:44,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 4.4ms preprocess, 26.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3781/30753 [06:25<47:36,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.3ms\nSpeed: 4.2ms preprocess, 25.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3782/30753 [06:25<47:54,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3783/30753 [06:25<47:31,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3784/30753 [06:25<47:09,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3785/30753 [06:25<47:32,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3786/30753 [06:25<48:18,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3787/30753 [06:26<47:42,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3788/30753 [06:26<47:28,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3790/30753 [06:26<46:03,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3792/30753 [06:26<46:55,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3793/30753 [06:26<47:07,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3795/30753 [06:26<45:51,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3797/30753 [06:27<45:29,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3798/30753 [06:27<45:26,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.2ms\nSpeed: 3.2ms preprocess, 29.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3800/30753 [06:27<45:08,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3801/30753 [06:27<45:51,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.5ms\nSpeed: 3.7ms preprocess, 25.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3802/30753 [06:27<45:55,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 30.1ms\nSpeed: 3.0ms preprocess, 30.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3803/30753 [06:27<45:53,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3805/30753 [06:27<45:56,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3806/30753 [06:27<45:56,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3807/30753 [06:28<46:01,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3809/30753 [06:28<45:18,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 2.9ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3811/30753 [06:28<44:31, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.8ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3813/30753 [06:28<44:14, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3815/30753 [06:28<44:25, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3817/30753 [06:29<45:00,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3818/30753 [06:29<45:00,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.1ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.1ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3820/30753 [06:29<44:28, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3822/30753 [06:29<44:12, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.5ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3824/30753 [06:29<44:19, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.3ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3826/30753 [06:29<44:27, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.4ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3828/30753 [06:30<45:14,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3829/30753 [06:30<45:56,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.8ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3830/30753 [06:30<45:51,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3831/30753 [06:30<45:44,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.5ms\nSpeed: 3.6ms preprocess, 29.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3833/30753 [06:30<45:08,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.4ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3834/30753 [06:30<45:16,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3836/30753 [06:30<44:56,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3837/30753 [06:31<45:16,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3839/30753 [06:31<44:35, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.1ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.9ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3841/30753 [06:31<45:27,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3842/30753 [06:31<45:26,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.5ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  12%|█▏        | 3843/30753 [06:31<45:19,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3845/30753 [06:31<45:01,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3847/30753 [06:32<44:38, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.7ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.6ms\nSpeed: 4.9ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3849/30753 [06:32<44:58,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3850/30753 [06:32<45:07,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3851/30753 [06:32<46:23,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3852/30753 [06:32<46:35,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3853/30753 [06:32<46:50,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 2.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3855/30753 [06:32<45:55,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3857/30753 [06:33<44:54,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 30.0ms\nSpeed: 3.1ms preprocess, 30.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3859/30753 [06:33<45:02,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.3ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3860/30753 [06:33<45:46,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3861/30753 [06:33<45:40,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.9ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3862/30753 [06:33<45:47,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3863/30753 [06:33<45:45,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3864/30753 [06:33<45:57,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3865/30753 [06:33<46:50,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3866/30753 [06:34<46:39,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3867/30753 [06:34<46:22,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3868/30753 [06:34<46:03,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3869/30753 [06:34<46:00,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3871/30753 [06:34<44:58,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3873/30753 [06:34<44:35, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3875/30753 [06:34<44:24, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3877/30753 [06:35<45:08,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.6ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3878/30753 [06:35<45:10,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3880/30753 [06:35<44:33, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3882/30753 [06:35<44:44, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3884/30753 [06:35<44:38, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.5ms preprocess, 28.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 4.2ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3886/30753 [06:36<44:20, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 2.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3888/30753 [06:36<44:07, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.6ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3890/30753 [06:36<45:01,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3891/30753 [06:36<46:35,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3892/30753 [06:36<46:23,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3893/30753 [06:36<46:09,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3894/30753 [06:36<45:50,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.1ms\nSpeed: 4.1ms preprocess, 29.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3895/30753 [06:36<45:49,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3896/30753 [06:37<45:34,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3897/30753 [06:37<45:24,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.8ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3898/30753 [06:37<45:16,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.8ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3899/30753 [06:37<45:22,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3901/30753 [06:37<45:48,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3902/30753 [06:37<46:15,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3904/30753 [06:37<44:44, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3905/30753 [06:37<45:03,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3907/30753 [06:38<44:15, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3909/30753 [06:38<43:50, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3911/30753 [06:38<43:59, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3913/30753 [06:38<45:08,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3915/30753 [06:38<44:37, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3917/30753 [06:39<44:16, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3919/30753 [06:39<43:58, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3921/30753 [06:39<43:58, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.3ms\nSpeed: 4.3ms preprocess, 29.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3923/30753 [06:39<44:19, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3925/30753 [06:39<45:08,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3926/30753 [06:40<45:06,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3927/30753 [06:40<45:33,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.9ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3928/30753 [06:40<45:29,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.6ms\nSpeed: 4.0ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 2.9ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3930/30753 [06:40<44:49,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3932/30753 [06:40<44:48,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.9ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3933/30753 [06:40<45:20,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3935/30753 [06:40<44:41, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 4.4ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3937/30753 [06:41<45:24,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3939/30753 [06:41<44:33, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3941/30753 [06:41<44:01, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3943/30753 [06:41<44:03, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.5ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3945/30753 [06:41<44:04, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3947/30753 [06:42<43:50, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3949/30753 [06:42<44:36, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.7ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3951/30753 [06:42<45:16,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3952/30753 [06:42<45:45,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3953/30753 [06:42<45:34,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.6ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3955/30753 [06:42<45:03,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.7ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3957/30753 [06:43<44:34, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.4ms\nSpeed: 2.7ms preprocess, 29.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3959/30753 [06:43<44:12, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3961/30753 [06:43<45:10,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3962/30753 [06:43<45:11,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 4.3ms preprocess, 28.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3963/30753 [06:43<45:14,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3965/30753 [06:43<44:42,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3966/30753 [06:44<44:55,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3968/30753 [06:44<44:30, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 5.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3970/30753 [06:44<44:32, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3971/30753 [06:44<44:40,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3972/30753 [06:44<44:44,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 2.8ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3973/30753 [06:44<45:37,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.4ms\nSpeed: 3.2ms preprocess, 29.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3974/30753 [06:44<45:24,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3975/30753 [06:44<45:17,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 4.0ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3977/30753 [06:45<45:24,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3978/30753 [06:45<45:15,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3980/30753 [06:45<44:48,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3982/30753 [06:45<44:39,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3983/30753 [06:45<44:50,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3985/30753 [06:45<45:18,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3986/30753 [06:46<45:19,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3988/30753 [06:46<44:50,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3990/30753 [06:46<45:00,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3991/30753 [06:46<45:41,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3992/30753 [06:46<45:39,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3993/30753 [06:46<45:33,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3994/30753 [06:46<45:23,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.8ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3995/30753 [06:46<45:24,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.2ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3997/30753 [06:47<45:49,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 3998/30753 [06:47<45:44,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.0ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4000/30753 [06:47<45:10,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4001/30753 [06:47<46:01,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.3ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4002/30753 [06:47<46:33,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4003/30753 [06:47<46:11,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 4.0ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4004/30753 [06:47<45:52,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4005/30753 [06:48<46:05,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.2ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4006/30753 [06:48<45:46,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.6ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4007/30753 [06:48<45:46,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4008/30753 [06:48<45:27,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4009/30753 [06:48<46:42,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4010/30753 [06:48<46:28,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4011/30753 [06:48<46:00,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 4.6ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.0ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4013/30753 [06:48<45:20,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4014/30753 [06:48<45:11,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4015/30753 [06:49<45:03,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4016/30753 [06:49<44:58,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4018/30753 [06:49<44:17, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.7ms\nSpeed: 2.8ms preprocess, 29.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4020/30753 [06:49<44:02, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4022/30753 [06:49<45:15,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4023/30753 [06:49<45:15,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4024/30753 [06:49<45:14,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4026/30753 [06:50<45:02,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4027/30753 [06:50<45:23,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4028/30753 [06:50<45:17,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.7ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4029/30753 [06:50<45:08,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4030/30753 [06:50<45:15,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4031/30753 [06:50<45:07,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 4.1ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4032/30753 [06:50<45:05,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4033/30753 [06:50<46:17,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4034/30753 [06:50<45:56,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 4.1ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4035/30753 [06:51<45:40,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.2ms preprocess, 28.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4036/30753 [06:51<45:32,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.2ms\nSpeed: 4.1ms preprocess, 29.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4037/30753 [06:51<45:28,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4039/30753 [06:51<44:44,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4040/30753 [06:51<44:43,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4041/30753 [06:51<44:49,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4042/30753 [06:51<44:53,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 5.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4043/30753 [06:51<45:04,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 4.0ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4045/30753 [06:52<45:57,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.5ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4046/30753 [06:52<45:46,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4048/30753 [06:52<44:43,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4050/30753 [06:52<44:11, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4052/30753 [06:52<45:38,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4054/30753 [06:52<45:04,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4055/30753 [06:53<45:27,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4056/30753 [06:53<45:16,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4057/30753 [06:53<46:13,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4058/30753 [06:53<45:59,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4059/30753 [06:53<45:40,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4060/30753 [06:53<45:25,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4061/30753 [06:53<45:33,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4062/30753 [06:53<45:37,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4063/30753 [06:53<45:28,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4064/30753 [06:54<45:41,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4065/30753 [06:54<45:34,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4066/30753 [06:54<45:43,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4067/30753 [06:54<46:00,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4068/30753 [06:54<45:57,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4069/30753 [06:54<47:15,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 5.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4070/30753 [06:54<47:11,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4071/30753 [06:54<46:50,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.5ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4072/30753 [06:54<46:41,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4073/30753 [06:54<46:10,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.2ms\nSpeed: 4.3ms preprocess, 25.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4075/30753 [06:55<45:26,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4076/30753 [06:55<45:30,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.0ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4077/30753 [06:55<46:00,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4078/30753 [06:55<45:42,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4079/30753 [06:55<45:29,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4081/30753 [06:55<45:46,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4082/30753 [06:55<45:37,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4084/30753 [06:56<45:02,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4085/30753 [06:56<45:05,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 4.4ms preprocess, 26.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4086/30753 [06:56<48:42,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 5.6ms preprocess, 27.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4087/30753 [06:56<49:46,  8.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 6.0ms preprocess, 26.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4088/30753 [06:56<52:36,  8.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4089/30753 [06:56<51:13,  8.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 4.2ms preprocess, 25.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4090/30753 [06:56<50:19,  8.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4091/30753 [06:56<49:49,  8.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 5.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4092/30753 [06:57<49:17,  9.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4093/30753 [06:57<49:54,  8.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4094/30753 [06:57<49:35,  8.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4095/30753 [06:57<48:51,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4096/30753 [06:57<49:14,  9.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.5ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4097/30753 [06:57<48:52,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.9ms preprocess, 27.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4098/30753 [06:57<48:15,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4099/30753 [06:57<47:17,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4100/30753 [06:57<46:34,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4101/30753 [06:57<47:50,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4102/30753 [06:58<47:59,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4103/30753 [06:58<48:18,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4104/30753 [06:58<47:10,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4105/30753 [06:58<47:47,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4106/30753 [06:58<46:52,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4108/30753 [06:58<45:37,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4109/30753 [06:58<45:49,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4110/30753 [06:58<45:53,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4111/30753 [06:59<45:43,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 4.0ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4112/30753 [06:59<45:48,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4113/30753 [06:59<45:25,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4114/30753 [06:59<45:21,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4115/30753 [06:59<45:14,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4116/30753 [06:59<45:16,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4117/30753 [06:59<46:46,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4118/30753 [06:59<46:43,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4119/30753 [06:59<46:12,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4120/30753 [06:59<45:57,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4121/30753 [07:00<45:48,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4122/30753 [07:00<45:28,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4123/30753 [07:00<45:40,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4124/30753 [07:00<45:28,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4125/30753 [07:00<45:27,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 30.0ms\nSpeed: 3.1ms preprocess, 30.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4126/30753 [07:00<45:29,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.4ms preprocess, 26.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4127/30753 [07:00<46:14,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4128/30753 [07:00<46:10,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4129/30753 [07:00<47:05,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4130/30753 [07:00<46:30,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4132/30753 [07:01<45:08,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4134/30753 [07:01<44:48,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4135/30753 [07:01<44:59,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4136/30753 [07:01<45:01,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4137/30753 [07:01<44:54,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4138/30753 [07:01<45:02,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4139/30753 [07:01<44:59,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4140/30753 [07:01<45:04,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4141/30753 [07:02<46:16,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4142/30753 [07:02<45:58,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4143/30753 [07:02<45:50,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 30.5ms\nSpeed: 4.1ms preprocess, 30.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4144/30753 [07:02<45:45,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4145/30753 [07:02<45:28,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.9ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4146/30753 [07:02<45:28,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4147/30753 [07:02<45:30,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4148/30753 [07:02<45:14,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4149/30753 [07:02<45:23,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.8ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  13%|█▎        | 4151/30753 [07:03<45:39,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4152/30753 [07:03<46:02,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4153/30753 [07:03<46:23,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4155/30753 [07:03<45:30,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4156/30753 [07:03<45:31,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4157/30753 [07:03<45:29,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.5ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4158/30753 [07:03<45:16,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4160/30753 [07:04<44:46,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4161/30753 [07:04<44:40,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4162/30753 [07:04<44:35,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.8ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4164/30753 [07:04<44:25,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4165/30753 [07:04<45:16,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4167/30753 [07:04<44:41,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4169/30753 [07:04<44:24,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4170/30753 [07:05<44:27,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 2.8ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4172/30753 [07:05<43:51, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4174/30753 [07:05<43:47, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4176/30753 [07:05<43:43, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4178/30753 [07:05<44:47,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4179/30753 [07:05<44:42,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4180/30753 [07:06<44:44,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4181/30753 [07:06<45:00,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4182/30753 [07:06<45:08,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4183/30753 [07:06<44:56,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4184/30753 [07:06<45:45,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 4.5ms preprocess, 25.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4185/30753 [07:06<45:31,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4187/30753 [07:06<44:41,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4189/30753 [07:06<44:57,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4191/30753 [07:07<44:21,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4192/30753 [07:07<44:20,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4193/30753 [07:07<44:20,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4195/30753 [07:07<44:06, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 29.5ms\nSpeed: 2.9ms preprocess, 29.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4197/30753 [07:07<44:24,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4198/30753 [07:07<44:27,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4200/30753 [07:08<44:19,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4201/30753 [07:08<45:00,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4202/30753 [07:08<45:24,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4204/30753 [07:08<44:25,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4205/30753 [07:08<44:55,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.8ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4207/30753 [07:08<44:28,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4209/30753 [07:08<43:55, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4210/30753 [07:09<44:20,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4211/30753 [07:09<44:28,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4212/30753 [07:09<44:39,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4213/30753 [07:09<45:59,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4214/30753 [07:09<45:52,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4215/30753 [07:09<45:51,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 2.8ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4217/30753 [07:09<44:58,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4218/30753 [07:09<44:50,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4219/30753 [07:10<44:43,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4220/30753 [07:10<44:39,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4222/30753 [07:10<44:08, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4224/30753 [07:10<44:02, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4226/30753 [07:10<44:28,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▎        | 4227/30753 [07:10<45:00,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4229/30753 [07:11<44:09, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4231/30753 [07:11<43:25, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4233/30753 [07:11<43:04, 10.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4235/30753 [07:11<43:10, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4237/30753 [07:11<43:48, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4239/30753 [07:11<43:47, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4241/30753 [07:12<43:21, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 4.3ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4243/30753 [07:12<43:28, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 4.6ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4245/30753 [07:12<43:55, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4247/30753 [07:12<44:01, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4249/30753 [07:12<44:30,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4250/30753 [07:13<44:38,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4251/30753 [07:13<45:27,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4252/30753 [07:13<45:52,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 29.2ms\nSpeed: 3.2ms preprocess, 29.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4254/30753 [07:13<44:51,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4255/30753 [07:13<44:53,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.7ms\nSpeed: 2.8ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4257/30753 [07:13<44:12,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4259/30753 [07:13<43:47, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 clock, 28.6ms\nSpeed: 4.0ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4261/30753 [07:14<44:48,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4263/30753 [07:14<44:14,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 2.8ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4265/30753 [07:14<43:53, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4267/30753 [07:14<43:48, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.2ms\nSpeed: 3.0ms preprocess, 29.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4269/30753 [07:14<43:37, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 3.8ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4271/30753 [07:15<43:29, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4273/30753 [07:15<44:01, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 4.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4275/30753 [07:15<43:56, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4277/30753 [07:15<44:14,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4279/30753 [07:15<43:40, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4281/30753 [07:16<43:29, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.6ms\nSpeed: 3.5ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4283/30753 [07:16<43:53, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 25.5ms\nSpeed: 4.4ms preprocess, 25.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4285/30753 [07:16<45:59,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 3.5ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4286/30753 [07:16<46:01,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4287/30753 [07:16<45:51,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4288/30753 [07:16<45:28,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.5ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4290/30753 [07:17<44:41,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 30.3ms\nSpeed: 3.6ms preprocess, 30.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4291/30753 [07:17<44:38,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 29.4ms\nSpeed: 3.1ms preprocess, 29.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4293/30753 [07:17<44:13,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4295/30753 [07:17<44:09,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4296/30753 [07:17<44:12,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4297/30753 [07:17<45:23,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.9ms\nSpeed: 3.7ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4298/30753 [07:17<45:11,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.2ms\nSpeed: 4.5ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4299/30753 [07:18<45:05,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4300/30753 [07:18<44:54,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4301/30753 [07:18<46:01,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4302/30753 [07:18<46:02,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.8ms\nSpeed: 3.4ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4304/30753 [07:18<44:46,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4305/30753 [07:18<45:17,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4306/30753 [07:18<45:06,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.9ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 29.2ms\nSpeed: 3.8ms preprocess, 29.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4308/30753 [07:18<44:32,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4309/30753 [07:19<45:16,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 29.0ms\nSpeed: 4.8ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4310/30753 [07:19<45:19,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4312/30753 [07:19<44:37,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 3.6ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4314/30753 [07:19<44:19,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.8ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4316/30753 [07:19<43:57, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 2.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4318/30753 [07:19<43:19, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4320/30753 [07:20<43:17, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4322/30753 [07:20<44:01, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4324/30753 [07:20<43:37, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4326/30753 [07:20<43:24, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 29.0ms\nSpeed: 4.2ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4328/30753 [07:20<44:19,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4330/30753 [07:21<43:38, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4332/30753 [07:21<43:01, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4334/30753 [07:21<43:34, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 3.5ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4336/30753 [07:21<43:23, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4338/30753 [07:21<43:21, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4340/30753 [07:22<43:19, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4342/30753 [07:22<43:22, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4344/30753 [07:22<43:20, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4346/30753 [07:22<44:01, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4348/30753 [07:22<43:58, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4350/30753 [07:23<43:42, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4352/30753 [07:23<44:59,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4353/30753 [07:23<44:50,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4354/30753 [07:23<44:41,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4355/30753 [07:23<45:09,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4357/30753 [07:23<45:14,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4359/30753 [07:24<44:37,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4360/30753 [07:24<44:46,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4361/30753 [07:24<44:38,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4362/30753 [07:24<44:30,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 2.9ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4363/30753 [07:24<44:31,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4365/30753 [07:24<43:52, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4366/30753 [07:24<44:06,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4367/30753 [07:24<44:17,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4368/30753 [07:24<44:24,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4369/30753 [07:25<45:50,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 4.7ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4370/30753 [07:25<46:05,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4371/30753 [07:25<46:02,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4372/30753 [07:25<45:59,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4373/30753 [07:25<45:40,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.7ms preprocess, 26.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4374/30753 [07:25<45:22,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4376/30753 [07:25<44:21,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4377/30753 [07:25<44:58,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4378/30753 [07:25<44:45,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 2.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4380/30753 [07:26<44:07,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4381/30753 [07:26<45:20,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4382/30753 [07:26<45:00,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4383/30753 [07:26<47:55,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4384/30753 [07:26<46:53,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 2.9ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4385/30753 [07:26<46:08,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4386/30753 [07:26<45:32,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4387/30753 [07:26<45:05,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4388/30753 [07:27<44:45,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4389/30753 [07:27<44:38,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4391/30753 [07:27<44:09,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4393/30753 [07:27<44:29,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4394/30753 [07:27<44:34,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4395/30753 [07:27<46:18,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.9ms\nSpeed: 4.5ms preprocess, 25.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4396/30753 [07:27<47:26,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4397/30753 [07:27<47:58,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 24.4ms\nSpeed: 4.6ms preprocess, 24.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4398/30753 [07:28<48:15,  9.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.1ms\nSpeed: 5.0ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4399/30753 [07:28<48:07,  9.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.4ms\nSpeed: 3.4ms preprocess, 25.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4400/30753 [07:28<47:16,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4401/30753 [07:28<48:30,  9.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 4.7ms preprocess, 26.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4402/30753 [07:28<48:35,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4403/30753 [07:28<48:00,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4404/30753 [07:28<47:09,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 2.7ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4405/30753 [07:28<48:19,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4406/30753 [07:28<48:20,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 25.7ms\nSpeed: 4.6ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4408/30753 [07:29<45:53,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.2ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4409/30753 [07:29<45:26,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4410/30753 [07:29<45:04,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4411/30753 [07:29<45:27,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4412/30753 [07:29<45:11,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.2ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4413/30753 [07:29<44:54,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4415/30753 [07:29<44:22,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4417/30753 [07:30<44:38,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4419/30753 [07:30<44:18,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4421/30753 [07:30<44:04,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4423/30753 [07:30<43:53, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4424/30753 [07:30<43:59,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4425/30753 [07:30<43:59,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.7ms\nSpeed: 3.5ms preprocess, 28.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4426/30753 [07:30<44:05,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4427/30753 [07:31<45:12,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4428/30753 [07:31<44:55,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4429/30753 [07:31<46:14,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.6ms\nSpeed: 4.0ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4430/30753 [07:31<46:05,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4431/30753 [07:31<45:44,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 5.0ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4432/30753 [07:31<45:22,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4434/30753 [07:31<44:51,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4435/30753 [07:31<44:45,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4436/30753 [07:32<44:51,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4437/30753 [07:32<44:48,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4438/30753 [07:32<44:47,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4439/30753 [07:32<44:46,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4441/30753 [07:32<45:11,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4442/30753 [07:32<45:03,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4443/30753 [07:32<44:47,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4445/30753 [07:32<43:59,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4447/30753 [07:33<43:52,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4448/30753 [07:33<43:59,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4449/30753 [07:33<43:59,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4450/30753 [07:33<43:58,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4451/30753 [07:33<45:26,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 4.0ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4452/30753 [07:33<45:48,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4453/30753 [07:33<46:25,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4454/30753 [07:33<45:45,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4455/30753 [07:33<46:01,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4456/30753 [07:34<45:42,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4458/30753 [07:34<44:48,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  14%|█▍        | 4459/30753 [07:34<44:42,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4460/30753 [07:34<45:13,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4461/30753 [07:34<45:05,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4462/30753 [07:34<44:47,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4463/30753 [07:34<44:41,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4464/30753 [07:34<44:48,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4465/30753 [07:34<45:46,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4466/30753 [07:35<45:41,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4468/30753 [07:35<44:38,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4469/30753 [07:35<44:38,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4470/30753 [07:35<44:30,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4471/30753 [07:35<44:44,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4472/30753 [07:35<45:05,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4473/30753 [07:35<44:48,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4475/30753 [07:36<44:20,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4477/30753 [07:36<45:00,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4478/30753 [07:36<45:04,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4480/30753 [07:36<45:44,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4481/30753 [07:36<45:30,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4482/30753 [07:36<45:13,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 3.1ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4484/30753 [07:36<44:25,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4485/30753 [07:37<44:30,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4486/30753 [07:37<44:34,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4487/30753 [07:37<44:40,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.6ms\nSpeed: 3.7ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4488/30753 [07:37<44:43,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4489/30753 [07:37<45:51,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4490/30753 [07:37<45:20,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.4ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4491/30753 [07:37<44:53,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4492/30753 [07:37<44:35,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4493/30753 [07:37<44:32,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4495/30753 [07:38<44:02,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4497/30753 [07:38<43:44, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 4.5ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4498/30753 [07:38<43:51,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4500/30753 [07:38<43:48,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4501/30753 [07:38<44:52,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4502/30753 [07:38<45:24,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4503/30753 [07:38<45:02,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.0ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4505/30753 [07:39<44:24,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4506/30753 [07:39<44:59,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4507/30753 [07:39<44:46,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4509/30753 [07:39<44:09,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4510/30753 [07:39<44:17,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4511/30753 [07:39<44:26,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4513/30753 [07:39<44:48,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4514/30753 [07:39<44:42,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4516/30753 [07:40<44:04,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4518/30753 [07:40<44:03,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4519/30753 [07:40<44:10,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4520/30753 [07:40<44:29,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4521/30753 [07:40<44:27,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4522/30753 [07:40<44:32,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4523/30753 [07:40<44:24,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.1ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4525/30753 [07:41<44:41,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4526/30753 [07:41<44:31,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4527/30753 [07:41<45:09,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4529/30753 [07:41<44:30,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4530/30753 [07:41<44:19,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4531/30753 [07:41<44:15,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4533/30753 [07:41<43:44,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4535/30753 [07:42<43:22, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4537/30753 [07:42<44:05,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4538/30753 [07:42<44:08,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4539/30753 [07:42<44:18,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4540/30753 [07:42<44:16,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 2.9ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4541/30753 [07:42<44:21,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4542/30753 [07:42<44:29,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4543/30753 [07:42<44:33,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4544/30753 [07:43<44:24,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4546/30753 [07:43<43:37, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4548/30753 [07:43<43:35, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4549/30753 [07:43<44:29,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4550/30753 [07:43<44:30,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4551/30753 [07:43<45:35,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 4.3ms preprocess, 26.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4552/30753 [07:43<45:57,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4553/30753 [07:43<45:27,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4555/30753 [07:44<44:55,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4556/30753 [07:44<44:38,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4557/30753 [07:44<44:33,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4559/30753 [07:44<44:08,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4560/30753 [07:44<44:35,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 30.2ms\nSpeed: 3.8ms preprocess, 30.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4561/30753 [07:44<45:37,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4563/30753 [07:44<44:41,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4565/30753 [07:45<44:00,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4566/30753 [07:45<44:04,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4567/30753 [07:45<44:01,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4568/30753 [07:45<44:01,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4569/30753 [07:45<44:03,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4570/30753 [07:45<44:02,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4571/30753 [07:45<44:29,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▍        | 4572/30753 [07:45<44:15,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4649/30753 [07:53<45:07,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4650/30753 [07:53<45:15,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4651/30753 [07:54<46:57,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 4.2ms preprocess, 26.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4652/30753 [07:54<47:20,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4653/30753 [07:54<46:36,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4654/30753 [07:54<45:50,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4655/30753 [07:54<46:28,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4656/30753 [07:54<46:03,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4657/30753 [07:54<46:51,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4658/30753 [07:54<46:17,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4659/30753 [07:54<45:44,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4660/30753 [07:54<46:11,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4661/30753 [07:55<45:53,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4662/30753 [07:55<45:26,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4663/30753 [07:55<45:07,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4664/30753 [07:55<45:22,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4665/30753 [07:55<45:22,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4666/30753 [07:55<45:26,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4667/30753 [07:55<45:28,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4668/30753 [07:55<45:52,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4669/30753 [07:55<47:14,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4670/30753 [07:56<47:05,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4671/30753 [07:56<46:33,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4672/30753 [07:56<47:22,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4673/30753 [07:56<46:59,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4674/30753 [07:56<49:04,  8.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 24.9ms\nSpeed: 4.5ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4675/30753 [07:56<47:32,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4676/30753 [07:56<47:20,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4677/30753 [07:56<47:41,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4678/30753 [07:56<46:52,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4679/30753 [07:57<46:50,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4680/30753 [07:57<46:20,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4681/30753 [07:57<47:32,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4682/30753 [07:57<46:56,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4683/30753 [07:57<46:16,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4684/30753 [07:57<45:40,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4686/30753 [07:57<44:35,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.6ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4687/30753 [07:57<44:34,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4688/30753 [07:57<44:50,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4689/30753 [07:58<44:47,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4690/30753 [07:58<44:36,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.2ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4691/30753 [07:58<44:46,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.2ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4692/30753 [07:58<44:30,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4693/30753 [07:58<45:45,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4694/30753 [07:58<45:38,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 3.9ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4695/30753 [07:58<45:19,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 3.8ms preprocess, 26.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4696/30753 [07:58<45:12,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4697/30753 [07:58<44:56,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4698/30753 [07:59<44:51,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4699/30753 [07:59<45:47,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.1ms\nSpeed: 4.3ms preprocess, 25.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4700/30753 [07:59<47:01,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4701/30753 [07:59<49:36,  8.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4702/30753 [07:59<49:42,  8.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.6ms preprocess, 26.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4703/30753 [07:59<49:03,  8.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 3.8ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4704/30753 [07:59<47:50,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 25.2ms\nSpeed: 4.4ms preprocess, 25.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4705/30753 [07:59<48:12,  9.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4706/30753 [07:59<48:31,  8.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4707/30753 [08:00<47:47,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4708/30753 [08:00<48:23,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4709/30753 [08:00<48:23,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4710/30753 [08:00<47:48,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4711/30753 [08:00<47:40,  9.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4712/30753 [08:00<46:58,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4713/30753 [08:00<46:21,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4714/30753 [08:00<46:12,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4715/30753 [08:00<46:05,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.5ms\nSpeed: 4.0ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4716/30753 [08:00<45:48,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4717/30753 [08:01<46:49,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4718/30753 [08:01<46:26,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4719/30753 [08:01<45:48,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4720/30753 [08:01<45:17,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4721/30753 [08:01<45:17,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4722/30753 [08:01<44:51,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4723/30753 [08:01<44:43,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4724/30753 [08:01<44:54,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4725/30753 [08:01<44:33,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4726/30753 [08:02<44:26,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4727/30753 [08:02<45:17,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4728/30753 [08:02<45:18,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4729/30753 [08:02<45:40,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 2.8ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4731/30753 [08:02<43:50,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.4ms\nSpeed: 3.4ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4733/30753 [08:02<43:03, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.4ms\nSpeed: 2.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4735/30753 [08:02<42:48, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.5ms\nSpeed: 3.4ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4737/30753 [08:03<42:42, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4739/30753 [08:03<42:36, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4741/30753 [08:03<43:29,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4742/30753 [08:03<43:47,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4743/30753 [08:03<43:47,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4744/30753 [08:03<44:09,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4745/30753 [08:03<44:20,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4746/30753 [08:04<44:26,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4747/30753 [08:04<44:30,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4748/30753 [08:04<44:18,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4749/30753 [08:04<44:22,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4750/30753 [08:04<44:21,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4751/30753 [08:04<45:55,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.5ms\nSpeed: 4.2ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4752/30753 [08:04<46:15,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4753/30753 [08:04<47:01,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4754/30753 [08:04<46:02,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4755/30753 [08:05<46:45,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4756/30753 [08:05<46:08,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4757/30753 [08:05<45:40,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4758/30753 [08:05<45:06,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4759/30753 [08:05<44:56,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4760/30753 [08:05<45:30,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 26.0ms\nSpeed: 3.6ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4761/30753 [08:05<44:55,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4762/30753 [08:05<45:05,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4763/30753 [08:05<44:37,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4764/30753 [08:05<44:39,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4765/30753 [08:06<46:13,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  15%|█▌        | 4766/30753 [08:06<45:43,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4767/30753 [08:06<45:58,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4768/30753 [08:06<45:31,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4769/30753 [08:06<46:21,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.0ms\nSpeed: 4.5ms preprocess, 26.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4770/30753 [08:06<46:37,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4771/30753 [08:06<45:58,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4772/30753 [08:06<45:56,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4773/30753 [08:06<45:43,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4774/30753 [08:07<45:39,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4775/30753 [08:07<45:03,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4776/30753 [08:07<44:45,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4777/30753 [08:07<45:52,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4778/30753 [08:07<45:41,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4779/30753 [08:07<45:15,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4780/30753 [08:07<44:51,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4781/30753 [08:07<44:58,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4782/30753 [08:07<45:07,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4783/30753 [08:07<45:10,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 29.4ms\nSpeed: 4.0ms preprocess, 29.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4784/30753 [08:08<45:39,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4785/30753 [08:08<45:13,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4786/30753 [08:08<45:13,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4787/30753 [08:08<45:36,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4788/30753 [08:08<45:45,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.6ms\nSpeed: 4.0ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4789/30753 [08:08<47:12,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4790/30753 [08:08<46:49,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4791/30753 [08:08<46:22,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4792/30753 [08:08<45:57,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4793/30753 [08:09<45:46,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4794/30753 [08:09<45:17,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4795/30753 [08:09<45:09,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4796/30753 [08:09<45:22,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4797/30753 [08:09<45:11,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4798/30753 [08:09<44:57,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4799/30753 [08:09<44:56,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4800/30753 [08:09<44:57,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4801/30753 [08:09<45:56,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4802/30753 [08:09<46:29,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.5ms\nSpeed: 3.3ms preprocess, 26.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4803/30753 [08:10<45:54,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4804/30753 [08:10<45:13,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4805/30753 [08:10<44:40,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4806/30753 [08:10<45:16,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4807/30753 [08:10<45:05,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4808/30753 [08:10<44:45,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4809/30753 [08:10<44:48,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4810/30753 [08:10<44:45,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4811/30753 [08:10<45:30,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4812/30753 [08:10<45:17,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4813/30753 [08:11<46:39,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4814/30753 [08:11<46:26,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4815/30753 [08:11<45:52,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4816/30753 [08:11<45:18,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4817/30753 [08:11<44:53,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4818/30753 [08:11<44:52,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4819/30753 [08:11<44:51,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4820/30753 [08:11<44:48,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4821/30753 [08:11<44:51,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4822/30753 [08:12<45:01,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4823/30753 [08:12<44:57,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4824/30753 [08:12<44:45,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4825/30753 [08:12<46:05,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4826/30753 [08:12<45:47,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4827/30753 [08:12<46:11,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4828/30753 [08:12<45:45,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4829/30753 [08:12<45:17,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4830/30753 [08:12<45:00,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 28.9ms\nSpeed: 4.1ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4831/30753 [08:12<44:51,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 26.1ms\nSpeed: 4.0ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4832/30753 [08:13<44:39,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4833/30753 [08:13<44:43,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4834/30753 [08:13<44:47,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.2ms\nSpeed: 4.4ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4836/30753 [08:13<43:44,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4837/30753 [08:13<44:56,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4838/30753 [08:13<45:02,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4839/30753 [08:13<44:39,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4840/30753 [08:13<44:30,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4841/30753 [08:14<44:29,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4843/30753 [08:14<43:51,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4845/30753 [08:14<43:26,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4846/30753 [08:14<43:46,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4847/30753 [08:14<44:09,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 2.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4848/30753 [08:14<44:03,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4849/30753 [08:14<44:57,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4850/30753 [08:14<44:31,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4851/30753 [08:15<45:52,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4852/30753 [08:15<45:36,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.7ms\nSpeed: 3.1ms preprocess, 28.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4854/30753 [08:15<44:14,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4856/30753 [08:15<44:14,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4857/30753 [08:15<44:12,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 26.4ms\nSpeed: 4.6ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4858/30753 [08:15<43:58,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4859/30753 [08:15<44:05,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4860/30753 [08:15<43:54,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4861/30753 [08:16<45:25,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 29.5ms\nSpeed: 3.9ms preprocess, 29.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4862/30753 [08:16<45:36,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4863/30753 [08:16<45:34,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4865/30753 [08:16<46:00,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.1ms\nSpeed: 4.0ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4867/30753 [08:16<44:28,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4869/30753 [08:16<43:25,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4871/30753 [08:17<43:04, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4873/30753 [08:17<43:26,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4874/30753 [08:17<43:25,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4876/30753 [08:17<43:02, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4877/30753 [08:17<43:37,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4878/30753 [08:17<43:34,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4880/30753 [08:17<43:03, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4882/30753 [08:18<42:46, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4884/30753 [08:18<42:28, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4886/30753 [08:18<43:14,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4888/30753 [08:18<42:36, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4890/30753 [08:18<42:13, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4892/30753 [08:19<42:32, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 2.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4894/30753 [08:19<42:29, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4896/30753 [08:19<42:28, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4898/30753 [08:19<43:13,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4899/30753 [08:19<43:18,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4900/30753 [08:19<43:16,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4901/30753 [08:20<44:50,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4902/30753 [08:20<45:21,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4903/30753 [08:20<44:51,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4904/30753 [08:20<44:40,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4905/30753 [08:20<44:19,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4906/30753 [08:20<44:44,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4907/30753 [08:20<44:29,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4908/30753 [08:20<44:10,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4909/30753 [08:20<45:28,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4910/30753 [08:21<45:34,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4911/30753 [08:21<45:42,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4912/30753 [08:21<45:18,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4913/30753 [08:21<44:46,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4914/30753 [08:21<44:50,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4915/30753 [08:21<44:19,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4916/30753 [08:21<44:21,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4917/30753 [08:21<44:21,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4918/30753 [08:21<44:40,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4919/30753 [08:21<44:36,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4920/30753 [08:22<44:37,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4921/30753 [08:22<45:52,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4922/30753 [08:22<45:36,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4923/30753 [08:22<45:12,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 4.1ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4924/30753 [08:22<44:54,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4925/30753 [08:22<44:50,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4926/30753 [08:22<44:24,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4927/30753 [08:22<45:20,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4928/30753 [08:22<45:01,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4929/30753 [08:23<44:34,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.5ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4930/30753 [08:23<44:22,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4932/30753 [08:23<43:51,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4933/30753 [08:23<44:37,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 umbrella, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4934/30753 [08:23<44:32,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 umbrella, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4936/30753 [08:23<43:41,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 umbrella, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4938/30753 [08:23<43:33,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4939/30753 [08:24<43:58,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 26.2ms\nSpeed: 3.2ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4940/30753 [08:24<43:56,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4941/30753 [08:24<44:14,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4942/30753 [08:24<44:10,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 umbrella, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4943/30753 [08:24<44:34,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.5ms\nSpeed: 4.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4944/30753 [08:24<44:40,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4945/30753 [08:24<45:52,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4946/30753 [08:24<45:47,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4947/30753 [08:24<45:25,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4948/30753 [08:24<45:01,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4949/30753 [08:25<45:07,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 umbrella, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4950/30753 [08:25<44:36,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4951/30753 [08:25<45:36,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.8ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4952/30753 [08:25<45:42,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.3ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4953/30753 [08:25<45:05,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4955/30753 [08:25<43:57,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 2.6ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4956/30753 [08:25<44:12,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4957/30753 [08:25<44:50,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4958/30753 [08:26<44:48,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4960/30753 [08:26<43:46,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.5ms preprocess, 27.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4961/30753 [08:26<43:46,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4963/30753 [08:26<45:24,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4964/30753 [08:26<45:06,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4965/30753 [08:26<44:59,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4966/30753 [08:26<44:44,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4967/30753 [08:26<44:41,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4968/30753 [08:27<44:53,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4969/30753 [08:27<46:00,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4970/30753 [08:27<45:34,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4971/30753 [08:27<45:12,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4972/30753 [08:27<44:45,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4973/30753 [08:27<44:53,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4974/30753 [08:27<44:40,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4975/30753 [08:27<44:35,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4976/30753 [08:27<44:33,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4977/30753 [08:28<45:42,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.6ms\nSpeed: 3.4ms preprocess, 25.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4978/30753 [08:28<45:56,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4979/30753 [08:28<45:28,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4980/30753 [08:28<45:12,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4981/30753 [08:28<46:22,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4982/30753 [08:28<45:39,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4983/30753 [08:28<45:18,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4984/30753 [08:28<44:56,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4985/30753 [08:28<44:52,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4986/30753 [08:28<44:43,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4987/30753 [08:29<44:21,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4988/30753 [08:29<44:08,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4989/30753 [08:29<44:18,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4990/30753 [08:29<44:40,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4991/30753 [08:29<44:22,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4992/30753 [08:29<44:14,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4993/30753 [08:29<45:21,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4994/30753 [08:29<45:06,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4995/30753 [08:29<44:38,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4996/30753 [08:29<44:20,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▌        | 4997/30753 [08:30<44:31,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.6ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 4998/30753 [08:30<44:18,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 4999/30753 [08:30<44:02,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.7ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 4.1ms preprocess, 25.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5001/30753 [08:30<45:55,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5002/30753 [08:30<47:56,  8.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5003/30753 [08:30<47:34,  9.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.1ms\nSpeed: 4.3ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5004/30753 [08:30<47:24,  9.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5005/30753 [08:30<50:02,  8.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5006/30753 [08:31<49:51,  8.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.4ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5007/30753 [08:31<48:21,  8.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 4.3ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5008/30753 [08:31<47:03,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5009/30753 [08:31<47:30,  9.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5010/30753 [08:31<46:42,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5011/30753 [08:31<47:14,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5012/30753 [08:31<46:15,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5013/30753 [08:31<45:21,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5015/30753 [08:32<44:05,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.2ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5016/30753 [08:32<43:49,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5017/30753 [08:32<44:58,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5018/30753 [08:32<44:45,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5019/30753 [08:32<44:27,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5020/30753 [08:32<44:05,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5021/30753 [08:32<43:48,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 29.1ms\nSpeed: 3.0ms preprocess, 29.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5022/30753 [08:32<43:54,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5023/30753 [08:32<43:39,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5025/30753 [08:33<43:06,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5026/30753 [08:33<43:26,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5027/30753 [08:33<44:07,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5029/30753 [08:33<44:17,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5031/30753 [08:33<43:30,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 3.0ms preprocess, 26.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5032/30753 [08:33<43:38,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5033/30753 [08:33<43:45,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5034/30753 [08:33<43:57,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5035/30753 [08:34<43:48,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5037/30753 [08:34<43:11,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5038/30753 [08:34<43:34,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5039/30753 [08:34<43:46,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5040/30753 [08:34<43:46,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5041/30753 [08:34<44:49,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5042/30753 [08:34<44:35,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5043/30753 [08:34<44:30,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5044/30753 [08:35<44:15,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5045/30753 [08:35<44:23,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5046/30753 [08:35<44:09,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5047/30753 [08:35<43:55,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5048/30753 [08:35<43:36,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 4.3ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5049/30753 [08:35<43:46,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5050/30753 [08:35<43:41,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5051/30753 [08:35<45:01,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5052/30753 [08:35<45:20,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5053/30753 [08:35<45:51,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5054/30753 [08:36<45:14,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 3.1ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5056/30753 [08:36<44:55,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5057/30753 [08:36<44:37,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5058/30753 [08:36<46:20,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 3.4ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5059/30753 [08:36<45:57,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5060/30753 [08:36<45:15,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5061/30753 [08:36<44:55,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5062/30753 [08:36<44:36,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5063/30753 [08:37<44:05,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5064/30753 [08:37<43:58,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5065/30753 [08:37<44:49,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5066/30753 [08:37<44:20,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5068/30753 [08:37<42:49, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5070/30753 [08:37<42:21, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5072/30753 [08:37<42:00, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  16%|█▋        | 5074/30753 [08:38<42:11, 10.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 2.8ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5076/30753 [08:38<41:49, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5078/30753 [08:38<42:53,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.5ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5080/30753 [08:38<42:30, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5082/30753 [08:38<42:24, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5084/30753 [08:39<42:32, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5086/30753 [08:39<42:18, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5088/30753 [08:39<42:17, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5090/30753 [08:39<42:47,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.0ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5092/30753 [08:39<42:34, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5094/30753 [08:40<42:30, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 2.9ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5096/30753 [08:40<42:00, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5098/30753 [08:40<41:46, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5100/30753 [08:40<41:47, 10.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5102/30753 [08:40<43:15,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5103/30753 [08:40<43:21,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5104/30753 [08:41<43:19,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.1ms\nSpeed: 3.4ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5105/30753 [08:41<43:46,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5106/30753 [08:41<43:35,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 25.8ms\nSpeed: 3.0ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5108/30753 [08:41<43:11,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5109/30753 [08:41<43:13,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5110/30753 [08:41<43:50,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5111/30753 [08:41<43:52,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5112/30753 [08:41<43:56,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5113/30753 [08:42<45:16,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5114/30753 [08:42<45:06,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.5ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5115/30753 [08:42<44:29,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5116/30753 [08:42<44:19,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5117/30753 [08:42<44:04,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5118/30753 [08:42<44:11,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5119/30753 [08:42<44:07,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.8ms\nSpeed: 4.2ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5120/30753 [08:42<44:20,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5121/30753 [08:42<44:21,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5122/30753 [08:42<44:06,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5123/30753 [08:43<44:21,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5124/30753 [08:43<44:11,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5125/30753 [08:43<45:12,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5126/30753 [08:43<44:29,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5127/30753 [08:43<45:05,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5128/30753 [08:43<44:27,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 4.1ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5129/30753 [08:43<44:07,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5130/30753 [08:43<44:01,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5131/30753 [08:43<44:05,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5132/30753 [08:43<44:19,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5133/30753 [08:44<44:11,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 25.9ms\nSpeed: 4.4ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5134/30753 [08:44<43:56,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5135/30753 [08:44<43:38,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5136/30753 [08:44<43:42,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5137/30753 [08:44<44:45,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 4.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5138/30753 [08:44<44:28,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5139/30753 [08:44<44:18,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5140/30753 [08:44<44:18,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5141/30753 [08:44<44:32,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5142/30753 [08:45<44:46,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5143/30753 [08:45<44:44,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5144/30753 [08:45<44:44,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5145/30753 [08:45<44:30,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5146/30753 [08:45<44:29,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5147/30753 [08:45<44:25,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 25.6ms\nSpeed: 3.9ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5148/30753 [08:45<44:08,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5149/30753 [08:45<45:24,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5150/30753 [08:45<44:45,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5151/30753 [08:45<45:58,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5152/30753 [08:46<46:13,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5153/30753 [08:46<45:24,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5154/30753 [08:46<45:32,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5155/30753 [08:46<44:50,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5156/30753 [08:46<48:32,  8.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5157/30753 [08:46<47:07,  9.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5158/30753 [08:46<45:58,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5159/30753 [08:46<45:33,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5160/30753 [08:46<45:06,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5161/30753 [08:47<45:46,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5162/30753 [08:47<46:19,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5163/30753 [08:47<45:51,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5164/30753 [08:47<45:31,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5165/30753 [08:47<45:14,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5166/30753 [08:47<44:53,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5167/30753 [08:47<44:38,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5168/30753 [08:47<44:35,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5169/30753 [08:47<44:04,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5170/30753 [08:48<43:46,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5171/30753 [08:48<43:45,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5172/30753 [08:48<43:57,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5173/30753 [08:48<45:13,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5174/30753 [08:48<44:58,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5175/30753 [08:48<44:34,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.6ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5176/30753 [08:48<44:18,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5177/30753 [08:48<45:12,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5178/30753 [08:48<44:50,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5179/30753 [08:48<44:33,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5180/30753 [08:49<44:36,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.7ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5181/30753 [08:49<44:17,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5182/30753 [08:49<44:03,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5183/30753 [08:49<43:50,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5184/30753 [08:49<44:05,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5185/30753 [08:49<45:22,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5186/30753 [08:49<45:11,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5187/30753 [08:49<44:40,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5189/30753 [08:49<43:59,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5190/30753 [08:50<43:57,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5191/30753 [08:50<43:55,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5192/30753 [08:50<44:19,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5193/30753 [08:50<44:18,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5194/30753 [08:50<44:20,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5195/30753 [08:50<44:05,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5196/30753 [08:50<43:49,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5197/30753 [08:50<44:57,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5198/30753 [08:50<44:44,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5200/30753 [08:51<43:32,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5201/30753 [08:51<44:44,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5202/30753 [08:51<45:12,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5203/30753 [08:51<44:35,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5205/30753 [08:51<43:34,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5206/30753 [08:51<44:07,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5207/30753 [08:51<44:07,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 26.2ms\nSpeed: 2.9ms preprocess, 26.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5209/30753 [08:52<44:24,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5210/30753 [08:52<44:24,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5211/30753 [08:52<44:50,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.1ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5212/30753 [08:52<44:24,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.1ms\nSpeed: 3.1ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5213/30753 [08:52<43:57,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5214/30753 [08:52<43:48,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 2.9ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5215/30753 [08:52<43:32,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5216/30753 [08:52<43:29,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5217/30753 [08:52<43:23,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.1ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5218/30753 [08:52<43:35,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5219/30753 [08:53<44:05,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 25.9ms\nSpeed: 3.2ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5220/30753 [08:53<43:59,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5221/30753 [08:53<45:00,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5222/30753 [08:53<44:37,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5223/30753 [08:53<44:17,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 2.7ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5224/30753 [08:53<44:07,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5225/30753 [08:53<44:19,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5226/30753 [08:53<44:36,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 4.0ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5227/30753 [08:53<45:35,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5228/30753 [08:54<45:36,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5229/30753 [08:54<45:08,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5230/30753 [08:54<44:52,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5231/30753 [08:54<44:40,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5232/30753 [08:54<44:41,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5233/30753 [08:54<46:01,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5234/30753 [08:54<45:40,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5235/30753 [08:54<45:33,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5236/30753 [08:54<45:08,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5237/30753 [08:55<44:49,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5238/30753 [08:55<44:36,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.7ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5239/30753 [08:55<44:21,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5240/30753 [08:55<44:13,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5241/30753 [08:55<44:34,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.3ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5242/30753 [08:55<44:11,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5243/30753 [08:55<44:16,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5244/30753 [08:55<44:14,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5245/30753 [08:55<45:46,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5246/30753 [08:55<45:32,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5247/30753 [08:56<45:30,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.3ms\nSpeed: 3.7ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5248/30753 [08:56<45:02,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5249/30753 [08:56<47:03,  9.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5250/30753 [08:56<46:24,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5251/30753 [08:56<51:09,  8.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5252/30753 [08:56<50:03,  8.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5253/30753 [08:56<48:42,  8.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5254/30753 [08:56<47:18,  8.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5255/30753 [08:56<46:37,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5256/30753 [08:57<45:57,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5257/30753 [08:57<46:40,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5258/30753 [08:57<46:05,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5259/30753 [08:57<46:18,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5260/30753 [08:57<46:03,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5261/30753 [08:57<45:36,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5262/30753 [08:57<45:18,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5263/30753 [08:57<45:01,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5264/30753 [08:57<44:43,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 2.9ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5265/30753 [08:58<44:41,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5266/30753 [08:58<44:30,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5267/30753 [08:58<44:30,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5268/30753 [08:58<44:23,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5269/30753 [08:58<45:26,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5270/30753 [08:58<45:11,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.3ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5271/30753 [08:58<44:52,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5272/30753 [08:58<44:50,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5273/30753 [08:58<44:41,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5274/30753 [08:58<44:45,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5275/30753 [08:59<44:37,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.4ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5276/30753 [08:59<44:42,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5277/30753 [08:59<45:31,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5278/30753 [08:59<45:38,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5279/30753 [08:59<45:50,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5280/30753 [08:59<45:37,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5281/30753 [08:59<46:46,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5282/30753 [08:59<46:31,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5283/30753 [08:59<46:11,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5284/30753 [09:00<45:44,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5285/30753 [09:00<45:30,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 25.7ms\nSpeed: 4.2ms preprocess, 25.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5286/30753 [09:00<44:55,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5287/30753 [09:00<44:47,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5288/30753 [09:00<45:11,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5289/30753 [09:00<45:32,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5290/30753 [09:00<45:25,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5291/30753 [09:00<45:13,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5292/30753 [09:00<45:12,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5293/30753 [09:01<46:20,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5294/30753 [09:01<46:11,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5295/30753 [09:01<45:48,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5296/30753 [09:01<45:27,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 4.0ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5297/30753 [09:01<45:07,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5298/30753 [09:01<44:48,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 4.2ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5299/30753 [09:01<44:36,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5300/30753 [09:01<46:09,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 25.5ms\nSpeed: 5.3ms preprocess, 25.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5301/30753 [09:01<49:20,  8.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5302/30753 [09:02<51:05,  8.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 26.3ms\nSpeed: 4.3ms preprocess, 26.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5303/30753 [09:02<49:40,  8.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 25.7ms\nSpeed: 4.3ms preprocess, 25.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5304/30753 [09:02<51:56,  8.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5305/30753 [09:02<51:06,  8.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 4.1ms preprocess, 26.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5306/30753 [09:02<49:30,  8.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.8ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5307/30753 [09:02<48:42,  8.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 26.2ms\nSpeed: 4.1ms preprocess, 26.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5308/30753 [09:02<48:32,  8.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 17 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5309/30753 [09:02<48:06,  8.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5310/30753 [09:02<47:35,  8.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5311/30753 [09:03<47:23,  8.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.8ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5312/30753 [09:03<47:16,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5313/30753 [09:03<46:35,  9.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5314/30753 [09:03<46:06,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5315/30753 [09:03<45:55,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 4.2ms preprocess, 26.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5316/30753 [09:03<45:39,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 28.9ms\nSpeed: 4.2ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5317/30753 [09:03<46:55,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5318/30753 [09:03<46:30,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5319/30753 [09:03<46:16,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.6ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5320/30753 [09:04<45:47,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5321/30753 [09:04<45:38,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5322/30753 [09:04<45:34,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 18 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5323/30753 [09:04<45:40,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5324/30753 [09:04<45:40,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5325/30753 [09:04<45:29,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5326/30753 [09:04<45:18,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.7ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5327/30753 [09:04<46:10,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5328/30753 [09:04<45:47,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5329/30753 [09:05<46:37,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5330/30753 [09:05<46:19,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5331/30753 [09:05<46:22,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5332/30753 [09:05<46:04,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5333/30753 [09:05<45:53,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5334/30753 [09:05<45:34,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5335/30753 [09:05<45:18,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5336/30753 [09:05<45:24,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5337/30753 [09:05<45:12,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5338/30753 [09:05<45:00,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5339/30753 [09:06<44:44,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5340/30753 [09:06<44:31,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5341/30753 [09:06<45:54,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 2.8ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5342/30753 [09:06<45:30,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5343/30753 [09:06<48:20,  8.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5344/30753 [09:06<47:19,  8.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5345/30753 [09:06<46:31,  9.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5346/30753 [09:06<45:49,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5347/30753 [09:06<45:18,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5348/30753 [09:07<45:14,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5349/30753 [09:07<45:10,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5350/30753 [09:07<45:19,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5351/30753 [09:07<46:38,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 3.6ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5352/30753 [09:07<46:42,  9.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5353/30753 [09:07<46:49,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.4ms\nSpeed: 4.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5354/30753 [09:07<46:11,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5355/30753 [09:07<45:35,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5356/30753 [09:07<45:13,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5357/30753 [09:08<45:43,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5358/30753 [09:08<45:11,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5359/30753 [09:08<44:58,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5360/30753 [09:08<44:36,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5361/30753 [09:08<45:10,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5362/30753 [09:08<45:16,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5363/30753 [09:08<44:55,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5364/30753 [09:08<44:52,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5365/30753 [09:08<45:34,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5366/30753 [09:08<44:50,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5368/30753 [09:09<43:34,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5370/30753 [09:09<42:47,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 16 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5371/30753 [09:09<43:09,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5372/30753 [09:09<43:21,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5373/30753 [09:09<43:23,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5374/30753 [09:09<43:15,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5375/30753 [09:09<43:16,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5376/30753 [09:10<43:17,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5377/30753 [09:10<44:49,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5378/30753 [09:10<44:41,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5379/30753 [09:10<44:13,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5380/30753 [09:10<44:04,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  17%|█▋        | 5381/30753 [09:10<43:54,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.2ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5382/30753 [09:10<43:36,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5383/30753 [09:10<43:29,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5384/30753 [09:10<43:19,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5386/30753 [09:11<42:31,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5387/30753 [09:11<42:28,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5389/30753 [09:11<42:56,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5391/30753 [09:11<42:22,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 9 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 2.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5393/30753 [09:11<42:08, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5395/30753 [09:11<42:27,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5396/30753 [09:12<42:38,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5397/30753 [09:12<42:51,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5398/30753 [09:12<42:57,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5399/30753 [09:12<43:09,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5400/30753 [09:12<43:03,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5401/30753 [09:12<44:07,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5402/30753 [09:12<45:07,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 26.4ms\nSpeed: 3.5ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5404/30753 [09:12<43:36,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5406/30753 [09:13<42:44,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5407/30753 [09:13<43:16,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 26.8ms\nSpeed: 3.4ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5408/30753 [09:13<43:17,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5410/30753 [09:13<42:41,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5411/30753 [09:13<42:54,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5413/30753 [09:13<43:08,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5415/30753 [09:13<42:27,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5416/30753 [09:14<42:25,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 2.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5418/30753 [09:14<42:13, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5420/30753 [09:14<42:09, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5422/30753 [09:14<41:54, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5424/30753 [09:14<41:57, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5426/30753 [09:15<42:31,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5427/30753 [09:15<42:51,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5429/30753 [09:15<42:21,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5431/30753 [09:15<41:46, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5433/30753 [09:15<41:49, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5435/30753 [09:15<42:15,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5437/30753 [09:16<42:51,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5438/30753 [09:16<43:20,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5439/30753 [09:16<43:12,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.5ms preprocess, 27.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5440/30753 [09:16<45:01,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5441/30753 [09:16<44:39,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5442/30753 [09:16<44:09,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5443/30753 [09:16<43:55,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5444/30753 [09:16<43:40,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5445/30753 [09:17<43:21,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5446/30753 [09:17<43:05,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 25.9ms\nSpeed: 4.4ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5448/30753 [09:17<42:20,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 handbags, 1 suitcase, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5449/30753 [09:17<43:19,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 handbags, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5450/30753 [09:17<43:10,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5451/30753 [09:17<44:09,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5452/30753 [09:17<44:30,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5454/30753 [09:17<43:14,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5455/30753 [09:18<43:03,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5456/30753 [09:18<43:31,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5457/30753 [09:18<43:10,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5458/30753 [09:18<42:57,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5459/30753 [09:18<42:57,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5461/30753 [09:18<43:18,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5463/30753 [09:18<42:30,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5465/30753 [09:19<41:56, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5467/30753 [09:19<41:57, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5469/30753 [09:19<42:13,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5470/30753 [09:19<42:17,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5471/30753 [09:19<42:27,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 handbag, 1 suitcase, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5472/30753 [09:19<42:43,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5473/30753 [09:19<43:45,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5474/30753 [09:19<43:31,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5475/30753 [09:20<43:18,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5477/30753 [09:20<43:16,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5478/30753 [09:20<43:10,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5480/30753 [09:20<42:31,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5482/30753 [09:20<42:01, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5484/30753 [09:20<41:51, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5486/30753 [09:21<42:27,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5487/30753 [09:21<42:31,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 4.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5488/30753 [09:21<42:36,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5489/30753 [09:21<42:43,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 26.9ms\nSpeed: 4.6ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5491/30753 [09:21<42:16,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5493/30753 [09:21<42:15,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5494/30753 [09:22<42:26,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5495/30753 [09:22<42:29,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.4ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5497/30753 [09:22<43:02,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.3ms\nSpeed: 4.2ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5498/30753 [09:22<42:54,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5499/30753 [09:22<42:50,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.8ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5500/30753 [09:22<42:39,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5501/30753 [09:22<43:53,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5502/30753 [09:22<44:19,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5503/30753 [09:22<43:56,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5504/30753 [09:23<43:36,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5505/30753 [09:23<43:25,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5506/30753 [09:23<43:36,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5507/30753 [09:23<43:11,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5509/30753 [09:23<43:04,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5510/30753 [09:23<42:56,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5511/30753 [09:23<42:46,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5512/30753 [09:23<42:42,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5513/30753 [09:23<42:56,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 26.5ms\nSpeed: 3.5ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5514/30753 [09:24<42:56,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5516/30753 [09:24<42:31,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 3 backpacks, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5518/30753 [09:24<42:03, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 3 backpacks, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5519/30753 [09:24<42:11,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5520/30753 [09:24<42:38,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5521/30753 [09:24<43:42,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5522/30753 [09:24<43:29,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5523/30753 [09:24<43:18,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5524/30753 [09:25<43:03,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5525/30753 [09:25<43:19,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5526/30753 [09:25<43:25,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5527/30753 [09:25<44:07,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5528/30753 [09:25<43:38,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5529/30753 [09:25<43:09,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5530/30753 [09:25<42:49,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5531/30753 [09:25<42:52,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5532/30753 [09:25<43:00,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5533/30753 [09:26<44:12,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5534/30753 [09:26<43:46,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5536/30753 [09:26<43:18,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5537/30753 [09:26<43:11,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 26.4ms\nSpeed: 4.3ms preprocess, 26.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5538/30753 [09:26<44:47,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5539/30753 [09:26<44:05,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 3 backpacks, 1 clock, 26.5ms\nSpeed: 3.2ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5540/30753 [09:26<43:50,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 3 backpacks, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5541/30753 [09:26<43:28,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 3 backpacks, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 3 backpacks, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5543/30753 [09:27<42:35,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 3 backpacks, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5544/30753 [09:27<42:31,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5545/30753 [09:27<43:47,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5546/30753 [09:27<44:04,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5547/30753 [09:27<43:50,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 25.2ms\nSpeed: 4.6ms preprocess, 25.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5548/30753 [09:27<43:29,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 26.5ms\nSpeed: 4.3ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5549/30753 [09:27<43:22,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5550/30753 [09:27<43:21,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 26.3ms\nSpeed: 3.8ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5551/30753 [09:27<44:36,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5552/30753 [09:27<44:34,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5553/30753 [09:28<43:59,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5554/30753 [09:28<43:56,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5555/30753 [09:28<43:33,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.6ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5556/30753 [09:28<43:10,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5557/30753 [09:28<44:17,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5558/30753 [09:28<44:04,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5559/30753 [09:28<43:39,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.6ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5561/30753 [09:28<42:50,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5562/30753 [09:29<42:49,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 4.3ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5563/30753 [09:29<42:51,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5564/30753 [09:29<42:50,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5565/30753 [09:29<42:58,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5567/30753 [09:29<42:32,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5568/30753 [09:29<42:40,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5569/30753 [09:29<44:02,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5570/30753 [09:29<44:04,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5571/30753 [09:29<43:37,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5572/30753 [09:30<43:12,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5573/30753 [09:30<42:53,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.3ms\nSpeed: 4.4ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.6ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5575/30753 [09:30<42:14,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5577/30753 [09:30<42:32,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5578/30753 [09:30<42:32,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5579/30753 [09:30<42:41,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5580/30753 [09:30<42:48,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.6ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5581/30753 [09:30<43:57,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5582/30753 [09:31<43:35,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5583/30753 [09:31<43:13,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5584/30753 [09:31<43:00,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5585/30753 [09:31<42:48,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5587/30753 [09:31<42:22,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5588/30753 [09:31<42:31,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5589/30753 [09:31<42:29,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5591/30753 [09:31<41:50, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 2.7ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5593/30753 [09:32<42:11,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5595/30753 [09:32<42:04,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5596/30753 [09:32<42:09,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5597/30753 [09:32<42:19,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.3ms\nSpeed: 4.0ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5599/30753 [09:32<41:59,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5601/30753 [09:32<42:33,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.8ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5602/30753 [09:33<43:07,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 24.8ms\nSpeed: 4.2ms preprocess, 24.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5603/30753 [09:33<44:13,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 25.8ms\nSpeed: 4.5ms preprocess, 25.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5604/30753 [09:33<44:22,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5605/30753 [09:33<45:08,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 25.5ms\nSpeed: 4.2ms preprocess, 25.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5606/30753 [09:33<45:26,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5607/30753 [09:33<45:26,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5608/30753 [09:33<44:47,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5609/30753 [09:33<46:05,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5610/30753 [09:33<45:16,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5611/30753 [09:34<45:29,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5612/30753 [09:34<45:45,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 25.8ms\nSpeed: 4.9ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5613/30753 [09:34<44:47,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 29.0ms\nSpeed: 3.4ms preprocess, 29.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5614/30753 [09:34<44:31,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5616/30753 [09:34<42:58,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5617/30753 [09:34<43:17,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5618/30753 [09:34<42:56,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5620/30753 [09:35<42:09,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.0ms preprocess, 26.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5622/30753 [09:35<41:27, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 25.8ms\nSpeed: 4.4ms preprocess, 25.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5624/30753 [09:35<41:34, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5626/30753 [09:35<41:36, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5628/30753 [09:35<42:06,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5629/30753 [09:35<42:50,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5631/30753 [09:36<42:07,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5633/30753 [09:36<41:51, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5635/30753 [09:36<42:47,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 29.2ms\nSpeed: 3.4ms preprocess, 29.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5636/30753 [09:36<43:06,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5637/30753 [09:36<43:01,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 4.3ms preprocess, 28.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5638/30753 [09:36<43:03,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.5ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5639/30753 [09:36<43:00,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5640/30753 [09:37<43:02,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5641/30753 [09:37<44:25,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5642/30753 [09:37<44:00,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5643/30753 [09:37<43:37,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5644/30753 [09:37<43:24,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5645/30753 [09:37<43:10,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5646/30753 [09:37<42:58,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5647/30753 [09:37<42:49,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 4.3ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5648/30753 [09:37<42:43,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5650/30753 [09:38<42:04,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5651/30753 [09:38<43:18,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.2ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5652/30753 [09:38<43:44,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5653/30753 [09:38<44:09,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5655/30753 [09:38<43:24,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5656/30753 [09:38<43:08,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5657/30753 [09:38<42:48,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5658/30753 [09:38<42:40,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5659/30753 [09:38<42:28,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5660/30753 [09:39<42:27,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5662/30753 [09:39<41:52,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 3.5ms preprocess, 26.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5664/30753 [09:39<41:35, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5666/30753 [09:39<42:46,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5667/30753 [09:39<42:38,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5668/30753 [09:39<42:51,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5669/30753 [09:40<42:40,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5671/30753 [09:40<42:06,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5672/30753 [09:40<42:03,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5673/30753 [09:40<42:13,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5674/30753 [09:40<42:22,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5675/30753 [09:40<42:19,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 2.9ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5677/30753 [09:40<42:34,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5678/30753 [09:40<42:26,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5680/30753 [09:41<41:41, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5682/30753 [09:41<41:36, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 29.2ms\nSpeed: 2.6ms preprocess, 29.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5684/30753 [09:41<41:38, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5686/30753 [09:41<41:34, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.4ms\nSpeed: 3.7ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  18%|█▊        | 5688/30753 [09:41<41:40, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5690/30753 [09:42<42:30,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5692/30753 [09:42<41:57,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5693/30753 [09:42<42:08,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5694/30753 [09:42<42:06,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5696/30753 [09:42<41:49,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5697/30753 [09:42<42:07,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5698/30753 [09:42<42:14,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5699/30753 [09:43<42:27,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5701/30753 [09:43<42:53,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5702/30753 [09:43<43:14,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5703/30753 [09:43<42:58,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5704/30753 [09:43<42:40,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5705/30753 [09:43<43:23,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5706/30753 [09:43<43:21,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5707/30753 [09:43<42:56,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5708/30753 [09:43<42:36,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5709/30753 [09:44<42:36,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5710/30753 [09:44<42:29,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 15 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5711/30753 [09:44<42:16,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 2.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5713/30753 [09:44<42:56,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5714/30753 [09:44<43:00,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5715/30753 [09:44<42:48,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5716/30753 [09:44<42:35,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5717/30753 [09:44<42:34,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5718/30753 [09:44<42:35,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5719/30753 [09:45<42:42,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 3.7ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5720/30753 [09:45<42:41,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5721/30753 [09:45<42:50,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5722/30753 [09:45<42:52,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5723/30753 [09:45<43:00,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5724/30753 [09:45<42:48,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5725/30753 [09:45<43:50,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5726/30753 [09:45<43:22,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5727/30753 [09:45<43:41,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5729/30753 [09:46<42:38,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5731/30753 [09:46<42:23,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5732/30753 [09:46<42:24,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5733/30753 [09:46<44:24,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5734/30753 [09:46<44:09,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.8ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5735/30753 [09:46<44:19,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5736/30753 [09:46<44:20,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5737/30753 [09:46<45:19,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5738/30753 [09:47<44:40,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5739/30753 [09:47<44:10,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.3ms\nSpeed: 3.1ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5741/30753 [09:47<43:13,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5742/30753 [09:47<43:21,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5743/30753 [09:47<43:16,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.7ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5744/30753 [09:47<43:02,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5745/30753 [09:47<43:02,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5746/30753 [09:47<42:56,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5747/30753 [09:47<42:50,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5748/30753 [09:48<42:45,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5749/30753 [09:48<43:47,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5750/30753 [09:48<43:15,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5751/30753 [09:48<44:21,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 3.5ms preprocess, 26.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5752/30753 [09:48<44:35,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5753/30753 [09:48<44:07,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5755/30753 [09:48<43:29,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.5ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5756/30753 [09:48<43:14,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5757/30753 [09:49<43:08,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5758/30753 [09:49<42:56,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5759/30753 [09:49<42:51,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 2.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5760/30753 [09:49<42:57,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5761/30753 [09:49<43:59,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5762/30753 [09:49<43:38,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5764/30753 [09:49<42:24,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▊        | 5765/30753 [09:49<42:14,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5767/30753 [09:50<41:29, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5769/30753 [09:50<41:13, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5771/30753 [09:50<41:21, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5773/30753 [09:50<42:06,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5774/30753 [09:50<42:17,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5776/30753 [09:50<41:46,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5777/30753 [09:51<42:21,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5778/30753 [09:51<42:13,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5779/30753 [09:51<42:10,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5780/30753 [09:51<42:11,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.0ms\nSpeed: 2.9ms preprocess, 26.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5782/30753 [09:51<41:49,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5783/30753 [09:51<41:53,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5784/30753 [09:51<42:10,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5785/30753 [09:51<43:28,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5786/30753 [09:51<43:10,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5787/30753 [09:52<42:59,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5788/30753 [09:52<42:58,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5789/30753 [09:52<42:53,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.0ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5790/30753 [09:52<42:52,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5791/30753 [09:52<43:00,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5792/30753 [09:52<42:36,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5793/30753 [09:52<42:23,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 4.4ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5794/30753 [09:52<42:41,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5795/30753 [09:52<43:07,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5796/30753 [09:53<43:05,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5797/30753 [09:53<44:31,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 4.6ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5798/30753 [09:53<44:16,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5799/30753 [09:53<44:05,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5800/30753 [09:53<44:04,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5801/30753 [09:53<45:30,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5802/30753 [09:53<45:37,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5803/30753 [09:53<44:54,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5804/30753 [09:53<44:24,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5805/30753 [09:53<44:42,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5806/30753 [09:54<43:56,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.1ms\nSpeed: 4.3ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5807/30753 [09:54<43:24,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5808/30753 [09:54<42:54,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5809/30753 [09:54<43:47,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5810/30753 [09:54<44:20,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5811/30753 [09:54<43:47,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5813/30753 [09:54<43:00,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.6ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5814/30753 [09:54<42:46,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5815/30753 [09:55<42:45,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 29.6ms\nSpeed: 3.8ms preprocess, 29.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5816/30753 [09:55<43:09,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5817/30753 [09:55<43:15,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5818/30753 [09:55<43:09,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5819/30753 [09:55<43:20,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5820/30753 [09:55<43:24,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5821/30753 [09:55<44:29,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5822/30753 [09:55<44:16,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5823/30753 [09:55<44:02,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5824/30753 [09:55<43:48,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5825/30753 [09:56<43:43,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5826/30753 [09:56<43:37,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5827/30753 [09:56<46:56,  8.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5828/30753 [09:56<46:08,  9.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 24.9ms\nSpeed: 3.7ms preprocess, 24.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5829/30753 [09:56<46:57,  8.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5830/30753 [09:56<45:38,  9.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 3.3ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5832/30753 [09:56<43:36,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5833/30753 [09:56<44:34,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5834/30753 [09:57<44:06,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 3.6ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 3.1ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5836/30753 [09:57<42:42,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.9ms\nSpeed: 3.0ms preprocess, 28.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5837/30753 [09:57<42:36,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5838/30753 [09:57<42:34,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5839/30753 [09:57<42:43,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5840/30753 [09:57<42:50,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5841/30753 [09:57<42:44,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5842/30753 [09:57<42:42,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5843/30753 [09:57<42:39,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5844/30753 [09:58<42:44,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5845/30753 [09:58<43:55,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5846/30753 [09:58<43:40,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5847/30753 [09:58<43:18,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.3ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5848/30753 [09:58<43:10,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.1ms\nSpeed: 4.3ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5849/30753 [09:58<43:06,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.8ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5850/30753 [09:58<42:51,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5851/30753 [09:58<44:08,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.5ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5852/30753 [09:58<44:38,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5853/30753 [09:59<44:04,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5854/30753 [09:59<43:33,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5855/30753 [09:59<43:50,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5856/30753 [09:59<43:17,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5857/30753 [09:59<44:35,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5858/30753 [09:59<44:01,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5859/30753 [09:59<43:46,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5860/30753 [09:59<43:15,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5861/30753 [09:59<42:51,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5863/30753 [10:00<42:11,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.1ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5865/30753 [10:00<41:35,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 3.3ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 2.8ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5867/30753 [10:00<41:30,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5869/30753 [10:00<42:10,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5870/30753 [10:00<42:22,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 3.1ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5871/30753 [10:00<42:11,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5872/30753 [10:00<42:17,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 4.2ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5873/30753 [10:01<42:38,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5874/30753 [10:01<42:41,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5875/30753 [10:01<42:54,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5876/30753 [10:01<42:39,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5877/30753 [10:01<43:37,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 3.8ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5878/30753 [10:01<43:23,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5879/30753 [10:01<43:10,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5880/30753 [10:01<43:09,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.8ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5881/30753 [10:01<44:19,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5882/30753 [10:02<43:58,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5883/30753 [10:02<43:48,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5884/30753 [10:02<43:38,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5885/30753 [10:02<43:23,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.9ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5886/30753 [10:02<43:13,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5887/30753 [10:02<43:14,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5888/30753 [10:02<43:21,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5889/30753 [10:02<43:05,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5890/30753 [10:02<42:50,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5891/30753 [10:02<42:51,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5892/30753 [10:03<42:44,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5893/30753 [10:03<44:02,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5894/30753 [10:03<43:42,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5895/30753 [10:03<43:17,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 4.1ms preprocess, 26.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5896/30753 [10:03<43:22,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5897/30753 [10:03<43:20,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5898/30753 [10:03<43:07,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5899/30753 [10:03<42:54,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5900/30753 [10:03<42:57,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5901/30753 [10:04<44:00,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5902/30753 [10:04<44:07,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5903/30753 [10:04<43:31,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.3ms\nSpeed: 3.1ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 2.9ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5905/30753 [10:04<43:55,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 4.8ms preprocess, 26.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5906/30753 [10:04<45:41,  9.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5907/30753 [10:04<45:32,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.1ms\nSpeed: 4.4ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5908/30753 [10:04<45:51,  9.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5909/30753 [10:04<46:05,  8.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5910/30753 [10:05<45:44,  9.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5911/30753 [10:05<45:10,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.3ms\nSpeed: 4.2ms preprocess, 25.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5912/30753 [10:05<44:32,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5913/30753 [10:05<44:02,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.3ms\nSpeed: 4.1ms preprocess, 25.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5914/30753 [10:05<43:24,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 26.0ms\nSpeed: 4.0ms preprocess, 26.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5915/30753 [10:05<43:18,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5916/30753 [10:05<43:29,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5917/30753 [10:05<44:49,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5918/30753 [10:05<44:10,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5919/30753 [10:05<43:51,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5920/30753 [10:06<43:34,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.7ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5921/30753 [10:06<43:18,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5922/30753 [10:06<43:35,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.2ms preprocess, 26.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5923/30753 [10:06<43:13,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5924/30753 [10:06<45:57,  9.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.6ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5925/30753 [10:06<45:47,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5926/30753 [10:06<44:36,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5927/30753 [10:06<45:01,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5928/30753 [10:06<44:04,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5929/30753 [10:07<44:25,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5930/30753 [10:07<43:41,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5932/30753 [10:07<42:29,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5933/30753 [10:07<42:15,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5934/30753 [10:07<42:03,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5936/30753 [10:07<41:32,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 3.1ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5937/30753 [10:07<41:34,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5939/30753 [10:08<41:24,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5941/30753 [10:08<41:53,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5943/30753 [10:08<41:34,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.6ms\nSpeed: 3.2ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5945/30753 [10:08<41:25,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 2.8ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5947/30753 [10:08<41:28,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5948/30753 [10:08<41:34,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5949/30753 [10:09<41:41,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5950/30753 [10:09<41:46,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5951/30753 [10:09<43:09,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5952/30753 [10:09<43:33,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.1ms\nSpeed: 4.1ms preprocess, 26.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5953/30753 [10:09<43:50,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5955/30753 [10:09<42:28,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5956/30753 [10:09<42:55,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5957/30753 [10:09<42:57,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 2.8ms preprocess, 27.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5958/30753 [10:09<42:40,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5959/30753 [10:10<42:21,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5960/30753 [10:10<42:18,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5961/30753 [10:10<42:36,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5963/30753 [10:10<42:06,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5964/30753 [10:10<42:07,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5965/30753 [10:10<43:11,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5966/30753 [10:10<43:01,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5968/30753 [10:11<42:13,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5969/30753 [10:11<42:01,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5970/30753 [10:11<42:06,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5971/30753 [10:11<42:18,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5972/30753 [10:11<42:10,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5973/30753 [10:11<42:03,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5974/30753 [10:11<42:10,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.8ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5975/30753 [10:11<42:00,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.0ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5976/30753 [10:11<41:56,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5977/30753 [10:11<43:01,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5978/30753 [10:12<42:38,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5980/30753 [10:12<41:45,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5982/30753 [10:12<41:17, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5983/30753 [10:12<41:32,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5984/30753 [10:12<41:41,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5986/30753 [10:12<41:23,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5987/30753 [10:12<41:30,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5988/30753 [10:13<41:40,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5989/30753 [10:13<42:51,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5990/30753 [10:13<42:37,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5991/30753 [10:13<42:27,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5992/30753 [10:13<42:28,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5993/30753 [10:13<42:22,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5994/30753 [10:13<42:24,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5995/30753 [10:13<42:20,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  19%|█▉        | 5996/30753 [10:13<42:29,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 5997/30753 [10:13<42:09,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 5998/30753 [10:14<42:06,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 5999/30753 [10:14<42:15,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6000/30753 [10:14<42:11,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6001/30753 [10:14<43:05,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6002/30753 [10:14<43:21,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 4.2ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6004/30753 [10:14<42:05,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6006/30753 [10:14<42:05,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.5ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6008/30753 [10:15<41:34,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6010/30753 [10:15<41:23,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6011/30753 [10:15<41:24,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6012/30753 [10:15<41:28,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6013/30753 [10:15<42:15,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6014/30753 [10:15<42:11,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6016/30753 [10:15<41:43,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6017/30753 [10:16<41:47,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.7ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6018/30753 [10:16<41:52,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.3ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6019/30753 [10:16<42:09,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6020/30753 [10:16<42:19,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.7ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6021/30753 [10:16<42:24,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6022/30753 [10:16<44:30,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6023/30753 [10:16<43:53,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 4.8ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6024/30753 [10:16<43:12,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6025/30753 [10:16<44:07,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6026/30753 [10:16<43:25,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6027/30753 [10:17<43:50,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6028/30753 [10:17<43:16,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.8ms preprocess, 28.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6029/30753 [10:17<42:51,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6031/30753 [10:17<41:23,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 3.1ms preprocess, 26.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6033/30753 [10:17<40:53, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 2.9ms preprocess, 26.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6035/30753 [10:17<40:11, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 2.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6037/30753 [10:18<41:07, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6039/30753 [10:18<40:52, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6041/30753 [10:18<40:39, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6043/30753 [10:18<40:55, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6045/30753 [10:18<40:56, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6047/30753 [10:19<40:50, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.2ms preprocess, 26.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6049/30753 [10:19<40:50, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6051/30753 [10:19<41:10, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6052/30753 [10:19<41:38,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.8ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6054/30753 [10:19<41:03, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6056/30753 [10:19<41:28,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6057/30753 [10:20<41:31,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6059/30753 [10:20<41:17,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6061/30753 [10:20<41:52,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6062/30753 [10:20<41:56,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6063/30753 [10:20<41:47,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6065/30753 [10:20<41:31,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6067/30753 [10:21<41:16,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 5.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6069/30753 [10:21<40:44, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6071/30753 [10:21<40:15, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.5ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6073/30753 [10:21<41:18,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.8ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6074/30753 [10:21<41:25,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6076/30753 [10:21<40:56, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6078/30753 [10:22<41:32,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6080/30753 [10:22<41:02, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6082/30753 [10:22<41:01, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6084/30753 [10:22<40:55, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6086/30753 [10:22<41:34,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6088/30753 [10:23<41:01, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6090/30753 [10:23<40:52, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.1ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6092/30753 [10:23<40:57, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6094/30753 [10:23<40:51, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6096/30753 [10:23<40:42, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.4ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6098/30753 [10:24<41:28,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6100/30753 [10:24<40:56, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6102/30753 [10:24<41:56,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6104/30753 [10:24<41:12,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6106/30753 [10:24<41:16,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6108/30753 [10:25<40:56, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6110/30753 [10:25<41:23,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6111/30753 [10:25<41:39,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6112/30753 [10:25<41:42,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6114/30753 [10:25<41:19,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 5.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6116/30753 [10:25<41:03, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6117/30753 [10:26<41:13,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 29.4ms\nSpeed: 4.2ms preprocess, 29.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6118/30753 [10:26<41:29,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6119/30753 [10:26<41:57,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6121/30753 [10:26<43:21,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.1ms\nSpeed: 4.3ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6122/30753 [10:26<42:56,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6124/30753 [10:26<42:05,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6125/30753 [10:26<41:54,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.4ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6127/30753 [10:27<42:08,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6128/30753 [10:27<42:03,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6129/30753 [10:27<41:56,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6130/30753 [10:27<41:49,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6131/30753 [10:27<41:50,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6132/30753 [10:27<41:45,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6133/30753 [10:27<43:10,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6134/30753 [10:27<42:38,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6135/30753 [10:27<42:27,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6137/30753 [10:28<41:55,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 25.2ms\nSpeed: 3.5ms preprocess, 25.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6138/30753 [10:28<42:10,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6140/30753 [10:28<41:16,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6142/30753 [10:28<41:11,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.4ms\nSpeed: 4.4ms preprocess, 26.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6143/30753 [10:28<41:12,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6144/30753 [10:28<41:22,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.5ms preprocess, 26.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6145/30753 [10:28<42:31,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6146/30753 [10:29<42:43,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.8ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6147/30753 [10:29<42:21,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6148/30753 [10:29<42:08,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|█▉        | 6149/30753 [10:29<41:51,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6151/30753 [10:29<42:25,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6152/30753 [10:29<42:50,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.1ms\nSpeed: 4.5ms preprocess, 26.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6154/30753 [10:29<41:47,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6156/30753 [10:30<41:32,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6157/30753 [10:30<42:04,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6158/30753 [10:30<41:53,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6160/30753 [10:30<41:19,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6161/30753 [10:30<41:54,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 2.9ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6163/30753 [10:30<41:27,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6165/30753 [10:30<40:57, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 30.9ms\nSpeed: 3.2ms preprocess, 30.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6166/30753 [10:31<41:04,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6168/30753 [10:31<40:43, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6170/30753 [10:31<41:16,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6171/30753 [10:31<41:24,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6173/30753 [10:31<41:00,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.3ms\nSpeed: 3.8ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6175/30753 [10:31<40:50, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6176/30753 [10:32<40:53, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6177/30753 [10:32<41:22,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6179/30753 [10:32<40:51, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 2.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6181/30753 [10:32<41:16,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6182/30753 [10:32<41:22,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6184/30753 [10:32<40:58,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6186/30753 [10:33<40:47, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6188/30753 [10:33<40:40, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6190/30753 [10:33<40:43, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 4.4ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6192/30753 [10:33<40:53, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6194/30753 [10:33<41:49,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6195/30753 [10:33<41:47,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.5ms\nSpeed: 4.2ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6196/30753 [10:34<41:58,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6197/30753 [10:34<41:52,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.1ms\nSpeed: 4.6ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6199/30753 [10:34<41:28,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6201/30753 [10:34<42:00,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.7ms\nSpeed: 3.2ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6202/30753 [10:34<42:13,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6204/30753 [10:34<41:33,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6205/30753 [10:35<42:11,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6206/30753 [10:35<42:36,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 4.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6207/30753 [10:35<42:31,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6208/30753 [10:35<42:12,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6209/30753 [10:35<42:07,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6210/30753 [10:35<42:00,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.3ms\nSpeed: 4.9ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6211/30753 [10:35<41:50,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6212/30753 [10:35<41:40,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6213/30753 [10:35<43:53,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 25.6ms\nSpeed: 4.2ms preprocess, 25.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6214/30753 [10:35<44:34,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6215/30753 [10:36<44:13,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 4.5ms preprocess, 26.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6216/30753 [10:36<43:32,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6217/30753 [10:36<47:52,  8.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6218/30753 [10:36<46:36,  8.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 24.5ms\nSpeed: 4.2ms preprocess, 24.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6219/30753 [10:36<47:09,  8.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 26.5ms\nSpeed: 3.2ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6220/30753 [10:36<46:00,  8.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6221/30753 [10:36<44:56,  9.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6222/30753 [10:36<44:16,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 clock, 25.8ms\nSpeed: 4.3ms preprocess, 25.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6223/30753 [10:36<43:17,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6224/30753 [10:37<43:20,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6226/30753 [10:37<41:57,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.0ms\nSpeed: 3.1ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6227/30753 [10:37<42:21,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6228/30753 [10:37<42:01,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6229/30753 [10:37<43:06,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6231/30753 [10:37<41:49,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6233/30753 [10:37<41:08,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6235/30753 [10:38<40:43, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6237/30753 [10:38<40:29, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6239/30753 [10:38<40:33, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6241/30753 [10:38<41:07,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6242/30753 [10:38<41:17,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6243/30753 [10:38<41:12,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 4.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6245/30753 [10:39<40:43, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6247/30753 [10:39<40:14, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.2ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6249/30753 [10:39<39:57, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 3.3ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6251/30753 [10:39<40:39, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.0ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6253/30753 [10:39<41:46,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6255/30753 [10:40<41:39,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6257/30753 [10:40<41:06,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6259/30753 [10:40<40:57,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6260/30753 [10:40<41:13,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6261/30753 [10:40<41:12,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 4.4ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6262/30753 [10:40<41:18,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6264/30753 [10:41<41:03,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.7ms\nSpeed: 3.8ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6265/30753 [10:41<41:51,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6266/30753 [10:41<41:41,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 2.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6268/30753 [10:41<40:46, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6269/30753 [10:41<40:53,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.4ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6271/30753 [10:41<40:25, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6273/30753 [10:41<40:23, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6275/30753 [10:42<40:23, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6277/30753 [10:42<41:06,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6278/30753 [10:42<41:06,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6279/30753 [10:42<41:07,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6280/30753 [10:42<41:09,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6282/30753 [10:42<40:40, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6284/30753 [10:43<40:26, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6286/30753 [10:43<40:20, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 3.8ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6288/30753 [10:43<40:18, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6290/30753 [10:43<41:06,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6291/30753 [10:43<41:06,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6292/30753 [10:43<41:14,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6293/30753 [10:44<41:28,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6295/30753 [10:44<40:57,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 29.0ms\nSpeed: 3.8ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6296/30753 [10:44<41:08,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6298/30753 [10:44<40:34, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6300/30753 [10:44<40:38, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6301/30753 [10:44<41:23,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 3.6ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6302/30753 [10:44<41:49,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  20%|██        | 6303/30753 [10:45<41:39,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6305/30753 [10:45<41:38,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6306/30753 [10:45<41:42,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6308/30753 [10:45<41:10,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6309/30753 [10:45<41:19,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6310/30753 [10:45<41:14,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6311/30753 [10:45<41:44,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6313/30753 [10:46<41:59,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6314/30753 [10:46<41:54,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 5.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6315/30753 [10:46<41:43,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 25.5ms\nSpeed: 4.7ms preprocess, 25.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6316/30753 [10:46<41:33,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6317/30753 [10:46<41:31,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 25.9ms\nSpeed: 4.7ms preprocess, 25.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6318/30753 [10:46<43:01,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6319/30753 [10:46<42:34,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6320/30753 [10:46<42:25,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 29.6ms\nSpeed: 3.2ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6321/30753 [10:46<42:14,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6322/30753 [10:46<41:57,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.8ms\nSpeed: 3.4ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 25.7ms\nSpeed: 4.5ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6324/30753 [10:47<41:12,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6325/30753 [10:47<42:03,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6326/30753 [10:47<41:47,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6327/30753 [10:47<42:08,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6328/30753 [10:47<42:01,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6330/30753 [10:47<40:54,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.1ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6332/30753 [10:47<40:18, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6334/30753 [10:48<39:59, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 handbag, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6336/30753 [10:48<39:55, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6338/30753 [10:48<40:53,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 handbag, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6339/30753 [10:48<40:59,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 handbag, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6340/30753 [10:48<41:02,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6341/30753 [10:48<41:06,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 handbag, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6342/30753 [10:48<41:12,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6343/30753 [10:49<41:26,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 handbag, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6344/30753 [10:49<41:19,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6346/30753 [10:49<40:43,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6347/30753 [10:49<40:43,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6349/30753 [10:49<41:40,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6351/30753 [10:49<42:05,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6352/30753 [10:50<42:14,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6353/30753 [10:50<42:04,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.3ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 2.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6355/30753 [10:50<41:48,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6356/30753 [10:50<41:50,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6357/30753 [10:50<41:37,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 handbag, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6359/30753 [10:50<40:52,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6361/30753 [10:50<41:13,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6363/30753 [10:51<40:30, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6365/30753 [10:51<39:54, 10.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6367/30753 [10:51<39:46, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.5ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6369/30753 [10:51<39:50, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6371/30753 [10:51<40:10, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6373/30753 [10:52<40:43,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6374/30753 [10:52<40:47,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6376/30753 [10:52<40:25, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6378/30753 [10:52<40:43,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6380/30753 [10:52<40:20, 10.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6382/30753 [10:52<40:16, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 4.0ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6384/30753 [10:53<40:31, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6386/30753 [10:53<41:17,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6387/30753 [10:53<41:11,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.0ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6388/30753 [10:53<41:08,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6389/30753 [10:53<41:17,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6391/30753 [10:53<40:43,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6393/30753 [10:54<40:34, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6394/30753 [10:54<40:50,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6395/30753 [10:54<41:01,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6396/30753 [10:54<41:01,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6397/30753 [10:54<41:53,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 4.7ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6398/30753 [10:54<41:38,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 4.3ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6400/30753 [10:54<40:59,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.9ms preprocess, 28.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6401/30753 [10:54<42:18,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6402/30753 [10:55<42:50,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6403/30753 [10:55<42:20,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6405/30753 [10:55<41:37,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6406/30753 [10:55<41:23,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.0ms\nSpeed: 4.4ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6408/30753 [10:55<41:13,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6409/30753 [10:55<42:02,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6410/30753 [10:55<42:25,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6412/30753 [10:56<41:34,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6413/30753 [10:56<41:22,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6414/30753 [10:56<42:19,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 4.5ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6415/30753 [10:56<42:07,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6416/30753 [10:56<43:10,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.2ms preprocess, 26.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6417/30753 [10:56<42:52,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6418/30753 [10:56<42:36,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6419/30753 [10:56<42:03,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 handbag, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 handbag, 1 clock, 26.9ms\nSpeed: 4.1ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6421/30753 [10:57<42:09,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6422/30753 [10:57<41:50,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6423/30753 [10:57<41:36,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6424/30753 [10:57<41:38,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6426/30753 [10:57<40:43,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6427/30753 [10:57<41:20,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6428/30753 [10:57<41:19,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6430/30753 [10:57<40:47,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6431/30753 [10:58<40:47,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6433/30753 [10:58<41:04,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.9ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6435/30753 [10:58<40:25, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6437/30753 [10:58<40:09, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 2.8ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6439/30753 [10:58<39:50, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6441/30753 [10:59<40:01, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6443/30753 [10:59<40:15, 10.06it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6445/30753 [10:59<40:56,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6446/30753 [10:59<41:04,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 4.4ms preprocess, 26.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6448/30753 [10:59<40:49,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6449/30753 [10:59<40:53,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6451/30753 [11:00<41:11,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6452/30753 [11:00<41:40,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6454/30753 [11:00<40:39,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6455/30753 [11:00<41:19,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6456/30753 [11:00<41:18,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6457/30753 [11:00<42:17,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6458/30753 [11:00<41:53,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6459/30753 [11:00<41:38,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6460/30753 [11:00<41:55,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6461/30753 [11:01<41:44,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6462/30753 [11:01<41:23,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.7ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6464/30753 [11:01<40:50,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6465/30753 [11:01<40:48,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6466/30753 [11:01<40:56,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6467/30753 [11:01<40:50,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6469/30753 [11:01<41:16,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6471/30753 [11:02<40:45,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.2ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6473/30753 [11:02<40:43,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.4ms\nSpeed: 3.1ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6475/30753 [11:02<40:00, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.6ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6477/30753 [11:02<40:21, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6479/30753 [11:02<40:01, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.9ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6481/30753 [11:03<40:41,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6482/30753 [11:03<40:46,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 2.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6484/30753 [11:03<40:19, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6486/30753 [11:03<39:48, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 2.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6488/30753 [11:03<39:20, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.2ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6490/30753 [11:03<39:13, 10.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6492/30753 [11:04<39:19, 10.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.1ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6494/30753 [11:04<40:06, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6496/30753 [11:04<39:44, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6498/30753 [11:04<39:32, 10.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6500/30753 [11:04<39:25, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6502/30753 [11:05<40:34,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6504/30753 [11:05<40:05, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.3ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6506/30753 [11:05<40:16, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6508/30753 [11:05<40:00, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6510/30753 [11:05<40:17, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6512/30753 [11:06<40:23, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6514/30753 [11:06<40:11, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6516/30753 [11:06<41:24,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6517/30753 [11:06<42:01,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6518/30753 [11:06<41:48,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6519/30753 [11:06<41:31,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6520/30753 [11:06<41:16,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6521/30753 [11:07<41:34,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6522/30753 [11:07<41:56,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 4.2ms preprocess, 26.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6523/30753 [11:07<43:16,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 25.4ms\nSpeed: 4.2ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6524/30753 [11:07<42:44,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6525/30753 [11:07<43:42,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 25.4ms\nSpeed: 4.4ms preprocess, 25.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6526/30753 [11:07<44:15,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.9ms\nSpeed: 4.0ms preprocess, 25.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6527/30753 [11:07<44:59,  8.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6528/30753 [11:07<44:42,  9.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6529/30753 [11:07<44:52,  9.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.4ms\nSpeed: 4.4ms preprocess, 25.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6530/30753 [11:08<43:40,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6532/30753 [11:08<41:50,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6533/30753 [11:08<41:29,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██        | 6535/30753 [11:08<40:56,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6536/30753 [11:08<40:49,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6537/30753 [11:08<40:56,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6538/30753 [11:08<41:00,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6539/30753 [11:08<41:06,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6541/30753 [11:09<41:18,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6543/30753 [11:09<40:49,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6544/30753 [11:09<40:57,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.5ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6545/30753 [11:09<40:59,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6546/30753 [11:09<41:01,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6547/30753 [11:09<41:04,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6548/30753 [11:09<41:04,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6549/30753 [11:09<40:59,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6550/30753 [11:10<40:58,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.5ms\nSpeed: 4.0ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6551/30753 [11:10<42:36,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 25.9ms\nSpeed: 4.5ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6552/30753 [11:10<42:46,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6553/30753 [11:10<43:20,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6554/30753 [11:10<42:38,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6555/30753 [11:10<42:50,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6556/30753 [11:10<42:28,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6557/30753 [11:10<41:59,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.3ms\nSpeed: 4.1ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6558/30753 [11:10<41:54,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6559/30753 [11:11<41:56,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6560/30753 [11:11<41:57,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6562/30753 [11:11<41:14,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 26.9ms\nSpeed: 3.0ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 26.7ms\nSpeed: 3.5ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6564/30753 [11:11<40:39,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6565/30753 [11:11<41:26,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6566/30753 [11:11<41:20,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6568/30753 [11:11<40:41,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6569/30753 [11:12<40:53,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6570/30753 [11:12<40:54,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6571/30753 [11:12<40:56,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6572/30753 [11:12<40:48,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6573/30753 [11:12<40:56,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6574/30753 [11:12<40:47,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6575/30753 [11:12<40:44,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 26.5ms\nSpeed: 3.3ms preprocess, 26.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6576/30753 [11:12<40:48,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6577/30753 [11:12<42:03,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6578/30753 [11:12<41:32,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.1ms\nSpeed: 3.7ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.6ms\nSpeed: 4.2ms preprocess, 28.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6580/30753 [11:13<40:58,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6582/30753 [11:13<40:24,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6584/30753 [11:13<40:09, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.8ms\nSpeed: 3.3ms preprocess, 28.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6586/30753 [11:13<40:15, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6588/30753 [11:13<40:11, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6589/30753 [11:14<41:00,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6590/30753 [11:14<41:10,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6591/30753 [11:14<40:58,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6593/30753 [11:14<40:36,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.5ms\nSpeed: 3.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6595/30753 [11:14<40:19,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6596/30753 [11:14<40:31,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6598/30753 [11:14<40:19,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.8ms\nSpeed: 3.8ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6600/30753 [11:15<40:12, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6601/30753 [11:15<40:58,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6602/30753 [11:15<41:18,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6604/30753 [11:15<40:46,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6605/30753 [11:15<41:06,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6606/30753 [11:15<41:10,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6608/30753 [11:15<40:35,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6609/30753 [11:16<40:36,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.8ms\nSpeed: 4.5ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6610/30753 [11:16<40:52,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  21%|██▏       | 6611/30753 [11:16<41:32,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6613/30753 [11:16<43:37,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6614/30753 [11:16<43:06,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6616/30753 [11:16<41:47,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6617/30753 [11:16<41:42,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6618/30753 [11:17<41:28,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.7ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6619/30753 [11:17<41:20,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6620/30753 [11:17<41:11,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6621/30753 [11:17<41:07,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6622/30753 [11:17<41:08,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6624/30753 [11:17<40:27,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6625/30753 [11:17<41:27,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 2.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6627/30753 [11:17<41:21,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.9ms\nSpeed: 2.9ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6629/30753 [11:18<40:28,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 29.1ms\nSpeed: 3.1ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6631/30753 [11:18<39:51, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6633/30753 [11:18<39:40, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6635/30753 [11:18<39:33, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 5.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6637/30753 [11:18<40:13,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6638/30753 [11:19<40:13,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6640/30753 [11:19<39:39, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6642/30753 [11:19<39:34, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6644/30753 [11:19<39:32, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6646/30753 [11:19<40:05, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6648/30753 [11:20<40:12,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6649/30753 [11:20<40:55,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6650/30753 [11:20<40:58,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6651/30753 [11:20<41:55,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6652/30753 [11:20<42:14,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.7ms\nSpeed: 3.1ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6653/30753 [11:20<42:05,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6654/30753 [11:20<41:41,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6655/30753 [11:20<42:16,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6657/30753 [11:20<40:52,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6659/30753 [11:21<40:14,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6661/30753 [11:21<40:35,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6662/30753 [11:21<40:31,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6663/30753 [11:21<40:29,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6665/30753 [11:21<40:23,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6666/30753 [11:21<40:32,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6667/30753 [11:21<40:37,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 30.1ms\nSpeed: 3.9ms preprocess, 30.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6668/30753 [11:22<40:45,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.2ms\nSpeed: 4.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6670/30753 [11:22<40:23,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6671/30753 [11:22<40:27,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6673/30753 [11:22<41:02,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6675/30753 [11:22<40:36,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.7ms\nSpeed: 3.5ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6677/30753 [11:22<40:48,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6678/30753 [11:23<40:40,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6679/30753 [11:23<40:33,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.5ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6681/30753 [11:23<40:13,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6683/30753 [11:23<39:56, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6685/30753 [11:23<40:44,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6686/30753 [11:23<40:49,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.7ms\nSpeed: 4.3ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6687/30753 [11:23<40:59,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6688/30753 [11:24<41:01,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 2.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6690/30753 [11:24<40:34,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6692/30753 [11:24<40:21,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6693/30753 [11:24<40:26,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.6ms\nSpeed: 4.6ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6694/30753 [11:24<40:41,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6695/30753 [11:24<40:57,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6696/30753 [11:24<40:46,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6697/30753 [11:25<41:44,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6698/30753 [11:25<41:23,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6700/30753 [11:25<40:43,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.8ms\nSpeed: 3.7ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6701/30753 [11:25<42:10,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6702/30753 [11:25<42:45,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6703/30753 [11:25<42:14,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6704/30753 [11:25<42:00,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6705/30753 [11:25<42:45,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.7ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6706/30753 [11:25<42:01,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6707/30753 [11:26<41:43,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 3.2ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6709/30753 [11:26<41:52,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6710/30753 [11:26<41:59,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6711/30753 [11:26<43:10,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 25.5ms\nSpeed: 4.1ms preprocess, 25.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6712/30753 [11:26<42:47,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6713/30753 [11:26<42:14,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.8ms\nSpeed: 4.2ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6714/30753 [11:26<42:05,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6715/30753 [11:26<42:01,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6716/30753 [11:27<41:56,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 4.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6717/30753 [11:27<41:29,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6719/30753 [11:27<40:49,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6720/30753 [11:27<41:03,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6721/30753 [11:27<42:10,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6722/30753 [11:27<41:46,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6724/30753 [11:27<41:07,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6725/30753 [11:27<40:58,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6726/30753 [11:28<41:02,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6727/30753 [11:28<41:36,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6728/30753 [11:28<42:03,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6729/30753 [11:28<41:31,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6730/30753 [11:28<41:07,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6731/30753 [11:28<40:58,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6732/30753 [11:28<40:44,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6733/30753 [11:28<41:49,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 2.9ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6735/30753 [11:28<40:44,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6736/30753 [11:29<40:37,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.8ms\nSpeed: 3.9ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6738/30753 [11:29<40:13,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6740/30753 [11:29<40:02, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6741/30753 [11:29<40:09,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6742/30753 [11:29<40:08,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6743/30753 [11:29<40:06,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6744/30753 [11:29<40:31,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6745/30753 [11:29<41:28,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.5ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6746/30753 [11:30<41:10,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6747/30753 [11:30<40:58,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6748/30753 [11:30<41:03,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6749/30753 [11:30<40:57,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6750/30753 [11:30<40:53,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6751/30753 [11:30<42:05,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 4.8ms preprocess, 25.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6752/30753 [11:30<42:36,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6753/30753 [11:30<42:34,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6754/30753 [11:30<42:22,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6755/30753 [11:31<42:27,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6756/30753 [11:31<41:56,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6757/30753 [11:31<42:52,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.5ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6758/30753 [11:31<42:31,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 4.5ms preprocess, 29.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6759/30753 [11:31<42:27,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.3ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6760/30753 [11:31<42:13,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6761/30753 [11:31<41:55,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.7ms\nSpeed: 3.9ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6762/30753 [11:31<41:38,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 26.4ms\nSpeed: 4.2ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6763/30753 [11:31<41:15,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6764/30753 [11:31<41:20,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6765/30753 [11:32<41:09,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6766/30753 [11:32<41:34,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6767/30753 [11:32<41:16,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6768/30753 [11:32<41:04,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6769/30753 [11:32<42:29,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6770/30753 [11:32<41:56,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6772/30753 [11:32<41:09,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6773/30753 [11:32<41:04,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6774/30753 [11:33<41:00,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.8ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6775/30753 [11:33<40:50,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.5ms\nSpeed: 2.9ms preprocess, 29.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6776/30753 [11:33<40:54,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6777/30753 [11:33<41:25,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6778/30753 [11:33<41:11,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6779/30753 [11:33<41:04,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6780/30753 [11:33<40:56,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6781/30753 [11:33<41:56,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6782/30753 [11:33<41:54,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6783/30753 [11:33<41:41,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 5.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6784/30753 [11:34<41:30,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6785/30753 [11:34<41:17,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6786/30753 [11:34<41:02,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.8ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6787/30753 [11:34<40:55,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6788/30753 [11:34<40:54,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6789/30753 [11:34<40:40,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6790/30753 [11:34<40:29,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6791/30753 [11:34<40:28,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 4.5ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6792/30753 [11:34<40:25,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6793/30753 [11:34<41:39,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6794/30753 [11:35<41:09,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6796/30753 [11:35<40:29,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6797/30753 [11:35<40:36,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6798/30753 [11:35<40:38,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.6ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6799/30753 [11:35<40:47,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6800/30753 [11:35<40:40,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6801/30753 [11:35<41:38,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6802/30753 [11:35<41:39,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6804/30753 [11:36<40:45,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6805/30753 [11:36<41:33,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6806/30753 [11:36<42:10,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6807/30753 [11:36<41:52,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.9ms preprocess, 27.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6808/30753 [11:36<43:49,  9.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6809/30753 [11:36<42:46,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6811/30753 [11:36<41:28,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6812/30753 [11:36<41:08,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 2.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6814/30753 [11:37<40:15,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6816/30753 [11:37<39:44, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6817/30753 [11:37<40:40,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.3ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6818/30753 [11:37<40:49,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.7ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6819/30753 [11:37<40:42,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6820/30753 [11:37<40:43,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6821/30753 [11:37<40:53,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6822/30753 [11:37<40:42,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 4.1ms preprocess, 25.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6824/30753 [11:38<40:18,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6825/30753 [11:38<40:28,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6826/30753 [11:38<40:39,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6827/30753 [11:38<41:27,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6828/30753 [11:38<43:17,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.9ms\nSpeed: 4.3ms preprocess, 25.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6829/30753 [11:38<44:40,  8.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.0ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6830/30753 [11:38<45:44,  8.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 4.6ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6831/30753 [11:38<44:35,  8.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6832/30753 [11:39<44:54,  8.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6833/30753 [11:39<44:05,  9.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.4ms\nSpeed: 4.8ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6834/30753 [11:39<42:57,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6835/30753 [11:39<42:19,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6836/30753 [11:39<42:04,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6837/30753 [11:39<41:47,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6838/30753 [11:39<41:34,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6839/30753 [11:39<41:24,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6840/30753 [11:39<41:31,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6841/30753 [11:39<42:22,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.6ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6842/30753 [11:40<41:48,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6843/30753 [11:40<41:13,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6844/30753 [11:40<40:52,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6845/30753 [11:40<40:59,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.9ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6846/30753 [11:40<40:57,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6847/30753 [11:40<40:37,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6848/30753 [11:40<40:24,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6850/30753 [11:40<40:04,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6851/30753 [11:41<41:18,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 4.1ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6852/30753 [11:41<41:56,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6853/30753 [11:41<42:24,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6854/30753 [11:41<41:52,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6855/30753 [11:41<41:57,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6856/30753 [11:41<41:35,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6857/30753 [11:41<41:07,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6858/30753 [11:41<41:04,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6859/30753 [11:41<40:55,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6860/30753 [11:41<41:13,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6861/30753 [11:42<40:51,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.7ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6863/30753 [11:42<40:30,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6864/30753 [11:42<40:31,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6865/30753 [11:42<41:16,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6866/30753 [11:42<41:00,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6867/30753 [11:42<40:43,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6869/30753 [11:42<40:22,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6870/30753 [11:42<40:17,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6871/30753 [11:43<40:31,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 suitcase, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6872/30753 [11:43<40:30,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.7ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6873/30753 [11:43<40:31,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6874/30753 [11:43<40:24,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6875/30753 [11:43<40:13,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6876/30753 [11:43<40:09,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.0ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6877/30753 [11:43<41:41,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6878/30753 [11:43<41:11,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6879/30753 [11:43<41:09,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6880/30753 [11:43<40:55,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6881/30753 [11:44<41:00,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6882/30753 [11:44<40:55,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6883/30753 [11:44<40:43,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6884/30753 [11:44<40:43,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 2.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6885/30753 [11:44<40:27,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6887/30753 [11:44<39:56,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6889/30753 [11:44<40:34,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6890/30753 [11:45<40:25,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6892/30753 [11:45<40:08,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6893/30753 [11:45<40:13,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6894/30753 [11:45<40:21,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6895/30753 [11:45<40:31,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6896/30753 [11:45<40:24,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6898/30753 [11:45<40:06,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6899/30753 [11:45<40:27,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6900/30753 [11:46<40:29,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6901/30753 [11:46<41:27,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6902/30753 [11:46<41:58,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 25.6ms\nSpeed: 4.9ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6903/30753 [11:46<41:38,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6904/30753 [11:46<41:24,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 4.8ms preprocess, 26.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6905/30753 [11:46<42:31,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6906/30753 [11:46<42:46,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6907/30753 [11:46<42:07,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6908/30753 [11:46<41:51,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6909/30753 [11:46<41:17,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6911/30753 [11:47<40:47,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.2ms\nSpeed: 3.1ms preprocess, 29.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6913/30753 [11:47<41:10,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6914/30753 [11:47<40:58,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6915/30753 [11:47<40:57,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6916/30753 [11:47<40:54,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6917/30753 [11:47<40:59,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6918/30753 [11:47<40:56,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  22%|██▏       | 6919/30753 [11:47<40:55,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.5ms preprocess, 27.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6920/30753 [11:48<40:53,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6921/30753 [11:48<40:39,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6923/30753 [11:48<40:14,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6924/30753 [11:48<40:24,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6925/30753 [11:48<41:30,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6926/30753 [11:48<41:13,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6927/30753 [11:48<41:48,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6928/30753 [11:48<41:26,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.9ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6930/30753 [11:49<40:13,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6932/30753 [11:49<39:56,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6933/30753 [11:49<39:56,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6934/30753 [11:49<39:57,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.6ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6936/30753 [11:49<40:05,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6937/30753 [11:49<41:00,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.6ms\nSpeed: 3.8ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6938/30753 [11:49<40:48,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6939/30753 [11:50<40:42,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 4.6ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6940/30753 [11:50<40:37,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6941/30753 [11:50<40:47,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 4.5ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6942/30753 [11:50<40:46,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6943/30753 [11:50<40:31,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 4.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6944/30753 [11:50<40:52,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 5.7ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6945/30753 [11:50<40:50,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6946/30753 [11:50<40:34,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6947/30753 [11:50<40:50,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6948/30753 [11:50<41:00,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6949/30753 [11:51<42:01,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6950/30753 [11:51<41:31,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6951/30753 [11:51<42:20,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6952/30753 [11:51<42:42,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6953/30753 [11:51<42:03,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.9ms\nSpeed: 3.7ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6954/30753 [11:51<41:27,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6955/30753 [11:51<41:52,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6956/30753 [11:51<41:42,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.6ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6957/30753 [11:51<41:19,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6958/30753 [11:52<40:58,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.5ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6959/30753 [11:52<40:48,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6960/30753 [11:52<41:20,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6961/30753 [11:52<42:07,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6962/30753 [11:52<41:26,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6963/30753 [11:52<40:54,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6964/30753 [11:52<40:34,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6965/30753 [11:52<40:21,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6966/30753 [11:52<40:10,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.6ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6967/30753 [11:52<40:15,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6968/30753 [11:53<40:15,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6969/30753 [11:53<40:10,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6970/30753 [11:53<40:04,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6972/30753 [11:53<39:35, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6973/30753 [11:53<40:46,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.4ms\nSpeed: 4.5ms preprocess, 26.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6974/30753 [11:53<40:45,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6975/30753 [11:53<40:27,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6976/30753 [11:53<40:15,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 4.5ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6977/30753 [11:53<41:12,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6978/30753 [11:54<41:00,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6979/30753 [11:54<40:41,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6980/30753 [11:54<40:26,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6981/30753 [11:54<40:11,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.0ms\nSpeed: 4.0ms preprocess, 29.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6982/30753 [11:54<40:20,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6983/30753 [11:54<40:10,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6984/30753 [11:54<40:00,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.0ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6985/30753 [11:54<41:13,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6986/30753 [11:54<40:50,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6988/30753 [11:55<39:59,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6989/30753 [11:55<40:06,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6990/30753 [11:55<40:01,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6991/30753 [11:55<40:02,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6992/30753 [11:55<40:07,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6993/30753 [11:55<40:13,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6994/30753 [11:55<40:06,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6995/30753 [11:55<40:01,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6996/30753 [11:55<40:04,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 4.6ms preprocess, 27.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6997/30753 [11:56<41:30,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.0ms\nSpeed: 4.7ms preprocess, 26.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6998/30753 [11:56<41:21,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 2.8ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 6999/30753 [11:56<41:03,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.8ms\nSpeed: 3.9ms preprocess, 28.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7000/30753 [11:56<41:49,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7001/30753 [11:56<42:46,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 24.9ms\nSpeed: 6.0ms preprocess, 24.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7002/30753 [11:56<44:27,  8.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7003/30753 [11:56<43:33,  9.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7004/30753 [11:56<42:43,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 4.7ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7005/30753 [11:56<43:06,  9.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7006/30753 [11:56<42:24,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7007/30753 [11:57<41:39,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7008/30753 [11:57<41:24,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 3.1ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7009/30753 [11:57<42:14,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 2.9ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7010/30753 [11:57<41:58,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7011/30753 [11:57<41:20,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7012/30753 [11:57<41:17,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7013/30753 [11:57<40:54,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7014/30753 [11:57<40:44,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7015/30753 [11:57<40:35,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7016/30753 [11:58<40:37,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7017/30753 [11:58<40:28,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7018/30753 [11:58<40:31,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 4.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7019/30753 [11:58<40:31,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7020/30753 [11:58<40:16,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7021/30753 [11:58<41:14,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7022/30753 [11:58<40:58,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7024/30753 [11:58<40:13,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7025/30753 [11:58<40:31,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.8ms\nSpeed: 3.7ms preprocess, 29.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7026/30753 [11:59<40:36,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7027/30753 [11:59<41:00,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7028/30753 [11:59<41:04,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.1ms\nSpeed: 4.0ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7029/30753 [11:59<41:05,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7030/30753 [11:59<41:02,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 4.3ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7031/30753 [11:59<41:04,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 4.5ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7032/30753 [11:59<41:02,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.2ms\nSpeed: 2.9ms preprocess, 29.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7033/30753 [11:59<42:14,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7034/30753 [11:59<41:45,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.6ms\nSpeed: 5.1ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7035/30753 [11:59<41:33,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 4.1ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7036/30753 [12:00<41:05,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7037/30753 [12:00<40:52,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7038/30753 [12:00<40:49,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7039/30753 [12:00<40:38,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7040/30753 [12:00<40:40,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7041/30753 [12:00<40:41,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7042/30753 [12:00<40:32,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 4.6ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7043/30753 [12:00<40:26,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 4.2ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7044/30753 [12:00<40:29,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 4.6ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7045/30753 [12:01<41:46,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7046/30753 [12:01<41:29,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.3ms\nSpeed: 4.6ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7048/30753 [12:01<40:30,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7049/30753 [12:01<40:25,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7051/30753 [12:01<40:53,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7052/30753 [12:01<41:09,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7053/30753 [12:01<40:56,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7054/30753 [12:01<40:47,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7055/30753 [12:02<41:01,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7056/30753 [12:02<40:53,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 4.4ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7057/30753 [12:02<41:59,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7058/30753 [12:02<41:19,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.0ms\nSpeed: 3.1ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7060/30753 [12:02<40:10,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 3.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.7ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7062/30753 [12:02<39:41,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.1ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7064/30753 [12:02<39:31,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7065/30753 [12:03<39:32,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.5ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7066/30753 [12:03<39:36,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7068/30753 [12:03<39:27, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7069/30753 [12:03<40:28,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 4.5ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7070/30753 [12:03<40:28,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7071/30753 [12:03<40:14,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 4.4ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7073/30753 [12:03<39:48,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7075/30753 [12:04<39:28, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7077/30753 [12:04<39:50,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7078/30753 [12:04<39:49,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7080/30753 [12:04<39:21, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7081/30753 [12:04<40:18,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7083/30753 [12:04<39:47,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7085/30753 [12:05<39:29,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7087/30753 [12:05<39:15, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7089/30753 [12:05<39:03, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 2.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7091/30753 [12:05<38:59, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7093/30753 [12:05<39:44,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.3ms preprocess, 28.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7094/30753 [12:05<40:01,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.4ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7096/30753 [12:06<39:40,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7097/30753 [12:06<39:57,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7098/30753 [12:06<39:58,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.6ms\nSpeed: 2.7ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7099/30753 [12:06<42:15,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7100/30753 [12:06<42:10,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7101/30753 [12:06<42:49,  9.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7102/30753 [12:06<42:44,  9.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.1ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7103/30753 [12:06<42:13,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7104/30753 [12:07<41:36,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7105/30753 [12:07<42:33,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 suitcase, 1 clock, 28.2ms\nSpeed: 4.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7106/30753 [12:07<42:07,  9.36it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 26.5ms\nSpeed: 4.8ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7107/30753 [12:07<41:26,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7108/30753 [12:07<41:22,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7109/30753 [12:07<41:05,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7110/30753 [12:07<41:10,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.6ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7112/30753 [12:07<40:00,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.4ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7113/30753 [12:07<40:12,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7114/30753 [12:08<40:24,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 29.6ms\nSpeed: 3.8ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7115/30753 [12:08<40:38,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7116/30753 [12:08<40:41,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.9ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7117/30753 [12:08<41:44,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7118/30753 [12:08<41:39,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 3.9ms preprocess, 27.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7119/30753 [12:08<41:22,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7120/30753 [12:08<41:13,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7121/30753 [12:08<41:23,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7122/30753 [12:08<40:58,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.9ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7123/30753 [12:09<41:38,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 4.0ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7124/30753 [12:09<41:08,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7125/30753 [12:09<41:06,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.6ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7126/30753 [12:09<40:56,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.7ms\nSpeed: 3.1ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7127/30753 [12:09<41:36,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7128/30753 [12:09<41:22,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7129/30753 [12:09<42:07,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7130/30753 [12:09<41:30,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 4.2ms preprocess, 28.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7132/30753 [12:09<42:49,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.5ms\nSpeed: 4.9ms preprocess, 25.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7133/30753 [12:10<42:56,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.7ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7134/30753 [12:10<43:23,  9.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7135/30753 [12:10<42:56,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 25.8ms\nSpeed: 3.8ms preprocess, 25.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7136/30753 [12:10<42:28,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7137/30753 [12:10<42:22,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.1ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7138/30753 [12:10<43:00,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.4ms preprocess, 26.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7139/30753 [12:10<42:49,  9.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7140/30753 [12:10<42:05,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.8ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7141/30753 [12:10<43:02,  9.14it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7142/30753 [12:11<42:30,  9.26it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7143/30753 [12:11<41:53,  9.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7145/30753 [12:11<40:32,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7147/30753 [12:11<39:41,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 29.1ms\nSpeed: 3.0ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7148/30753 [12:11<39:41,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7150/30753 [12:11<39:26,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 10 persons, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7151/30753 [12:11<40:48,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.2ms\nSpeed: 3.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7152/30753 [12:12<41:07,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 30.5ms\nSpeed: 3.4ms preprocess, 30.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7153/30753 [12:12<41:49,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7155/30753 [12:12<40:55,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 3.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.9ms\nSpeed: 3.0ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7157/30753 [12:12<40:17,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7159/30753 [12:12<39:54,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7160/30753 [12:12<40:17,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 30.3ms\nSpeed: 3.1ms preprocess, 30.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7161/30753 [12:13<40:25,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7162/30753 [12:13<40:10,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7164/30753 [12:13<39:49,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7165/30753 [12:13<40:48,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7166/30753 [12:13<40:40,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.2ms\nSpeed: 4.4ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7167/30753 [12:13<40:21,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 29.0ms\nSpeed: 4.4ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7168/30753 [12:13<40:28,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7169/30753 [12:13<40:26,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7170/30753 [12:13<40:13,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.1ms\nSpeed: 4.1ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.6ms\nSpeed: 4.5ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7172/30753 [12:14<39:49,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7173/30753 [12:14<39:48,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 26.6ms\nSpeed: 4.4ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7175/30753 [12:14<39:38,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7176/30753 [12:14<39:42,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7177/30753 [12:14<40:58,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7178/30753 [12:14<40:47,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7179/30753 [12:14<40:26,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7180/30753 [12:14<40:16,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7181/30753 [12:15<40:13,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.1ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7183/30753 [12:15<39:42,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7184/30753 [12:15<39:42,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.5ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7185/30753 [12:15<39:42,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7187/30753 [12:15<39:20,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7189/30753 [12:15<39:53,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7190/30753 [12:15<39:53,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7192/30753 [12:16<39:11, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.5ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7193/30753 [12:16<39:35,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7194/30753 [12:16<39:54,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7195/30753 [12:16<41:41,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 24.5ms\nSpeed: 4.6ms preprocess, 24.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7196/30753 [12:16<41:20,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7197/30753 [12:16<41:11,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.2ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7198/30753 [12:16<41:10,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 4.3ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7199/30753 [12:16<40:56,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7200/30753 [12:17<40:56,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7201/30753 [12:17<41:45,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7202/30753 [12:17<42:14,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7203/30753 [12:17<41:25,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7204/30753 [12:17<40:48,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.6ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7205/30753 [12:17<41:26,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7206/30753 [12:17<40:53,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7208/30753 [12:17<40:02,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7209/30753 [12:17<39:52,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 3.2ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7210/30753 [12:18<40:15,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7211/30753 [12:18<40:13,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7212/30753 [12:18<40:16,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.3ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7213/30753 [12:18<41:26,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.6ms\nSpeed: 3.9ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7214/30753 [12:18<41:26,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7215/30753 [12:18<41:07,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7216/30753 [12:18<41:03,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7217/30753 [12:18<41:02,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7218/30753 [12:18<40:51,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7219/30753 [12:19<40:52,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.9ms preprocess, 27.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7220/30753 [12:19<40:46,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7221/30753 [12:19<40:29,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.1ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7222/30753 [12:19<40:35,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7223/30753 [12:19<40:31,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 4.4ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7224/30753 [12:19<40:36,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 29.3ms\nSpeed: 3.8ms preprocess, 29.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7225/30753 [12:19<42:06,  9.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  23%|██▎       | 7226/30753 [12:19<42:10,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7227/30753 [12:19<42:29,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7228/30753 [12:19<41:50,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 4.3ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7229/30753 [12:20<41:26,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 4.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7230/30753 [12:20<41:03,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 4.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7231/30753 [12:20<40:50,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.8ms\nSpeed: 4.6ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7232/30753 [12:20<40:51,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7233/30753 [12:20<40:54,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 4.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7234/30753 [12:20<40:54,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 4.0ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7235/30753 [12:20<40:54,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.1ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7236/30753 [12:20<40:59,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7237/30753 [12:20<42:03,  9.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 4.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7238/30753 [12:21<41:46,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7239/30753 [12:21<41:01,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7240/30753 [12:21<40:46,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7241/30753 [12:21<40:33,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7242/30753 [12:21<40:22,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7243/30753 [12:21<40:26,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7244/30753 [12:21<40:31,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7245/30753 [12:21<40:37,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.5ms\nSpeed: 3.5ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7246/30753 [12:21<40:42,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7247/30753 [12:21<40:46,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7248/30753 [12:22<41:05,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7249/30753 [12:22<41:54,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7250/30753 [12:22<41:52,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.8ms preprocess, 26.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7251/30753 [12:22<42:49,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7252/30753 [12:22<43:08,  9.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 26.3ms\nSpeed: 4.3ms preprocess, 26.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7253/30753 [12:22<42:20,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7254/30753 [12:22<41:44,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 4.0ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7255/30753 [12:22<41:25,  9.45it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 4.3ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7256/30753 [12:22<42:13,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.4ms\nSpeed: 3.8ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7257/30753 [12:23<42:07,  9.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 2.8ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7258/30753 [12:23<41:18,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7259/30753 [12:23<40:53,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7260/30753 [12:23<40:38,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7261/30753 [12:23<41:44,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7262/30753 [12:23<41:05,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 3.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7263/30753 [12:23<40:37,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7264/30753 [12:23<40:19,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7265/30753 [12:23<40:00,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7266/30753 [12:23<39:46,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7267/30753 [12:24<39:37,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7268/30753 [12:24<39:36,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7269/30753 [12:24<39:32,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7270/30753 [12:24<39:36,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7271/30753 [12:24<39:44,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7272/30753 [12:24<39:44,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7273/30753 [12:24<41:01,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7274/30753 [12:24<40:33,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7275/30753 [12:24<40:09,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.3ms\nSpeed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7277/30753 [12:25<40:26,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7279/30753 [12:25<39:35,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7281/30753 [12:25<39:14,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 handbag, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7282/30753 [12:25<39:19,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.3ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7284/30753 [12:25<39:02, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7285/30753 [12:25<39:44,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.6ms\nSpeed: 2.9ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7286/30753 [12:25<39:49,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7287/30753 [12:26<39:53,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7288/30753 [12:26<39:52,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7289/30753 [12:26<40:15,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.7ms\nSpeed: 3.3ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7290/30753 [12:26<40:08,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.5ms\nSpeed: 2.9ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7291/30753 [12:26<41:54,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.1ms\nSpeed: 4.3ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7292/30753 [12:26<41:12,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.7ms\nSpeed: 3.1ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7293/30753 [12:26<40:44,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 1 clock, 27.2ms\nSpeed: 3.1ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7295/30753 [12:26<39:41,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7296/30753 [12:27<39:48,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7297/30753 [12:27<40:50,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 28.6ms\nSpeed: 3.2ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7299/30753 [12:27<39:54,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7301/30753 [12:27<40:22,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7302/30753 [12:27<40:51,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▎       | 7303/30753 [12:27<40:35,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7305/30753 [12:27<39:50,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7306/30753 [12:28<40:37,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7307/30753 [12:28<40:22,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 1 handbag, 1 suitcase, 1 clock, 27.2ms\nSpeed: 3.2ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 29.0ms\nSpeed: 4.8ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7309/30753 [12:28<40:46,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 27.1ms\nSpeed: 3.1ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7310/30753 [12:28<40:23,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7311/30753 [12:28<40:32,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 29.0ms\nSpeed: 3.4ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7312/30753 [12:28<40:30,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7313/30753 [12:28<40:08,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.4ms\nSpeed: 3.0ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7315/30753 [12:28<39:37,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.1ms\nSpeed: 3.6ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7316/30753 [12:29<39:53,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 28.6ms\nSpeed: 3.9ms preprocess, 28.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7317/30753 [12:29<39:54,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.8ms\nSpeed: 4.5ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7318/30753 [12:29<39:59,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.3ms\nSpeed: 4.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7319/30753 [12:29<40:09,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7320/30753 [12:29<40:10,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7321/30753 [12:29<41:09,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7322/30753 [12:29<41:08,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 28.5ms\nSpeed: 4.3ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7323/30753 [12:29<40:55,  9.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7324/30753 [12:29<40:42,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 backpack, 2 suitcases, 1 clock, 27.8ms\nSpeed: 4.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7325/30753 [12:30<40:34,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 2 suitcases, 1 clock, 26.8ms\nSpeed: 3.0ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 backpacks, 2 suitcases, 1 clock, 28.8ms\nSpeed: 2.7ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7327/30753 [12:30<40:26,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 2 suitcases, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7328/30753 [12:30<40:16,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 2 suitcases, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7329/30753 [12:30<40:00,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 2 suitcases, 1 clock, 28.2ms\nSpeed: 3.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7330/30753 [12:30<39:54,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 2 suitcases, 1 clock, 27.3ms\nSpeed: 4.3ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7331/30753 [12:30<40:12,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 2 backpacks, 2 suitcases, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7332/30753 [12:30<40:15,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 3 backpacks, 1 suitcase, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7333/30753 [12:30<41:29,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 3 backpacks, 1 suitcase, 1 clock, 28.2ms\nSpeed: 3.7ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7334/30753 [12:30<41:45,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 3 backpacks, 1 suitcase, 1 clock, 28.8ms\nSpeed: 4.3ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7335/30753 [12:31<41:46,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 3 backpacks, 1 suitcase, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7336/30753 [12:31<41:20,  9.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 3 backpacks, 1 suitcase, 1 clock, 28.2ms\nSpeed: 2.9ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7337/30753 [12:31<40:41,  9.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 3 backpacks, 1 suitcase, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7338/30753 [12:31<40:21,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 3 backpacks, 1 suitcase, 1 clock, 27.8ms\nSpeed: 2.8ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 3 backpacks, 1 suitcase, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7340/30753 [12:31<39:36,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 3 backpacks, 1 clock, 27.8ms\nSpeed: 3.1ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 2.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7342/30753 [12:31<39:13,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.6ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7343/30753 [12:31<39:23,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.4ms\nSpeed: 3.4ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 backpacks, 1 clock, 28.0ms\nSpeed: 2.9ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7345/30753 [12:32<39:48,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7346/30753 [12:32<39:43,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7347/30753 [12:32<40:02,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 2 backpacks, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7348/30753 [12:32<40:04,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 26.3ms\nSpeed: 3.8ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7349/30753 [12:32<40:15,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7350/30753 [12:32<40:20,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7351/30753 [12:32<41:47,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7352/30753 [12:32<42:09,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 1 clock, 28.3ms\nSpeed: 3.8ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7353/30753 [12:32<41:36,  9.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 3.4ms preprocess, 27.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7354/30753 [12:33<41:07,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.8ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7355/30753 [12:33<40:42,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.9ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7356/30753 [12:33<41:21,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7357/30753 [12:33<42:03,  9.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7358/30753 [12:33<41:29,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7359/30753 [12:33<41:09,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7360/30753 [12:33<41:06,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7361/30753 [12:33<41:25,  9.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7362/30753 [12:33<40:46,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 4.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7364/30753 [12:34<40:15,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7365/30753 [12:34<40:07,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7366/30753 [12:34<40:00,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7367/30753 [12:34<40:00,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 4.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7368/30753 [12:34<39:49,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 3.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7369/30753 [12:34<41:01,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7370/30753 [12:34<40:36,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.8ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7372/30753 [12:34<39:28,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.8ms\nSpeed: 3.8ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7374/30753 [12:35<39:17,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7376/30753 [12:35<39:04,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7377/30753 [12:35<39:35,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7378/30753 [12:35<39:35,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 4.4ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7379/30753 [12:35<39:28,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.0ms\nSpeed: 3.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.2ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7381/30753 [12:35<39:38,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 4.5ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7383/30753 [12:35<39:23,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7384/30753 [12:36<39:36,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7385/30753 [12:36<39:33,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7386/30753 [12:36<39:58,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.7ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7387/30753 [12:36<39:55,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 4.6ms preprocess, 28.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7388/30753 [12:36<42:06,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 4.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7390/30753 [12:36<40:34,  9.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7392/30753 [12:36<39:45,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7393/30753 [12:37<40:22,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.8ms\nSpeed: 3.5ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.5ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7395/30753 [12:37<39:35,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 29.1ms\nSpeed: 3.4ms preprocess, 29.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7397/30753 [12:37<39:12,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 4.0ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7399/30753 [12:37<39:01,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7400/30753 [12:37<39:06,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.8ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7401/30753 [12:37<39:59,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.6ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7402/30753 [12:37<40:15,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 4.2ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7404/30753 [12:38<39:35,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.6ms preprocess, 27.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7405/30753 [12:38<40:00,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 4.7ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 4.2ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7407/30753 [12:38<39:16,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.8ms\nSpeed: 3.5ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7408/30753 [12:38<39:17,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.7ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7409/30753 [12:38<39:19,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 4.3ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7410/30753 [12:38<39:41,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.8ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 4.3ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7412/30753 [12:38<39:25,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7413/30753 [12:39<39:28,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 30.2ms\nSpeed: 3.9ms preprocess, 30.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7414/30753 [12:39<39:44,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7415/30753 [12:39<39:36,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7417/30753 [12:39<39:43,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7419/30753 [12:39<39:03,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.5ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7420/30753 [12:39<39:09,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.1ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7421/30753 [12:39<39:08,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 4.5ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7422/30753 [12:39<39:23,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7423/30753 [12:40<39:26,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7425/30753 [12:40<38:46, 10.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.9ms\nSpeed: 3.5ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7427/30753 [12:40<39:10,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7428/30753 [12:40<39:13,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 3.7ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7429/30753 [12:40<39:56,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7430/30753 [12:40<39:56,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.0ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7432/30753 [12:40<39:16,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7433/30753 [12:41<39:13,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.6ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7434/30753 [12:41<39:24,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7435/30753 [12:41<40:24,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 24.5ms\nSpeed: 4.3ms preprocess, 24.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7436/30753 [12:41<40:15,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7437/30753 [12:41<41:50,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 4.7ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7438/30753 [12:41<41:15,  9.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.1ms\nSpeed: 4.8ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7439/30753 [12:41<40:40,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 24.9ms\nSpeed: 4.4ms preprocess, 24.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7440/30753 [12:41<40:55,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7441/30753 [12:41<41:39,  9.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.6ms\nSpeed: 4.1ms preprocess, 26.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7442/30753 [12:42<41:00,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 3.7ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7443/30753 [12:42<40:23,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7444/30753 [12:42<40:08,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 4.6ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7446/30753 [12:42<39:34,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7447/30753 [12:42<39:40,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.5ms preprocess, 27.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7448/30753 [12:42<39:47,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7449/30753 [12:42<39:44,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 4.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7450/30753 [12:42<39:29,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7451/30753 [12:42<40:35,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7452/30753 [12:43<40:44,  9.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.6ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7453/30753 [12:43<41:33,  9.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.3ms\nSpeed: 3.7ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7454/30753 [12:43<40:50,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7456/30753 [12:43<40:12,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.5ms\nSpeed: 4.4ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.3ms\nSpeed: 4.3ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7458/30753 [12:43<39:29,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7460/30753 [12:43<39:08,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.0ms\nSpeed: 4.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7461/30753 [12:43<39:25,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.6ms\nSpeed: 4.5ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7462/30753 [12:44<39:33,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7464/30753 [12:44<38:51,  9.99it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7465/30753 [12:44<39:28,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 29.8ms\nSpeed: 4.3ms preprocess, 29.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7466/30753 [12:44<39:23,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 27.8ms\nSpeed: 4.3ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 clock, 27.9ms\nSpeed: 4.4ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7468/30753 [12:44<38:46, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 11 persons, 1 clock, 28.4ms\nSpeed: 4.5ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 29.2ms\nSpeed: 4.3ms preprocess, 29.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7470/30753 [12:44<38:29, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.4ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.9ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7472/30753 [12:45<38:12, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.0ms\nSpeed: 4.1ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7474/30753 [12:45<38:02, 10.20it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.7ms preprocess, 27.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7476/30753 [12:45<37:47, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.4ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 3.9ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7478/30753 [12:45<38:34, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7480/30753 [12:45<38:18, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.2ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7482/30753 [12:46<38:24, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.7ms\nSpeed: 4.2ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7484/30753 [12:46<38:50,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 4.2ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7485/30753 [12:46<38:51,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7486/30753 [12:46<39:51,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.6ms preprocess, 27.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7487/30753 [12:46<39:44,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.3ms\nSpeed: 2.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7489/30753 [12:46<39:44,  9.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7491/30753 [12:46<39:03,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.1ms\nSpeed: 3.0ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7493/30753 [12:47<38:35, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 3.3ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7495/30753 [12:47<38:23, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.3ms\nSpeed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.5ms\nSpeed: 3.3ms preprocess, 28.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7497/30753 [12:47<38:06, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7499/30753 [12:47<38:22, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 27.6ms\nSpeed: 3.9ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7501/30753 [12:47<39:05,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7502/30753 [12:48<39:20,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7504/30753 [12:48<38:33, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.5ms\nSpeed: 3.8ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 clock, 28.4ms\nSpeed: 4.2ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7506/30753 [12:48<38:35, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 clock, 27.2ms\nSpeed: 3.0ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7508/30753 [12:48<38:05, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7510/30753 [12:48<38:10, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.4ms\nSpeed: 3.9ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7512/30753 [12:49<38:14, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.2ms preprocess, 27.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.9ms\nSpeed: 3.5ms preprocess, 28.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7514/30753 [12:49<39:02,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 29.0ms\nSpeed: 3.7ms preprocess, 29.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 2.9ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7516/30753 [12:49<38:41, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7518/30753 [12:49<38:30, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.4ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7520/30753 [12:49<38:25, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.7ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7522/30753 [12:50<38:20, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.5ms\nSpeed: 3.0ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7524/30753 [12:50<38:24, 10.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.0ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 28.5ms\nSpeed: 3.7ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7526/30753 [12:50<39:07,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.3ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7527/30753 [12:50<39:29,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.3ms\nSpeed: 3.4ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7528/30753 [12:50<39:34,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7529/30753 [12:50<39:30,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.7ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7530/30753 [12:50<39:30,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7532/30753 [12:51<39:00,  9.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.5ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  24%|██▍       | 7534/30753 [12:51<38:50,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.6ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7535/30753 [12:51<38:50,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 3.4ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7537/30753 [12:51<39:11,  9.87it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7539/30753 [12:51<38:58,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 3.2ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7541/30753 [12:51<38:51,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.2ms\nSpeed: 4.6ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7542/30753 [12:52<39:02,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.7ms\nSpeed: 3.7ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7543/30753 [12:52<39:07,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.8ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7544/30753 [12:52<39:08,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7545/30753 [12:52<39:01,  9.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7546/30753 [12:52<39:04,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 clock, 26.9ms\nSpeed: 4.9ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7548/30753 [12:52<38:49,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 26.7ms\nSpeed: 4.4ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7549/30753 [12:52<39:55,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 3.7ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7550/30753 [12:52<39:49,  9.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 3.6ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7551/30753 [12:53<40:49,  9.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.4ms\nSpeed: 4.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7552/30753 [12:53<41:20,  9.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 3.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7553/30753 [12:53<40:40,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7554/30753 [12:53<40:15,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.7ms\nSpeed: 3.7ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7555/30753 [12:53<40:23,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.1ms\nSpeed: 4.5ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7556/30753 [12:53<40:10,  9.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 28.1ms\nSpeed: 4.4ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7557/30753 [12:53<39:57,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.3ms\nSpeed: 4.2ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7558/30753 [12:53<39:41,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 clock, 27.6ms\nSpeed: 4.2ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7559/30753 [12:53<39:31,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7560/30753 [12:53<39:27,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7561/30753 [12:54<40:41,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 4.8ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7562/30753 [12:54<40:40,  9.50it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.1ms preprocess, 27.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7563/30753 [12:54<40:22,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.9ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7565/30753 [12:54<39:23,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.8ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7566/30753 [12:54<39:15,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.6ms\nSpeed: 3.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7567/30753 [12:54<39:07,  9.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7568/30753 [12:54<39:19,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.0ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7570/30753 [12:54<38:33, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.2ms preprocess, 28.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7571/30753 [12:55<38:41,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7572/30753 [12:55<38:51,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.9ms\nSpeed: 3.6ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7573/30753 [12:55<39:58,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 4.3ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7574/30753 [12:55<39:50,  9.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7575/30753 [12:55<39:44,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.0ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7576/30753 [12:55<39:34,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.1ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7577/30753 [12:55<40:34,  9.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.4ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.1ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7579/30753 [12:55<39:31,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7580/30753 [12:55<39:19,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7581/30753 [12:56<39:14,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.3ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7583/30753 [12:56<40:23,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 4.1ms preprocess, 27.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7585/30753 [12:56<42:08,  9.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7586/30753 [12:56<41:32,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.8ms\nSpeed: 4.7ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7587/30753 [12:56<40:55,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.9ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.2ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7589/30753 [12:56<39:52,  9.68it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.8ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7590/30753 [12:57<40:06,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.3ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7591/30753 [12:57<40:09,  9.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.1ms\nSpeed: 3.2ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7592/30753 [12:57<40:01,  9.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7593/30753 [12:57<39:55,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.6ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7594/30753 [12:57<39:42,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.5ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7595/30753 [12:57<39:33,  9.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.6ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7596/30753 [12:57<39:42,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.6ms\nSpeed: 3.6ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7597/30753 [12:57<40:55,  9.43it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 25.7ms\nSpeed: 4.2ms preprocess, 25.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7598/30753 [12:57<40:19,  9.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7599/30753 [12:57<40:04,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.2ms\nSpeed: 3.9ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.1ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7601/30753 [12:58<40:22,  9.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7602/30753 [12:58<40:42,  9.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 2.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.1ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7604/30753 [12:58<39:40,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7605/30753 [12:58<39:55,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.3ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7606/30753 [12:58<39:42,  9.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 4.6ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7607/30753 [12:58<39:45,  9.70it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 2.9ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7609/30753 [12:59<39:38,  9.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 4.2ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7611/30753 [12:59<39:09,  9.85it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.3ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7613/30753 [12:59<38:41,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.6ms\nSpeed: 3.1ms preprocess, 26.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7615/30753 [12:59<38:14, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.4ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.1ms\nSpeed: 3.3ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7617/30753 [12:59<38:04, 10.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.8ms\nSpeed: 3.4ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.6ms\nSpeed: 3.5ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7619/30753 [13:00<38:06, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 4.2ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.8ms preprocess, 28.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7621/30753 [13:00<38:49,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.8ms\nSpeed: 3.0ms preprocess, 27.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.2ms\nSpeed: 3.2ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7623/30753 [13:00<38:32, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 26.2ms\nSpeed: 4.5ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7625/30753 [13:00<38:24, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7627/30753 [13:00<38:41,  9.96it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.1ms\nSpeed: 3.4ms preprocess, 28.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7628/30753 [13:00<38:48,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.3ms\nSpeed: 3.3ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7630/30753 [13:01<38:29, 10.01it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 2 backpacks, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7631/30753 [13:01<38:31, 10.00it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7633/30753 [13:01<39:05,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.2ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.4ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7635/30753 [13:01<38:39,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.0ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7637/30753 [13:01<38:19, 10.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.1ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7639/30753 [13:02<37:56, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.3ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7641/30753 [13:02<37:54, 10.16it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.3ms\nSpeed: 4.0ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7643/30753 [13:02<38:08, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 2.9ms preprocess, 28.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7645/30753 [13:02<38:56,  9.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.5ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7646/30753 [13:02<39:04,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.6ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7647/30753 [13:02<39:17,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 4.5ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7648/30753 [13:02<39:17,  9.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 4.6ms preprocess, 27.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7649/30753 [13:03<39:22,  9.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.3ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7650/30753 [13:03<39:12,  9.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.5ms\nSpeed: 4.2ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7651/30753 [13:03<40:28,  9.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.0ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7652/30753 [13:03<40:41,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.5ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7653/30753 [13:03<40:12,  9.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.8ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7654/30753 [13:03<40:19,  9.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 4.0ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7655/30753 [13:03<39:48,  9.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 4.9ms preprocess, 29.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7656/30753 [13:03<39:30,  9.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 4.2ms preprocess, 28.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7657/30753 [13:03<39:59,  9.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.0ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 4.4ms preprocess, 27.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7659/30753 [13:04<39:14,  9.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.6ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.7ms\nSpeed: 3.0ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7661/30753 [13:04<38:40,  9.95it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.7ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.2ms\nSpeed: 3.3ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7663/30753 [13:04<38:04, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 29.2ms\nSpeed: 3.1ms preprocess, 29.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7665/30753 [13:04<38:03, 10.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.0ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.0ms\nSpeed: 3.2ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7667/30753 [13:04<37:49, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.3ms\nSpeed: 3.2ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 2.8ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7669/30753 [13:05<38:20, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.5ms preprocess, 28.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.4ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7671/30753 [13:05<38:01, 10.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.0ms preprocess, 27.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.3ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7673/30753 [13:05<37:30, 10.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 2.9ms preprocess, 27.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.0ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7675/30753 [13:05<37:04, 10.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 2.9ms preprocess, 26.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7677/30753 [13:05<37:16, 10.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.0ms\nSpeed: 3.4ms preprocess, 28.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.5ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7679/30753 [13:06<37:21, 10.29it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 12 persons, 1 backpack, 1 clock, 27.5ms\nSpeed: 3.2ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.1ms\nSpeed: 3.4ms preprocess, 29.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7681/30753 [13:06<38:03, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.4ms preprocess, 27.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.7ms\nSpeed: 3.3ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7683/30753 [13:06<37:53, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.1ms preprocess, 27.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 3.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7685/30753 [13:06<39:17,  9.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 26.9ms\nSpeed: 3.3ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.5ms\nSpeed: 4.1ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▍       | 7687/30753 [13:06<38:44,  9.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 3.2ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.2ms\nSpeed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▌       | 7689/30753 [13:07<38:22, 10.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 3.1ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.4ms\nSpeed: 3.2ms preprocess, 27.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▌       | 7691/30753 [13:07<37:52, 10.15it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.4ms preprocess, 28.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.6ms\nSpeed: 3.2ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▌       | 7693/30753 [13:07<38:16, 10.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.9ms\nSpeed: 2.9ms preprocess, 27.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 29.0ms\nSpeed: 3.8ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▌       | 7695/30753 [13:07<38:02, 10.10it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.8ms\nSpeed: 3.1ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.1ms\nSpeed: 3.1ms preprocess, 28.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▌       | 7697/30753 [13:07<37:44, 10.18it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.6ms\nSpeed: 3.3ms preprocess, 28.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 backpack, 1 clock, 28.4ms\nSpeed: 4.1ms preprocess, 28.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▌       | 7699/30753 [13:08<37:47, 10.17it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 13 persons, 1 backpack, 1 clock, 27.8ms\nSpeed: 3.9ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing Video:  25%|██▌       | 7700/30753 [13:08<39:19,  9.77it/s]","output_type":"stream"},{"name":"stdout","text":"Video frame is empty or video processing has been successfully completed.\nVideo processing complete and the file has been saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"polygon_points = np.array([[\n                    270.68710384915494,\n                    832.9224757715328\n                ],\n                [\n                    1412.6219817768006,\n                    376.6059232869861\n                ],\n                [\n                    1144.8227883536629,\n                    275.9507796486947\n                ],\n                [\n                    194.71646080318862,\n                    563.374785355727\n                ]]) \npolygon = Polygon(polygon_points)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T09:08:16.500115Z","iopub.execute_input":"2024-05-05T09:08:16.500476Z","iopub.status.idle":"2024-05-05T09:08:16.506265Z","shell.execute_reply.started":"2024-05-05T09:08:16.500445Z","shell.execute_reply":"2024-05-05T09:08:16.505274Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom collections import deque\nfrom shapely.geometry import Polygon\n\n# Load the YOLOv8 model\nmodel = YOLO(\"yolov8n.pt\")  # Replace with the path to your YOLOv8 model\n\n# Define the polygon region\npolygon_points = np.array([[270.68710384915494,832.9224757715328],[1412.6219817768006,376.6059232869861],[1144.8227883536629,275.9507796486947],[194.71646080318862,563.374785355727]])  # Replace with the coordinates of your polygon\npolygon = Polygon(polygon_points)\n\n# Define the time window and queue\ntime_window = 30  # Time window in seconds\ntime_window_frames = time_window * 30  # Assuming 30 FPS\nframe_queue = deque(maxlen=time_window_frames)\nalert_queue = deque(maxlen=10)  # Queue to track alert condition\n\n# Open the input video\nvideo_path = \"/kaggle/input/pune-metro-hackathon/dataset/ticketing-crowd/1 (4).avi\"\ncap = cv2.VideoCapture(video_path)\n\n# Get the video dimensions\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Create the output video writer\noutput_path = \"output_video.mp4\"\nfourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Run YOLOv8 detection\n    results = model(frame)\n    people_count = 0\n    for result in results:\n        for box in result.boxes:\n            if box.cls == 0:  # Assuming class 0 is for person\n                xyxy = box.xyxy.cpu().numpy()[0]  # Get the single element from the tensor\n                x1, y1, x2, y2 = xyxy.astype(int)  # Convert to int and unpack\n                center = ((x1 + x2) // 2, (y1 + y2) // 2)\n                if polygon.contains(Point(center)):\n                    people_count += 1\n\n    # Add the current frame's people count to the queue\n    frame_queue.append(people_count)\n\n    # Check if the people count increased in the last 10 seconds\n    if len(alert_queue) == 10:\n        if any(count > alert_queue[0] for count in alert_queue[1:]):\n            print(\"Alert! People count increased in the specified region.\")\n    alert_queue.append(people_count)\n\n    # Write the processed frame to the output video\n    for result in results:\n        result.render()\n        out.write(result.imgs[0])\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\nout.release()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T09:14:20.615728Z","iopub.execute_input":"2024-05-05T09:14:20.616484Z","iopub.status.idle":"2024-05-05T09:14:20.938610Z","shell.execute_reply.started":"2024-05-05T09:14:20.616450Z","shell.execute_reply":"2024-05-05T09:14:20.937233Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\n0: 384x640 15 persons, 1 skateboard, 1 clock, 11.2ms\nSpeed: 2.1ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m             x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m xyxy\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# Convert to int and unpack\u001b[39;00m\n\u001b[1;32m     47\u001b[0m             center \u001b[38;5;241m=\u001b[39m ((x1 \u001b[38;5;241m+\u001b[39m x2) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, (y1 \u001b[38;5;241m+\u001b[39m y2) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m polygon\u001b[38;5;241m.\u001b[39mcontains(\u001b[43mPoint\u001b[49m(center)):\n\u001b[1;32m     49\u001b[0m                 people_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Add the current frame's people count to the queue\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'Point' is not defined"],"ename":"NameError","evalue":"name 'Point' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}