{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8268787,"sourceType":"datasetVersion","datasetId":4908965},{"sourceId":8368255,"sourceType":"datasetVersion","datasetId":4974581},{"sourceId":8371617,"sourceType":"datasetVersion","datasetId":4976944},{"sourceId":8460748,"sourceType":"datasetVersion","datasetId":5043455}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics lapx shapely -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nfrom collections import defaultdict\nfrom ultralytics.solutions import object_counter\nfrom shapely.geometry import Point, Polygon\nfrom ultralytics.utils.checks import check_imshow, check_requirements\nfrom ultralytics.utils.plotting import Annotator, colors\ncheck_requirements(\"shapely>=2.0.0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QueueManager:\n    \"\"\"A class to manage the queue management in real-time video stream based on their tracks.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the queue manager with default values for various tracking and counting parameters.\"\"\"\n\n        # Mouse events\n        self.is_drawing = False\n        self.selected_point = None\n\n        # Region & Line Information\n#         self.reg_pts = [(20, 60), (20, 680), (1120, 680), (1120, 60)]\n        self.counting_region = None\n        self.region_color = (255, 0, 255)\n        self.region_thickness = 5\n\n        # Image and annotation Information\n        self.im0 = None\n        self.tf = None\n        self.view_img = False\n        self.view_queue_counts = True\n        self.fontsize = 0.6\n\n        self.names = None  # Classes names\n        self.annotator = None  # Annotator\n        self.window_name = \"Ultralytics YOLOv8 Queue Manager\"\n\n        # Object counting Information\n        self.counts = 0\n        self.count_txt_color = (255, 255, 255)\n\n        # Tracks info\n        self.track_history = defaultdict(list)\n        self.track_thickness = 2\n        self.draw_tracks = False\n        self.track_color = None\n\n        # Check if environment support imshow\n        self.env_check = check_imshow(warn=True)\n\n    def set_args(\n        self,\n        classes_names,\n        reg_pts,\n        line_thickness=2,\n        track_thickness=2,\n        view_img=True,\n        region_color=(255, 0, 255),\n        view_queue_counts=True,\n        draw_tracks=True,\n        count_txt_color=(255, 255, 255),\n        track_color=(0,128,0),\n        region_thickness=5,\n        fontsize=0.7,\n    ):\n        \"\"\"\n        Configures the Counter's image, bounding box line thickness, and counting region points.\n\n        Args:\n            line_thickness (int): Line thickness for bounding boxes.\n            view_img (bool): Flag to control whether to display the video stream.\n            view_queue_counts (bool): Flag to control whether to display the counts on video stream.\n            reg_pts (list): Initial list of points defining the counting region.\n            classes_names (dict): Classes names\n            region_color (RGB color): Color of queue region\n            track_thickness (int): Track thickness\n            draw_tracks (Bool): draw tracks\n            count_txt_color (RGB color): count text color value\n            track_color (RGB color): color for tracks\n            region_thickness (int): Object counting Region thickness\n            fontsize (float): Text display font size\n        \"\"\"\n        self.tf = line_thickness\n        self.view_img = view_img\n        self.view_queue_counts = view_queue_counts\n        self.track_thickness = track_thickness\n        self.draw_tracks = draw_tracks\n        self.region_color = region_color\n\n        if len(reg_pts) >= 3:\n            print(\"Queue region initiated...\")\n            self.reg_pts = reg_pts\n            self.counting_region = Polygon(self.reg_pts)\n        else:\n            print(\"Invalid region points provided...\")\n            print(\"Using default region now....\")\n            self.counting_region = Polygon(self.reg_pts)\n\n        self.names = classes_names\n        self.track_color = track_color\n        self.count_txt_color = count_txt_color\n        self.region_thickness = region_thickness\n        self.fontsize = fontsize\n\n    def extract_and_process_tracks(self, tracks):\n        \"\"\"Extracts and processes tracks for queue management in a video stream.\"\"\"\n\n        # Annotator Init and queue region drawing\n        self.annotator = Annotator(self.im0, self.tf, self.names)\n\n        if tracks[0].boxes.id is not None:\n            boxes = tracks[0].boxes.xyxy.cpu()\n            clss = tracks[0].boxes.cls.cpu().tolist()\n            track_ids = tracks[0].boxes.id.int().cpu().tolist()\n\n            # Extract tracks\n            for box, track_id, cls in zip(boxes, track_ids, clss):\n                # Draw bounding box\n                self.annotator.box_label(box, label=f\"{self.names[cls]}#{track_id}\", color=colors(int(track_id), True))\n                \n                # Use top point instead of centroid\n                top_point = (int((box[0] + box[2]) / 2), int(box[1]))\n                # bottom_point = (int((box[0] + box[2]) / 2), int(box[3]))\n                \n                # Draw Tracks\n                track_line = self.track_history[track_id]\n#                 track_line.append((float((box[0] + box[2]) / 2), float((box[1] + box[3]) / 2)))\n                track_line.append(top_point)\n                if len(track_line) > 30:\n                    track_line.pop(0)\n\n                # Draw track trails\n                if self.draw_tracks:\n                    self.annotator.draw_centroid_and_tracks(\n                        track_line,\n                        color=self.track_color if self.track_color else colors(int(track_id), True),\n                        track_thickness=self.track_thickness,\n                    )\n\n                prev_position = self.track_history[track_id][-2] if len(self.track_history[track_id]) > 1 else None\n\n                if len(self.reg_pts) >= 3:\n                    is_inside = self.counting_region.contains(Point(track_line[-1]))\n                    if prev_position is not None and is_inside:\n                        self.counts += 1\n            if self.counts > 7:\n                # print(\"Warning: Queue count exceeded.\")\n                cv2.putText(self.im0, \"Warning: Queue count exceeded!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA) \n\n        label = \"Queue Counts : \" + str(self.counts)\n\n        if label is not None:\n            self.annotator.queue_counts_display(\n                label,\n                points=self.reg_pts,\n                region_color=self.region_color,\n                txt_color=self.count_txt_color,\n                fontsize=self.fontsize,\n            )\n\n        self.counts = 0\n        self.display_frames()\n\n    def display_frames(self):\n        \"\"\"Display frame.\"\"\"\n        if self.env_check:\n            self.annotator.draw_region(reg_pts=self.reg_pts, thickness=self.region_thickness, color=self.region_color)\n            cv2.namedWindow(self.window_name)\n            cv2.imshow(self.window_name, self.im0)\n#             Break Window\n            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n                return\n\n    def process_queue(self, im0, tracks):\n        \"\"\"\n        Main function to start the queue management process.\n\n        Args:\n            im0 (ndarray): Current frame from the video stream.\n            tracks (list): List of tracks obtained from the object tracking process.\n        \"\"\"\n        self.im0 = im0  # store image\n        self.extract_and_process_tracks(tracks)  # draw region even if no objects\n\n        if self.view_img:\n            self.display_frames()\n        return self.im0\n    \nif __name__ == \"__main__\":\n    QueueManager()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# from ultralytics.solutions import queue_management\nmodel = YOLO(\"yolov8x.pt\")\ncap = cv2.VideoCapture(\"/kaggle/input/final-ticketing/Ticketing_Line_2_Cam3_1.avi\")\n\nassert cap.isOpened(), \"Error reading video file\"\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,\n                                       cv2.CAP_PROP_FRAME_HEIGHT,\n                                       cv2.CAP_PROP_FPS))\n\nvideo_writer = cv2.VideoWriter(\"queue_management.avi\",\n                               cv2.VideoWriter_fourcc(*'mp4v'),\n                               fps,\n                               (w, h))\n\n#queue_region = [[235,911],[179,381],[681,249],[1038,213],[1038,213],[1233,179],[1233,316],[930,554],[674,849]]\n#235,911,179,381,681,249,1038,213,1233,179,1233,316,930,554,674,849\" shape=\"poly\">\nclasses_to_count = [0,1]\n\n#queue = QueueManager()\n# #queue.set_args(classes_names=model.names,\n#                reg_pts=queue_region,\n#                line_thickness=3,\n#                fontsize=1.0,\n#                region_color=(255, 144, 31))\n\n\n# # Main loop for processing video frames\n# while cap.isOpened():\n#     success, im0 = cap.read()\n\n#     if success:\n#         # Object detection and tracking\n#         tracks = model.track(im0, show=False, persist=True,\n#                              verbose=False, classes=0)  # Only person class\n#         out = queue.process_queue(im0, tracks)\n\n#         # Write frame to video\n#         video_writer.write(im0)\n\n#     else:\n#         print(\"Video processing has been successfully completed.\")\n#         break\n\n# # Release resources\n# cap.release()\n# video_writer.release()\n# # cv2.destroyAllWindows()\n# Init Object Counter\n\ncounter = object_counter.ObjectCounter()\ncounter.set_args(view_img=True,\n                 reg_pts=queue_region,\n                 classes_names=model.names,\n                 draw_tracks=True,\n                 line_thickness=2)\n\n# Get total number of frames\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Main loop for processing video frames with tqdm\nfor _ in tqdm(range(total_frames)):\n    success, im0 = cap.read()\n\n    if success:\n        # Object detection and tracking\n        tracks = model.track(im0, show=False, persist=True,\n                             verbose=False, classes=0, imgsz=4096)  # Only person class\n        out = queue.process_queue(im0, tracks)\n\n        # Write frame to video\n        video_writer.write(im0)\n\n    else:\n        print(\"Video processing has been successfully completed.\")\n        break\n\n# Release resources\ncap.release()\nvideo_writer.release()\n# cv2.destroyAllWindows()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\n\ndef trim_video(input_path, output_path, start_time, end_time):\n    # Open input video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error: Unable to open input video file\")\n        return\n\n    # Get frame rate and total number of frames\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate start and end frame numbers\n    start_frame = int(start_time * fps)\n    end_frame = int(end_time * fps)\n    \n    # Validate start and end frame numbers\n    if start_frame >= total_frames:\n        print(\"Error: Start time exceeds video duration\")\n        return\n    if end_frame >= total_frames:\n        end_frame = total_frames - 1\n    \n    # Set start frame\n    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n\n    # Create video writer\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Change codec to XVID for .avi files\n    out = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n\n    # Read and write frames within the specified time range\n    for frame_num in range(start_frame, end_frame + 1):\n        success, frame = cap.read()\n        if success:\n            out.write(frame)\n        else:\n            break\n\n    # Release resources\n    cap.release()\n    out.release()\n\n    print(\"Video trimmed successfully\")\n\n\n# Input video file path\ninput_path = \"/kaggle/input/pune-metro-hackathon/dataset/ticketing-crowd/1 (6).avi \"\n\n# Output video file path\noutput_path = \"/kaggle/working/\"\n\n# Input start and end times for trimming (in seconds)\nstart_time = float(input(\"Enter the start time (in seconds):\"))\nend_time = float(input(\"Enter the end time (in seconds): \"))\n\n# Call function to trim the video\ntrim_video(input_path, output_path, start_time, end_time)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO, solutions\n\nmodel = YOLO(\"yolov8x.pt\")\ncap = cv2.VideoCapture(\"/kaggle/input/final-ticketing/Ticketing_Line_2_Cam3_1.avi\")\nassert cap.isOpened(), \"Error reading video file\"\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\n#line_points = [(20, 400), (1080, 400)]  # line or region points\nclasses_to_count = [0, 2]  # person and car classes for count\n\n# Video writer\nvideo_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Init Object Counter\ncounter = solutions.ObjectCounter(\n    view_img=True,\n    classes_names=model.names,\n    draw_tracks=True,\n    line_thickness=2,\n)\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n    tracks = model.track(im0, persist=True, show=False, classes=classes_to_count, imgsz=7680)\n\n    im0 = counter.start_counting(im0, tracks)\n    video_writer.write(im0)\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"GZfR2Cnq6cLeRcwf1pxa\")\nproject = rf.workspace(\"things-xuzgk\").project(\"1-bfedx\")\nversion = project.version(6)\ndataset = version.download(\"yolov8-obb\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U ipywidgets -q","metadata":{"execution":{"iopub.status.busy":"2024-05-20T07:40:38.353701Z","iopub.execute_input":"2024-05-20T07:40:38.354638Z","iopub.status.idle":"2024-05-20T07:40:52.583849Z","shell.execute_reply.started":"2024-05-20T07:40:38.354594Z","shell.execute_reply":"2024-05-20T07:40:52.582627Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb -q","metadata":{"execution":{"iopub.status.busy":"2024-05-20T07:39:10.718286Z","iopub.execute_input":"2024-05-20T07:39:10.719198Z","iopub.status.idle":"2024-05-20T07:39:28.332207Z","shell.execute_reply.started":"2024-05-20T07:39:10.719162Z","shell.execute_reply":"2024-05-20T07:39:28.330972Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-20T07:39:28.334220Z","iopub.execute_input":"2024-05-20T07:39:28.334596Z","iopub.status.idle":"2024-05-20T07:39:28.927710Z","shell.execute_reply.started":"2024-05-20T07:39:28.334561Z","shell.execute_reply":"2024-05-20T07:39:28.926926Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"cf4b308c1d949dd100d0264e19f74f57d611a6ee\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T07:39:57.197268Z","iopub.execute_input":"2024-05-20T07:39:57.197653Z","iopub.status.idle":"2024-05-20T07:39:59.409688Z","shell.execute_reply.started":"2024-05-20T07:39:57.197623Z","shell.execute_reply":"2024-05-20T07:39:59.408886Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n\n# Load a model\nmodel = YOLO(\"yolov8x.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model with 2 GPUs\nresults = model.train(data=\"/kaggle/working/1-6/data.yaml\", epochs=100, imgsz=960, device=[0, 1])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics==8.2.18","metadata":{"execution":{"iopub.status.busy":"2024-05-20T07:39:29.001917Z","iopub.execute_input":"2024-05-20T07:39:29.002174Z","iopub.status.idle":"2024-05-20T07:39:42.512756Z","shell.execute_reply.started":"2024-05-20T07:39:29.002152Z","shell.execute_reply":"2024-05-20T07:39:42.511866Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting ultralytics==8.2.18\n  Downloading ultralytics-8.2.18-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m756.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (4.9.0.80)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (9.0.0)\nCollecting thop>=0.1.1 (from ultralytics==8.2.18)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.2.18) (0.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.18) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.2.18) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.2.18) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.18) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.18) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.18) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.18) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.18) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.18) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.18) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.18) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.18) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.18) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.2.18) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.2.18) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics==8.2.18) (1.3.0)\nDownloading ultralytics-8.2.18-py3-none-any.whl (757 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.2.18\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"/kaggle/working/runs/detect/train18/weights/last.pt\")  # load a partially trained model\n\n# Resume training\nresults = model.train(resume=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T07:41:02.051469Z","iopub.execute_input":"2024-05-20T07:41:02.052029Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.18 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n                                                      CUDA:1 (Tesla T4, 15102MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/runs/detect/train18/weights/last.pt, data=/kaggle/working/1-6/data.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=[0, 1], workers=8, project=None, name=train18, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=/kaggle/working/runs/detect/train18/weights/last.pt, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train18\n\u001b[34m\u001b[1mDDP:\u001b[0m debug command /opt/conda/bin/python3.10 -m torch.distributed.run --nproc_per_node 2 --master_port 37685 /root/.config/Ultralytics/DDP/_temp_s2l2zgex137170454073584.py\nUltralytics YOLOv8.2.18 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n                                                      CUDA:1 (Tesla T4, 15102MiB)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train18', view at http://localhost:6006/\n","output_type":"stream"},{"name":"stderr","text":"wandb: Currently logged in as: ankit-das16381 (sakec). Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.17.0\nwandb: Run data is saved locally in /kaggle/working/wandb/run-20240520_074113-2fo4ykls\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run train18\nwandb: ⭐️ View project at https://wandb.ai/sakec/YOLOv8\nwandb: 🚀 View run at https://wandb.ai/sakec/YOLOv8/runs/2fo4ykls\n","output_type":"stream"},{"name":"stdout","text":"Transferred 595/595 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/1-6/train/labels.cache... 4012 images, 32 backgrounds, 0 corrupt: 100%|██████████| 4012/4012 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/1-6/valid/labels.cache... 1176 images, 5 backgrounds, 0 corrupt: 100%|██████████| 1176/1176 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/train18/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\nResuming training /kaggle/working/runs/detect/train18/weights/last.pt from epoch 57 to 100 total epochs\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 960 train, 960 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train18\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     57/100      15.4G     0.9051      0.759      1.206         33        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:55<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.782      0.709      0.773      0.521\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     58/100      14.7G     0.8884      0.703      1.183         40        960: 100%|██████████| 251/251 [06:04<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.794      0.701      0.777      0.525\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     59/100      14.8G     0.8749     0.6821      1.187         27        960: 100%|██████████| 251/251 [06:04<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.801       0.71      0.781      0.529\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     60/100      15.4G     0.8694     0.6546      1.167         28        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.797      0.711      0.782       0.53\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     61/100      15.5G     0.8568      0.654      1.158         39        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.789      0.719       0.78      0.526\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     62/100      14.7G     0.8446     0.6349      1.162         42        960: 100%|██████████| 251/251 [06:04<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.794      0.723      0.784      0.529\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     63/100      15.4G     0.8513     0.6481      1.161         43        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.792      0.718      0.785      0.532\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     64/100      15.2G     0.8249     0.6263      1.149         23        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.808      0.704      0.781      0.529\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     65/100      15.5G     0.8274     0.6298      1.146         38        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.805      0.718      0.787      0.533\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     66/100      15.4G     0.8198     0.6263      1.147         34        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.793      0.714      0.784      0.533\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.793      0.719      0.782       0.53\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     68/100      15.2G     0.8106     0.5869      1.137         49        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.785      0.729      0.785      0.533\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     69/100      15.5G     0.8024     0.5967       1.13         60        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.781       0.73      0.788      0.537\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     70/100      15.4G     0.7823     0.5721      1.116         36        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.776      0.731      0.782      0.535\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     71/100      15.5G     0.7814      0.573      1.122         32        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.783      0.728      0.779      0.531\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     72/100      15.2G      0.786     0.5723      1.123         46        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.805      0.699      0.778       0.53\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     73/100      15.5G     0.7783     0.5634      1.111         49        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726       0.81      0.702      0.781      0.533\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     74/100      15.5G     0.7707     0.5564      1.104         33        960: 100%|██████████| 251/251 [06:04<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.792      0.719      0.783      0.537\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     75/100      15.5G     0.7639     0.5524      1.099         41        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.781      0.727      0.784      0.533\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     76/100      15.2G     0.7542     0.5381      1.094         43        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.781      0.725      0.781      0.529\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     77/100      15.5G     0.7557      0.541      1.092         55        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.782      0.715      0.775      0.527\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     78/100      15.5G     0.7453     0.5288      1.093         45        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726       0.78      0.723       0.78      0.531\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     79/100      15.5G     0.7392     0.5296      1.087         42        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.789      0.723      0.781      0.531\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     80/100      15.2G     0.7269     0.5153      1.076         17        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.799      0.715      0.781      0.534\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     81/100      15.5G     0.7313     0.5179      1.084         55        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.791      0.721      0.783      0.534\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     82/100      15.4G     0.7327      0.518      1.082         73        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.779       0.73       0.78      0.534\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     83/100      15.4G     0.7182     0.5024       1.07         44        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.792      0.719      0.775      0.532\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     84/100      15.4G     0.7102     0.4965      1.065         62        960: 100%|██████████| 251/251 [06:02<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.785      0.721      0.775      0.532\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     85/100      15.3G     0.7077     0.5061      1.066         54        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.792      0.721      0.776      0.532\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     86/100      15.5G     0.7191     0.5015      1.076         54        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.783      0.724      0.777      0.531\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     87/100      15.4G     0.6944     0.4844      1.053         34        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.795       0.72      0.775      0.532\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     88/100      15.3G     0.6928      0.485       1.06         29        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:54<00:00,  1.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1176       4726      0.798      0.707      0.775      0.533\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     89/100      15.5G     0.6942     0.4866       1.06         45        960: 100%|██████████| 251/251 [06:03<00:00,  1.45s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  96%|█████████▌| 71/74 [00:52<00:02,  1.41it/s]","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"/kaggle/working/runs/detect/train18/weights/last.pt\")  # load a partially trained model\n\n# Resume training\nresults = model.train(resume=True, data=\"/kaggle/working/1-6/data.yaml\", epochs=100, imgsz=960, device=[0, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.device_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries (replace 'yaml' with 'pyyaml' if needed)\nimport yaml\n\n# Define path to the YAML file (replace with your actual path)\nyaml_file_path = \"/kaggle/working/1-6/data.yaml\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the YAML content\nwith open(yaml_file_path, 'r') as stream:\n    config = yaml.safe_load(stream)\nprint(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n# Modify desired configurations (replace 'key' and 'value' with your changes)\nconfig['valid'] = 'valid/images'\n\n# Write the modified configuration back to the YAML file\nwith open(yaml_file_path, 'w') as stream:\n    yaml.dump(config, stream)\n\nprint(f\"YAML file '{yaml_file_path}' successfully modified!\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}